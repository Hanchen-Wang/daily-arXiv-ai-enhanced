{"id": "2602.22808", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22808", "abs": "https://arxiv.org/abs/2602.22808", "authors": ["Shiqian Su", "Sen Xing", "Xuan Dong", "Muyan Zhong", "Bin Wang", "Xizhou Zhu", "Yuntao Chen", "Wenhai Wang", "Yue Deng", "Pengxiang Zhu", "Ziyuan Liu", "Tiantong Li", "Jiaheng Yu", "Zhe Chen", "Lidong Bing", "Jifeng Dai"], "title": "MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks", "comment": null, "summary": "Despite the remarkable progress of large language models (LLMs), the capabilities of standalone LLMs have begun to plateau when tackling real-world, complex tasks that require interaction with external tools and dynamic environments. Although recent agent frameworks aim to enhance model autonomy through tool integration and external interaction, they still suffer from naive workflows, unstable performance, limited support across diverse benchmarks and tasks, and heavy reliance on costly commercial APIs. In this work, we propose a high-performance and robust open-source agent framework, termed MiroFlow, which incorporates an agent graph for flexible orchestration, an optional deep reasoning mode to enhance performance, and a robust workflow execution to ensure stable and reproducible performance. Extensive experiments demonstrate that MiroFlow consistently achieves state-of-the-art performance across multiple agent benchmarks, including GAIA, BrowseComp-EN/ZH, HLE, xBench-DeepSearch, and notably FutureX. We hope it could serve as an easily accessible, reproducible, and comparable baseline for the deep research community.", "AI": {"tldr": "MiroFlow\u662f\u4e00\u4e2a\u9ad8\u6027\u80fd\u3001\u5f00\u6e90\u7684\u667a\u80fd\u4f53\u6846\u67b6\uff0c\u901a\u8fc7\u667a\u80fd\u4f53\u56fe\u3001\u6df1\u5ea6\u63a8\u7406\u6a21\u5f0f\u548c\u9c81\u68d2\u5de5\u4f5c\u6d41\u6267\u884c\u6765\u89e3\u51b3\u73b0\u6709LLM\u667a\u80fd\u4f53\u5728\u590d\u6742\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u74f6\u9888\u548c\u7a33\u5b9a\u6027\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5927\u578b\u8bed\u8a00\u6a21\u578b\u53d6\u5f97\u4e86\u663e\u8457\u8fdb\u5c55\uff0c\u4f46\u72ec\u7acbLLM\u5728\u5904\u7406\u9700\u8981\u4e0e\u5916\u90e8\u5de5\u5177\u548c\u52a8\u6001\u73af\u5883\u4ea4\u4e92\u7684\u590d\u6742\u73b0\u5b9e\u4efb\u52a1\u65f6\u80fd\u529b\u5f00\u59cb\u8d8b\u4e8e\u5e73\u7a33\u3002\u73b0\u6709\u7684\u667a\u80fd\u4f53\u6846\u67b6\u5b58\u5728\u5de5\u4f5c\u6d41\u7a0b\u7b80\u5355\u3001\u6027\u80fd\u4e0d\u7a33\u5b9a\u3001\u5bf9\u4e0d\u540c\u57fa\u51c6\u548c\u4efb\u52a1\u652f\u6301\u6709\u9650\u3001\u4ee5\u53ca\u8fc7\u5ea6\u4f9d\u8d56\u6602\u8d35\u7684\u5546\u4e1aAPI\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51faMiroFlow\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u667a\u80fd\u4f53\u56fe\u7528\u4e8e\u7075\u6d3b\u7f16\u6392\uff1b2\uff09\u53ef\u9009\u7684\u6df1\u5ea6\u63a8\u7406\u6a21\u5f0f\u4ee5\u589e\u5f3a\u6027\u80fd\uff1b3\uff09\u9c81\u68d2\u7684\u5de5\u4f5c\u6d41\u6267\u884c\u786e\u4fdd\u7a33\u5b9a\u548c\u53ef\u590d\u73b0\u7684\u6027\u80fd\u3002", "result": "\u5728\u591a\u4e2a\u667a\u80fd\u4f53\u57fa\u51c6\u6d4b\u8bd5\uff08\u5305\u62ecGAIA\u3001BrowseComp-EN/ZH\u3001HLE\u3001xBench-DeepSearch\u548cFutureX\uff09\u4e2d\uff0cMiroFlow\u59cb\u7ec8\u8fbe\u5230\u6700\u5148\u8fdb\u7684\u6027\u80fd\u6c34\u5e73\u3002", "conclusion": "MiroFlow\u53ef\u4ee5\u4f5c\u4e3a\u4e00\u4e2a\u6613\u4e8e\u8bbf\u95ee\u3001\u53ef\u590d\u73b0\u548c\u53ef\u6bd4\u8f83\u7684\u57fa\u51c6\uff0c\u4e3a\u6df1\u5ea6\u7814\u7a76\u793e\u533a\u63d0\u4f9b\u652f\u6301\uff0c\u63a8\u52a8\u667a\u80fd\u4f53\u6280\u672f\u7684\u53d1\u5c55\u3002"}}
{"id": "2602.22814", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2602.22814", "abs": "https://arxiv.org/abs/2602.22814", "authors": ["Soyoung Jung", "Daehoo Yoon", "Sung Gyu Koh", "Young Hwan Kim", "Yehan Ahn", "Sung Park"], "title": "When Should an AI Act? A Human-Centered Model of Scene, Context, and Behavior for Agentic AI Design", "comment": null, "summary": "Agentic AI increasingly intervenes proactively by inferring users' situations from contextual data yet often fails for lack of principled judgment about when, why, and whether to act. We address this gap by proposing a conceptual model that reframes behavior as an interpretive outcome integrating Scene (observable situation), Context (user-constructed meaning), and Human Behavior Factors (determinants shaping behavioral likelihood). Grounded in multidisciplinary perspectives across the humanities, social sciences, HCI, and engineering, the model separates what is observable from what is meaningful to the user and explains how the same scene can yield different behavioral meanings and outcomes. To translate this lens into design action, we derive five agent design principles (behavioral alignment, contextual sensitivity, temporal appropriateness, motivational calibration, and agency preservation) that guide intervention depth, timing, intensity, and restraint. Together, the model and principles provide a foundation for designing agentic AI systems that act with contextual sensitivity and judgment in interactions.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u4e2a\u6982\u5ff5\u6a21\u578b\uff0c\u5c06\u667a\u80fd\u4f53\u884c\u4e3a\u91cd\u65b0\u5b9a\u4e49\u4e3a\u573a\u666f\u3001\u4e0a\u4e0b\u6587\u548c\u4eba\u7c7b\u884c\u4e3a\u56e0\u7d20\u6574\u5408\u7684\u89e3\u91ca\u6027\u7ed3\u679c\uff0c\u5e76\u63a8\u5bfc\u51fa\u4e94\u4e2a\u667a\u80fd\u4f53\u8bbe\u8ba1\u539f\u5219\uff0c\u7528\u4e8e\u6307\u5bfc\u5177\u6709\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u548c\u5224\u65ad\u529b\u7684AI\u7cfb\u7edf\u8bbe\u8ba1\u3002", "motivation": "\u5f53\u524d\u4e3b\u52a8\u578bAI\u7cfb\u7edf\u901a\u8fc7\u4e0a\u4e0b\u6587\u6570\u636e\u63a8\u65ad\u7528\u6237\u60c5\u5883\u8fdb\u884c\u5e72\u9884\uff0c\u4f46\u7f3a\u4e4f\u5173\u4e8e\u4f55\u65f6\u3001\u4e3a\u4f55\u4ee5\u53ca\u662f\u5426\u5e94\u8be5\u91c7\u53d6\u884c\u52a8\u7684\u539f\u5219\u6027\u5224\u65ad\uff0c\u5bfc\u81f4\u7ecf\u5e38\u5931\u8d25\u3002\u9700\u8981\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u63d0\u51fa\u4e00\u4e2a\u6982\u5ff5\u6a21\u578b\uff0c\u5c06\u884c\u4e3a\u91cd\u65b0\u5b9a\u4e49\u4e3a\u89e3\u91ca\u6027\u7ed3\u679c\uff0c\u6574\u5408\u4e09\u4e2a\u8981\u7d20\uff1a\u573a\u666f\uff08\u53ef\u89c2\u5bdf\u7684\u60c5\u5883\uff09\u3001\u4e0a\u4e0b\u6587\uff08\u7528\u6237\u6784\u5efa\u7684\u610f\u4e49\uff09\u548c\u4eba\u7c7b\u884c\u4e3a\u56e0\u7d20\uff08\u5851\u9020\u884c\u4e3a\u53ef\u80fd\u6027\u7684\u51b3\u5b9a\u56e0\u7d20\uff09\u3002\u57fa\u4e8e\u8de8\u5b66\u79d1\u89c6\u89d2\uff0c\u5e76\u63a8\u5bfc\u51fa\u4e94\u4e2a\u667a\u80fd\u4f53\u8bbe\u8ba1\u539f\u5219\u3002", "result": "\u5f00\u53d1\u4e86\u4e00\u4e2a\u7406\u8bba\u6846\u67b6\uff0c\u80fd\u591f\u89e3\u91ca\u76f8\u540c\u573a\u666f\u5982\u4f55\u4ea7\u751f\u4e0d\u540c\u7684\u884c\u4e3a\u610f\u4e49\u548c\u7ed3\u679c\uff0c\u5e76\u63d0\u4f9b\u4e86\u6307\u5bfc\u5e72\u9884\u6df1\u5ea6\u3001\u65f6\u673a\u3001\u5f3a\u5ea6\u548c\u7ea6\u675f\u7684\u8bbe\u8ba1\u539f\u5219\u3002", "conclusion": "\u8be5\u6a21\u578b\u548c\u539f\u5219\u4e3a\u8bbe\u8ba1\u5177\u6709\u4e0a\u4e0b\u6587\u654f\u611f\u6027\u548c\u5224\u65ad\u529b\u7684\u4e3b\u52a8\u578bAI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\uff0c\u4f7fAI\u80fd\u591f\u5728\u4ea4\u4e92\u4e2d\u4ee5\u66f4\u5408\u7406\u7684\u65b9\u5f0f\u884c\u52a8\u3002"}}
{"id": "2602.22689", "categories": ["cs.CV", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.22689", "abs": "https://arxiv.org/abs/2602.22689", "authors": ["Joonsung Jeon", "Woo Jae Kim", "Suhyeon Ha", "Sooel Son", "Sung-Eui Yoon"], "title": "No Caption, No Problem: Caption-Free Membership Inference via Model-Fitted Embeddings", "comment": "Accepted to ICLR 2026", "summary": "Latent diffusion models have achieved remarkable success in high-fidelity text-to-image generation, but their tendency to memorize training data raises critical privacy and intellectual property concerns. Membership inference attacks (MIAs) provide a principled way to audit such memorization by determining whether a given sample was included in training. However, existing approaches assume access to ground-truth captions. This assumption fails in realistic scenarios where only images are available and their textual annotations remain undisclosed, rendering prior methods ineffective when substituted with vision-language model (VLM) captions. In this work, we propose MoFit, a caption-free MIA framework that constructs synthetic conditioning inputs that are explicitly overfitted to the target model's generative manifold. Given a query image, MoFit proceeds in two stages: (i) model-fitted surrogate optimization, where a perturbation applied to the image is optimized to construct a surrogate in regions of the model's unconditional prior learned from member samples, and (ii) surrogate-driven embedding extraction, where a model-fitted embedding is derived from the surrogate and then used as a mismatched condition for the query image. This embedding amplifies conditional loss responses for member samples while leaving hold-outs relatively less affected, thereby enhancing separability in the absence of ground-truth captions. Our comprehensive experiments across multiple datasets and diffusion models demonstrate that MoFit consistently outperforms prior VLM-conditioned baselines and achieves performance competitive with caption-dependent methods.", "AI": {"tldr": "MoFit\uff1a\u65e0\u9700\u771f\u5b9e\u6807\u6ce8\u7684\u6210\u5458\u63a8\u65ad\u653b\u51fb\u6846\u67b6\uff0c\u901a\u8fc7\u6784\u5efa\u4e0e\u76ee\u6807\u6a21\u578b\u751f\u6210\u6d41\u5f62\u8fc7\u62df\u5408\u7684\u5408\u6210\u6761\u4ef6\u8f93\u5165\u6765\u68c0\u6d4b\u6269\u6563\u6a21\u578b\u7684\u6570\u636e\u8bb0\u5fc6\u95ee\u9898", "motivation": "\u73b0\u6709\u6210\u5458\u63a8\u65ad\u653b\u51fb\u65b9\u6cd5\u4f9d\u8d56\u771f\u5b9e\u6587\u672c\u6807\u6ce8\uff0c\u4f46\u5728\u5b9e\u9645\u573a\u666f\u4e2d\u53ea\u6709\u56fe\u50cf\u53ef\u7528\u4e14\u6587\u672c\u6807\u6ce8\u672a\u516c\u5f00\uff0c\u8fd9\u4f7f\u5f97\u5148\u524d\u65b9\u6cd5\u5728\u4ec5\u4f7f\u7528\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6807\u6ce8\u65f6\u6548\u679c\u4e0d\u4f73", "method": "\u4e24\u9636\u6bb5\u6846\u67b6\uff1a1) \u6a21\u578b\u62df\u5408\u7684\u4ee3\u7406\u4f18\u5316 - \u4f18\u5316\u56fe\u50cf\u6270\u52a8\u4ee5\u6784\u5efa\u5728\u6210\u5458\u6837\u672c\u5b66\u4e60\u7684\u65e0\u6761\u4ef6\u5148\u9a8c\u533a\u57df\u4e2d\u7684\u4ee3\u7406\uff1b2) \u4ee3\u7406\u9a71\u52a8\u7684\u5d4c\u5165\u63d0\u53d6 - \u4ece\u4ee3\u7406\u5bfc\u51fa\u6a21\u578b\u62df\u5408\u5d4c\u5165\uff0c\u4f5c\u4e3a\u67e5\u8be2\u56fe\u50cf\u7684\u5931\u914d\u6761\u4ef6\uff0c\u653e\u5927\u6210\u5458\u6837\u672c\u7684\u6761\u4ef6\u635f\u5931\u54cd\u5e94", "result": "\u5728\u591a\u4e2a\u6570\u636e\u96c6\u548c\u6269\u6563\u6a21\u578b\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cMoFit\u59cb\u7ec8\u4f18\u4e8e\u5148\u524d\u7684VLM\u6761\u4ef6\u57fa\u7ebf\u65b9\u6cd5\uff0c\u5e76\u8fbe\u5230\u4e0e\u4f9d\u8d56\u6807\u6ce8\u65b9\u6cd5\u76f8\u7ade\u4e89\u7684\u6027\u80fd", "conclusion": "MoFit\u4e3a\u65e0\u9700\u771f\u5b9e\u6807\u6ce8\u7684\u6210\u5458\u63a8\u65ad\u653b\u51fb\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\uff0c\u80fd\u591f\u68c0\u6d4b\u6269\u6563\u6a21\u578b\u7684\u6570\u636e\u8bb0\u5fc6\u95ee\u9898\uff0c\u5728\u9690\u79c1\u548c\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u65b9\u9762\u5177\u6709\u91cd\u8981\u610f\u4e49"}}
{"id": "2602.23062", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23062", "abs": "https://arxiv.org/abs/2602.23062", "authors": ["Gabriela Anna Kaczmarek", "Pietro Ferrazzi", "Lorenzo Porta", "Vicky Rubini", "Bernardo Magnini"], "title": "Toward Automatic Filling of Case Report Forms: A Case Study on Data from an Italian Emergency Department", "comment": null, "summary": "Case Report Forms (CRFs) collect data about patients and are at the core of well-established practices to conduct research in clinical settings. With the recent progress of language technologies, there is an increasing interest in automatic CRF-filling from clinical notes, mostly based on the use of Large Language Models (LLMs). However, there is a general scarcity of annotated CRF data, both for training and testing LLMs, which limits the progress on this task. As a step in the direction of providing such data, we present a new dataset of clinical notes from an Italian Emergency Department annotated with respect to a pre-defined CRF containing 134 items to be filled. We provide an analysis of the data, define the CRF-filling task and metric for its evaluation, and report on pilot experiments where we use an open-source state-of-the-art LLM to automatically execute the task. Results of the case-study show that (i) CRF-filling from real clinical notes in Italian can be approached in a zero-shot setting; (ii) LLMs' results are affected by biases (e.g., a cautious behaviour favours \"unknown\" answers), which need to be corrected.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u610f\u5927\u5229\u6025\u8bca\u79d1\u4e34\u5e8a\u7b14\u8bb0\u6570\u636e\u96c6\uff0c\u7528\u4e8e\u81ea\u52a8\u586b\u5199\u75c5\u4f8b\u62a5\u544a\u8868\uff08CRF\uff09\uff0c\u5e76\u5c55\u793a\u4e86\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u4f7f\u7528\u5f00\u6e90LLM\u8fdb\u884cCRF\u586b\u5199\u7684\u521d\u6b65\u5b9e\u9a8c\u7ed3\u679c\u3002", "motivation": "\u4e34\u5e8a\u7814\u7a76\u4e2dCRF\u6570\u636e\u6536\u96c6\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u7f3a\u4e4f\u6807\u6ce8\u6570\u636e\u9650\u5236\u4e86\u57fa\u4e8eLLM\u7684\u81ea\u52a8CRF\u586b\u5199\u6280\u672f\u7684\u53d1\u5c55\u3002\u9700\u8981\u66f4\u591a\u6807\u6ce8\u6570\u636e\u6765\u8bad\u7ec3\u548c\u6d4b\u8bd5LLM\u3002", "method": "\u521b\u5efa\u610f\u5927\u5229\u6025\u8bca\u79d1\u4e34\u5e8a\u7b14\u8bb0\u6570\u636e\u96c6\uff0c\u5305\u542b134\u4e2aCRF\u9879\u76ee\u7684\u6807\u6ce8\u3002\u5b9a\u4e49CRF\u586b\u5199\u4efb\u52a1\u548c\u8bc4\u4f30\u6307\u6807\uff0c\u4f7f\u7528\u5f00\u6e90\u6700\u5148\u8fdbLLM\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8fdb\u884c\u5b9e\u9a8c\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a(1) \u610f\u5927\u5229\u8bed\u4e34\u5e8a\u7b14\u8bb0\u7684CRF\u586b\u5199\u53ef\u4ee5\u5728\u96f6\u6837\u672c\u8bbe\u7f6e\u4e0b\u8fdb\u884c\uff1b(2) LLM\u7ed3\u679c\u5b58\u5728\u504f\u5dee\uff08\u5982\u503e\u5411\u4e8e\"\u672a\u77e5\"\u7b54\u6848\uff09\uff0c\u9700\u8981\u4fee\u6b63\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aCRF\u586b\u5199\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u4ef7\u503c\u7684\u6807\u6ce8\u6570\u636e\u96c6\uff0c\u5c55\u793a\u4e86LLM\u5728\u8be5\u4efb\u52a1\u4e0a\u7684\u6f5c\u529b\uff0c\u540c\u65f6\u6307\u51fa\u4e86\u9700\u8981\u89e3\u51b3\u7684\u504f\u5dee\u95ee\u9898\u3002"}}
{"id": "2602.22822", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.22822", "abs": "https://arxiv.org/abs/2602.22822", "authors": ["Yunhua Zhong", "Yixuan Tang", "Yifan Li", "Jie Yang", "Pan Liu", "Jun Xia"], "title": "FlexMS is a flexible framework for benchmarking deep learning-based mass spectrum prediction tools in metabolomics", "comment": "28 pages, preprint version", "summary": "The identification and property prediction of chemical molecules is of central importance in the advancement of drug discovery and material science, where the tandem mass spectrometry technology gives valuable fragmentation cues in the form of mass-to-charge ratio peaks. However, the lack of experimental spectra hinders the attachment of each molecular identification, and thus urges the establishment of prediction approaches for computational models. Deep learning models appear promising for predicting molecular structure spectra, but overall assessment remains challenging as a result of the heterogeneity in methods and the lack of well-defined benchmarks. To address this, our contribution is the creation of benchmark framework FlexMS for constructing and evaluating diverse model architectures in mass spectrum prediction. With its easy-to-use flexibility, FlexMS supports the dynamic construction of numerous distinct combinations of model architectures, while assessing their performance on preprocessed public datasets using different metrics. In this paper, we provide insights into factors influencing performance, including the structural diversity of datasets, hyperparameters like learning rate and data sparsity, pretraining effects, metadata ablation settings and cross-domain transfer learning analysis. This provides practical guidance in choosing suitable models. Moreover, retrieval benchmarks simulate practical identification scenarios and score potential matches based on predicted spectra.", "AI": {"tldr": "FlexMS\u662f\u4e00\u4e2a\u7528\u4e8e\u8d28\u8c31\u9884\u6d4b\u7684\u57fa\u51c6\u6846\u67b6\uff0c\u652f\u6301\u6784\u5efa\u548c\u8bc4\u4f30\u591a\u79cd\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u67b6\u6784\uff0c\u63d0\u4f9b\u6027\u80fd\u5f71\u54cd\u56e0\u7d20\u5206\u6790\u548c\u5b9e\u9645\u5e94\u7528\u6307\u5bfc\u3002", "motivation": "\u8d28\u8c31\u6280\u672f\u5728\u836f\u7269\u53d1\u73b0\u548c\u6750\u6599\u79d1\u5b66\u4e2d\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u5b9e\u9a8c\u5149\u8c31\u6570\u636e\u7f3a\u4e4f\u963b\u788d\u4e86\u5206\u5b50\u8bc6\u522b\u3002\u73b0\u6709\u6df1\u5ea6\u5b66\u4e60\u6a21\u578b\u8bc4\u4f30\u56f0\u96be\uff0c\u7f3a\u4e4f\u7edf\u4e00\u57fa\u51c6\u3002", "method": "\u521b\u5efaFlexMS\u57fa\u51c6\u6846\u67b6\uff0c\u652f\u6301\u52a8\u6001\u6784\u5efa\u591a\u79cd\u6a21\u578b\u67b6\u6784\u7ec4\u5408\uff0c\u5728\u9884\u5904\u7406\u516c\u5171\u6570\u636e\u96c6\u4e0a\u4f7f\u7528\u4e0d\u540c\u6307\u6807\u8bc4\u4f30\u6027\u80fd\u3002", "result": "\u5206\u6790\u4e86\u5f71\u54cd\u6027\u80fd\u7684\u56e0\u7d20\uff1a\u6570\u636e\u96c6\u7ed3\u6784\u591a\u6837\u6027\u3001\u8d85\u53c2\u6570\uff08\u5b66\u4e60\u7387\u3001\u6570\u636e\u7a00\u758f\u6027\uff09\u3001\u9884\u8bad\u7ec3\u6548\u679c\u3001\u5143\u6570\u636e\u6d88\u878d\u8bbe\u7f6e\u548c\u8de8\u57df\u8fc1\u79fb\u5b66\u4e60\u5206\u6790\u3002", "conclusion": "FlexMS\u4e3a\u8d28\u8c31\u9884\u6d4b\u63d0\u4f9b\u4e86\u5b9e\u7528\u7684\u6a21\u578b\u9009\u62e9\u6307\u5bfc\uff0c\u5e76\u901a\u8fc7\u68c0\u7d22\u57fa\u51c6\u6a21\u62df\u5b9e\u9645\u8bc6\u522b\u573a\u666f\uff0c\u57fa\u4e8e\u9884\u6d4b\u5149\u8c31\u8bc4\u5206\u6f5c\u5728\u5339\u914d\u3002"}}
{"id": "2602.22695", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22695", "abs": "https://arxiv.org/abs/2602.22695", "authors": ["Yu Chen", "Zewei He", "Xingyu Liu", "Zixuan Chen", "Zheming Lu"], "title": "GFRRN: Explore the Gaps in Single Image Reflection Removal", "comment": "CVPR26", "summary": "Prior dual-stream methods with the feature interaction mechanism have achieved remarkable performance in single image reflection removal (SIRR). However, they often struggle with (1) semantic understanding gap between the features of pre-trained models and those of reflection removal models, and (2) reflection label inconsistencies between synthetic and real-world training data. In this work, we first adopt the parameter efficient fine-tuning (PEFT) strategy by integrating several learnable Mona layers into the pre-trained model to align the training directions. Then, a label generator is designed to unify the reflection labels for both synthetic and real-world data. In addition, a Gaussian-based Adaptive Frequency Learning Block (G-AFLB) is proposed to adaptively learn and fuse the frequency priors, and a Dynamic Agent Attention (DAA) is employed as an alternative to window-based attention by dynamically modeling the significance levels across windows (inter-) and within an individual window (intra-). These components constitute our proposed Gap-Free Reflection Removal Network (GFRRN). Extensive experiments demonstrate the effectiveness of our GFRRN, achieving superior performance against state-of-the-art SIRR methods.", "AI": {"tldr": "\u63d0\u51faGFRRN\u7f51\u7edc\u89e3\u51b3\u5355\u56fe\u50cf\u53bb\u53cd\u5c04\u4e2d\u7684\u7279\u5f81\u8bed\u4e49\u7406\u89e3\u5dee\u8ddd\u548c\u53cd\u5c04\u6807\u7b7e\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u901a\u8fc7PEFT\u5fae\u8c03\u3001\u7edf\u4e00\u6807\u7b7e\u751f\u6210\u5668\u3001\u81ea\u9002\u5e94\u9891\u7387\u5b66\u4e60\u548c\u52a8\u6001\u4ee3\u7406\u6ce8\u610f\u529b\u673a\u5236\u5b9e\u73b0\u4f18\u8d8a\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u53cc\u6d41\u65b9\u6cd5\u5728\u5355\u56fe\u50cf\u53cd\u5c04\u53bb\u9664\u4e2d\u5b58\u5728\u4e24\u4e2a\u4e3b\u8981\u95ee\u9898\uff1a(1)\u9884\u8bad\u7ec3\u6a21\u578b\u7279\u5f81\u4e0e\u53bb\u53cd\u5c04\u6a21\u578b\u7279\u5f81\u4e4b\u95f4\u7684\u8bed\u4e49\u7406\u89e3\u5dee\u8ddd\uff1b(2)\u5408\u6210\u6570\u636e\u4e0e\u771f\u5b9e\u4e16\u754c\u8bad\u7ec3\u6570\u636e\u4e4b\u95f4\u7684\u53cd\u5c04\u6807\u7b7e\u4e0d\u4e00\u81f4\u3002", "method": "1. \u91c7\u7528\u53c2\u6570\u9ad8\u6548\u5fae\u8c03(PEFT)\u7b56\u7565\uff0c\u5728\u9884\u8bad\u7ec3\u6a21\u578b\u4e2d\u96c6\u6210\u53ef\u5b66\u4e60\u7684Mona\u5c42\u4ee5\u5bf9\u9f50\u8bad\u7ec3\u65b9\u5411\uff1b2. \u8bbe\u8ba1\u6807\u7b7e\u751f\u6210\u5668\u7edf\u4e00\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u7684\u53cd\u5c04\u6807\u7b7e\uff1b3. \u63d0\u51fa\u9ad8\u65af\u81ea\u9002\u5e94\u9891\u7387\u5b66\u4e60\u5757(G-AFLB)\u81ea\u9002\u5e94\u5b66\u4e60\u548c\u878d\u5408\u9891\u7387\u5148\u9a8c\uff1b4. \u91c7\u7528\u52a8\u6001\u4ee3\u7406\u6ce8\u610f\u529b(DAA)\u66ff\u4ee3\u57fa\u4e8e\u7a97\u53e3\u7684\u6ce8\u610f\u529b\uff0c\u52a8\u6001\u5efa\u6a21\u7a97\u53e3\u95f4\u548c\u7a97\u53e3\u5185\u7684\u91cd\u8981\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8bc1\u660eGFRRN\u7684\u6709\u6548\u6027\uff0c\u5728\u5355\u56fe\u50cf\u53cd\u5c04\u53bb\u9664\u4efb\u52a1\u4e2d\u5b9e\u73b0\u4e86\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u7684\u6027\u80fd\u3002", "conclusion": "\u63d0\u51fa\u7684GFRRN\u7f51\u7edc\u901a\u8fc7\u89e3\u51b3\u7279\u5f81\u8bed\u4e49\u5dee\u8ddd\u548c\u6807\u7b7e\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u7ed3\u5408PEFT\u5fae\u8c03\u3001\u7edf\u4e00\u6807\u7b7e\u751f\u6210\u3001\u81ea\u9002\u5e94\u9891\u7387\u5b66\u4e60\u548c\u52a8\u6001\u6ce8\u610f\u529b\u673a\u5236\uff0c\u5728\u5355\u56fe\u50cf\u53cd\u5c04\u53bb\u9664\u4efb\u52a1\u4e2d\u53d6\u5f97\u4e86\u5353\u8d8a\u6027\u80fd\u3002"}}
{"id": "2602.22839", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22839", "abs": "https://arxiv.org/abs/2602.22839", "authors": ["Hao Zheng", "Guozhao Mo", "Xinru Yan", "Qianhao Yuan", "Wenkai Zhang", "Xuanang Chen", "Yaojie Lu", "Hongyu Lin", "Xianpei Han", "Le Sun"], "title": "DeepPresenter: Environment-Grounded Reflection for Agentic Presentation Generation", "comment": null, "summary": "Presentation generation requires deep content research, coherent visual design, and iterative refinement based on observation. However, existing presentation agents often rely on predefined workflows and fixed templates. To address this, we present DeepPresenter, an agentic framework that adapts to diverse user intents, enables effective feedback-driven refinement, and generalizes beyond a scripted pipeline. Specifically, DeepPresenter autonomously plans, renders, and revises intermediate slide artifacts to support long-horizon refinement with environmental observations. Furthermore, rather than relying on self-reflection over internal signals (e.g., reasoning traces), our environment-grounded reflection conditions the generation process on perceptual artifact states (e.g., rendered slides), enabling the system to identify and correct presentation-specific issues during execution. Results on the evaluation set covering diverse presentation-generation scenarios show that DeepPresenter achieves state-of-the-art performance, and the fine-tuned 9B model remains highly competitive at substantially lower cost. Our project is available at: https://github.com/icip-cas/PPTAgent", "AI": {"tldr": "DeepPresenter\u662f\u4e00\u4e2a\u81ea\u9002\u5e94\u6f14\u793a\u751f\u6210\u6846\u67b6\uff0c\u901a\u8fc7\u73af\u5883\u611f\u77e5\u7684\u53cd\u601d\u673a\u5236\u548c\u957f\u65f6\u7a0b\u8fed\u4ee3\u4f18\u5316\uff0c\u8d85\u8d8a\u4f20\u7edf\u57fa\u4e8e\u6a21\u677f\u7684\u65b9\u6cd5\uff0c\u5728\u591a\u6837\u5316\u573a\u666f\u4e2d\u5b9e\u73b0SOTA\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u6f14\u793a\u751f\u6210\u4ee3\u7406\u901a\u5e38\u4f9d\u8d56\u9884\u5b9a\u4e49\u5de5\u4f5c\u6d41\u548c\u56fa\u5b9a\u6a21\u677f\uff0c\u7f3a\u4e4f\u5bf9\u7528\u6237\u591a\u6837\u5316\u610f\u56fe\u7684\u9002\u5e94\u80fd\u529b\uff0c\u4e14\u96be\u4ee5\u8fdb\u884c\u6709\u6548\u7684\u53cd\u9988\u9a71\u52a8\u4f18\u5316\u3002", "method": "\u63d0\u51faDeepPresenter\u6846\u67b6\uff0c\u81ea\u4e3b\u89c4\u5212\u3001\u6e32\u67d3\u548c\u4fee\u8ba2\u4e2d\u95f4\u5e7b\u706f\u7247\u5de5\u4ef6\uff0c\u652f\u6301\u57fa\u4e8e\u73af\u5883\u89c2\u5bdf\u7684\u957f\u65f6\u7a0b\u4f18\u5316\uff1b\u91c7\u7528\u73af\u5883\u63a5\u5730\u7684\u53cd\u601d\u673a\u5236\uff0c\u57fa\u4e8e\u611f\u77e5\u5de5\u4ef6\u72b6\u6001\uff08\u5982\u6e32\u67d3\u7684\u5e7b\u706f\u7247\uff09\u800c\u975e\u5185\u90e8\u4fe1\u53f7\u6765\u6307\u5bfc\u751f\u6210\u8fc7\u7a0b\u3002", "result": "\u5728\u591a\u6837\u5316\u6f14\u793a\u751f\u6210\u573a\u666f\u7684\u8bc4\u4f30\u96c6\u4e0a\uff0cDeepPresenter\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff1b\u5fae\u8c03\u76849B\u6a21\u578b\u5728\u663e\u8457\u964d\u4f4e\u6210\u672c\u7684\u540c\u65f6\u4fdd\u6301\u9ad8\u5ea6\u7ade\u4e89\u529b\u3002", "conclusion": "DeepPresenter\u901a\u8fc7\u73af\u5883\u63a5\u5730\u7684\u53cd\u601d\u548c\u81ea\u9002\u5e94\u8fed\u4ee3\u4f18\u5316\uff0c\u4e3a\u6f14\u793a\u751f\u6210\u63d0\u4f9b\u4e86\u66f4\u7075\u6d3b\u3001\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u8d85\u8d8a\u4e86\u4f20\u7edf\u57fa\u4e8e\u6a21\u677f\u7684\u65b9\u6cd5\u3002"}}
{"id": "2602.22712", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22712", "abs": "https://arxiv.org/abs/2602.22712", "authors": ["Yuankai Chen", "Kai Lin", "Qihong Wu", "Xinxuan Yang", "Jiashuo Lai", "Ruoen Chen", "Haonan Shi", "Minfan He", "Meihua Wang"], "title": "UFO-DETR: Frequency-Guided End-to-End Detector for UAV Tiny Objects", "comment": "6 pages, 6 figures, published to 2026 International Conference on Computer Supported Cooperative Work in Design", "summary": "Small target detection in UAV imagery faces significant challenges such as scale variations, dense distribution, and the dominance of small targets. Existing algorithms rely on manually designed components, and general-purpose detectors are not optimized for UAV images, making it difficult to balance accuracy and complexity. To address these challenges, this paper proposes an end-to-end object detection framework, UFO-DETR, which integrates an LSKNet-based backbone network to optimize the receptive field and reduce the number of parameters. By combining the DAttention and AIFI modules, the model flexibly models multi-scale spatial relationships, improving multi-scale target detection performance. Additionally, the DynFreq-C3 module is proposed to enhance small target detection capability through cross-space frequency feature enhancement. Experimental results show that, compared to RT-DETR-L, the proposed method offers significant advantages in both detection performance and computational efficiency, providing an efficient solution for UAV edge computing.", "AI": {"tldr": "\u63d0\u51faUFO-DETR\u68c0\u6d4b\u6846\u67b6\uff0c\u9488\u5bf9\u65e0\u4eba\u673a\u56fe\u50cf\u5c0f\u76ee\u6807\u68c0\u6d4b\u95ee\u9898\uff0c\u901a\u8fc7LSKNet\u9aa8\u5e72\u7f51\u7edc\u3001DAttention/AIFI\u6a21\u5757\u548cDynFreq-C3\u6a21\u5757\uff0c\u5728\u68c0\u6d4b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u4f18\u4e8eRT-DETR-L\u3002", "motivation": "\u65e0\u4eba\u673a\u56fe\u50cf\u5c0f\u76ee\u6807\u68c0\u6d4b\u9762\u4e34\u5c3a\u5ea6\u53d8\u5316\u5927\u3001\u5206\u5e03\u5bc6\u96c6\u3001\u5c0f\u76ee\u6807\u5360\u4e3b\u5bfc\u7b49\u6311\u6218\u3002\u73b0\u6709\u7b97\u6cd5\u4f9d\u8d56\u4eba\u5de5\u8bbe\u8ba1\u7ec4\u4ef6\uff0c\u901a\u7528\u68c0\u6d4b\u5668\u672a\u9488\u5bf9\u65e0\u4eba\u673a\u56fe\u50cf\u4f18\u5316\uff0c\u96be\u4ee5\u5e73\u8861\u7cbe\u5ea6\u548c\u590d\u6742\u5ea6\u3002", "method": "\u63d0\u51fa\u7aef\u5230\u7aef\u76ee\u6807\u68c0\u6d4b\u6846\u67b6UFO-DETR\uff1a1) \u57fa\u4e8eLSKNet\u7684\u9aa8\u5e72\u7f51\u7edc\u4f18\u5316\u611f\u53d7\u91ce\u5e76\u51cf\u5c11\u53c2\u6570\uff1b2) \u7ed3\u5408DAttention\u548cAIFI\u6a21\u5757\u7075\u6d3b\u5efa\u6a21\u591a\u5c3a\u5ea6\u7a7a\u95f4\u5173\u7cfb\uff1b3) \u63d0\u51faDynFreq-C3\u6a21\u5757\u901a\u8fc7\u8de8\u7a7a\u95f4\u9891\u7387\u7279\u5f81\u589e\u5f3a\u63d0\u5347\u5c0f\u76ee\u6807\u68c0\u6d4b\u80fd\u529b\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u76f8\u6bd4RT-DETR-L\uff0c\u8be5\u65b9\u6cd5\u5728\u68c0\u6d4b\u6027\u80fd\u548c\u8ba1\u7b97\u6548\u7387\u4e0a\u90fd\u6709\u663e\u8457\u4f18\u52bf\uff0c\u4e3a\u65e0\u4eba\u673a\u8fb9\u7f18\u8ba1\u7b97\u63d0\u4f9b\u4e86\u9ad8\u6548\u89e3\u51b3\u65b9\u6848\u3002", "conclusion": "UFO-DETR\u6846\u67b6\u6709\u6548\u89e3\u51b3\u4e86\u65e0\u4eba\u673a\u56fe\u50cf\u5c0f\u76ee\u6807\u68c0\u6d4b\u7684\u6311\u6218\uff0c\u5728\u7cbe\u5ea6\u548c\u6548\u7387\u4e0a\u53d6\u5f97\u826f\u597d\u5e73\u8861\uff0c\u9002\u7528\u4e8e\u8fb9\u7f18\u8ba1\u7b97\u573a\u666f\u3002"}}
{"id": "2602.23075", "categories": ["cs.CL", "cs.IR"], "pdf": "https://arxiv.org/pdf/2602.23075", "abs": "https://arxiv.org/abs/2602.23075", "authors": ["Mengze Hong", "Di Jiang", "Chen Jason Zhang", "Zichang Guo", "Yawen Li", "Jun Chen", "Shaobo Cui", "Zhiyang Su"], "title": "CiteLLM: An Agentic Platform for Trustworthy Scientific Reference Discovery", "comment": "Accepted by TheWebConf 2026 Demo Track", "summary": "Large language models (LLMs) have created new opportunities to enhance the efficiency of scholarly activities; however, challenges persist in the ethical deployment of AI assistance, including (1) the trustworthiness of AI-generated content, (2) preservation of academic integrity and intellectual property, and (3) protection of information privacy. In this work, we present CiteLLM, a specialized agentic platform designed to enable trustworthy reference discovery for grounding author-drafted claims and statements. The system introduces a novel interaction paradigm by embedding LLM utilities directly within the LaTeX editor environment, ensuring a seamless user experience and no data transmission outside the local system. To guarantee hallucination-free references, we employ dynamic discipline-aware routing to retrieve candidates exclusively from trusted web-based academic repositories, while leveraging LLMs solely for generating context-aware search queries, ranking candidates by relevance, and validating and explaining support through paragraph-level semantic matching and an integrated chatbot. Evaluation results demonstrate the superior performance of the proposed system in returning valid and highly usable references.", "AI": {"tldr": "CiteLLM\u662f\u4e00\u4e2a\u5728LaTeX\u7f16\u8f91\u5668\u4e2d\u96c6\u6210\u7684AI\u4ee3\u7406\u5e73\u53f0\uff0c\u4e13\u95e8\u7528\u4e8e\u53ef\u4fe1\u7684\u53c2\u8003\u6587\u732e\u53d1\u73b0\uff0c\u901a\u8fc7\u672c\u5730\u5316\u5904\u7406\u548c\u53ef\u4fe1\u5b66\u672f\u8d44\u6e90\u68c0\u7d22\u6765\u907f\u514dAI\u5e7b\u89c9\u95ee\u9898\u3002", "motivation": "\u5c3d\u7ba1\u5927\u8bed\u8a00\u6a21\u578b\u80fd\u63d0\u5347\u5b66\u672f\u6d3b\u52a8\u6548\u7387\uff0c\u4f46\u5728AI\u8f85\u52a9\u7684\u4f26\u7406\u90e8\u7f72\u65b9\u9762\u5b58\u5728\u6311\u6218\uff1aAI\u751f\u6210\u5185\u5bb9\u7684\u53ef\u4fe1\u5ea6\u3001\u5b66\u672f\u8bda\u4fe1\u4e0e\u77e5\u8bc6\u4ea7\u6743\u4fdd\u62a4\u3001\u4fe1\u606f\u9690\u79c1\u4fdd\u62a4\u3002\u9700\u8981\u89e3\u51b3\u8fd9\u4e9b\u95ee\u9898\u7684\u53ef\u4fe1\u53c2\u8003\u6587\u732e\u53d1\u73b0\u7cfb\u7edf\u3002", "method": "1. \u5728LaTeX\u7f16\u8f91\u5668\u4e2d\u76f4\u63a5\u5d4c\u5165LLM\u529f\u80fd\uff0c\u5b9e\u73b0\u65e0\u7f1d\u7528\u6237\u4f53\u9a8c\u4e14\u6570\u636e\u4e0d\u79bb\u5f00\u672c\u5730\u7cfb\u7edf\uff1b2. \u91c7\u7528\u52a8\u6001\u5b66\u79d1\u611f\u77e5\u8def\u7531\u4ece\u53ef\u4fe1\u7684\u57fa\u4e8e\u7f51\u7edc\u7684\u5b66\u672f\u8d44\u6e90\u5e93\u68c0\u7d22\u5019\u9009\u6587\u732e\uff1b3. LLM\u4ec5\u7528\u4e8e\u751f\u6210\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u641c\u7d22\u67e5\u8be2\u3001\u6309\u76f8\u5173\u6027\u6392\u5e8f\u5019\u9009\u6587\u732e\u3001\u901a\u8fc7\u6bb5\u843d\u7ea7\u8bed\u4e49\u5339\u914d\u548c\u96c6\u6210\u804a\u5929\u673a\u5668\u4eba\u8fdb\u884c\u9a8c\u8bc1\u548c\u89e3\u91ca\u652f\u6301\u3002", "result": "\u8bc4\u4f30\u7ed3\u679c\u8868\u660e\uff0c\u6240\u63d0\u51fa\u7684\u7cfb\u7edf\u5728\u8fd4\u56de\u6709\u6548\u4e14\u9ad8\u5ea6\u53ef\u7528\u7684\u53c2\u8003\u6587\u732e\u65b9\u9762\u8868\u73b0\u51fa\u4f18\u8d8a\u6027\u80fd\u3002", "conclusion": "CiteLLM\u901a\u8fc7\u5c06LLM\u529f\u80fd\u76f4\u63a5\u5d4c\u5165LaTeX\u7f16\u8f91\u5668\uff0c\u7ed3\u5408\u53ef\u4fe1\u5b66\u672f\u8d44\u6e90\u68c0\u7d22\u548c\u672c\u5730\u5316\u5904\u7406\uff0c\u5b9e\u73b0\u4e86\u53ef\u4fe1\u7684\u53c2\u8003\u6587\u732e\u53d1\u73b0\uff0c\u89e3\u51b3\u4e86AI\u8f85\u52a9\u5b66\u672f\u5199\u4f5c\u4e2d\u7684\u4f26\u7406\u6311\u6218\u3002"}}
{"id": "2602.22842", "categories": ["cs.AI", "cs.CE", "math.NA"], "pdf": "https://arxiv.org/pdf/2602.22842", "abs": "https://arxiv.org/abs/2602.22842", "authors": ["Tan Bui-Thanh"], "title": "The AI Research Assistant: Promise, Peril, and a Proof of Concept", "comment": "11 pages, 1 figure", "summary": "Can artificial intelligence truly contribute to creative mathematical research, or does it merely automate routine calculations while introducing risks of error? We provide empirical evidence through a detailed case study: the discovery of novel error representations and bounds for Hermite quadrature rules via systematic human-AI collaboration.\n  Working with multiple AI assistants, we extended results beyond what manual work achieved, formulating and proving several theorems with AI assistance. The collaboration revealed both remarkable capabilities and critical limitations. AI excelled at algebraic manipulation, systematic proof exploration, literature synthesis, and LaTeX preparation. However, every step required rigorous human verification, mathematical intuition for problem formulation, and strategic direction.\n  We document the complete research workflow with unusual transparency, revealing patterns in successful human-AI mathematical collaboration and identifying failure modes researchers must anticipate. Our experience suggests that, when used with appropriate skepticism and verification protocols, AI tools can meaningfully accelerate mathematical discovery while demanding careful human oversight and deep domain expertise.", "AI": {"tldr": "AI\u5728\u521b\u9020\u6027\u6570\u5b66\u7814\u7a76\u4e2d\u80fd\u771f\u6b63\u53d1\u6325\u4f5c\u7528\uff0c\u4f46\u9700\u8981\u4eba\u7c7b\u4e25\u683c\u9a8c\u8bc1\u548c\u6307\u5bfc\u3002\u901a\u8fc7Hermite\u6c42\u79ef\u89c4\u5219\u8bef\u5dee\u8868\u793a\u4e0e\u754c\u7684\u6848\u4f8b\u7814\u7a76\uff0c\u5c55\u793a\u4e86\u4eba\u673a\u534f\u4f5c\u80fd\u8d85\u8d8a\u7eaf\u4eba\u5de5\u6210\u679c\uff0c\u4f46AI\u7684\u6bcf\u4e00\u6b65\u90fd\u9700\u8981\u4eba\u7c7b\u9a8c\u8bc1\u548c\u6570\u5b66\u76f4\u89c9\u3002", "motivation": "\u63a2\u7d22\u4eba\u5de5\u667a\u80fd\u662f\u5426\u771f\u6b63\u80fd\u4fc3\u8fdb\u521b\u9020\u6027\u6570\u5b66\u7814\u7a76\uff0c\u8fd8\u662f\u4ec5\u4ec5\u81ea\u52a8\u5316\u5e38\u89c4\u8ba1\u7b97\u5e76\u5f15\u5165\u9519\u8bef\u98ce\u9669\u3002\u901a\u8fc7\u5b9e\u8bc1\u6848\u4f8b\u7814\u7a76\u4eba\u673a\u534f\u4f5c\u5728\u6570\u5b66\u53d1\u73b0\u4e2d\u7684\u5b9e\u9645\u6548\u679c\u3002", "method": "\u91c7\u7528\u7cfb\u7edf\u5316\u4eba\u673a\u534f\u4f5c\u65b9\u6cd5\uff0c\u4e0e\u591a\u4e2aAI\u52a9\u624b\u5408\u4f5c\u7814\u7a76Hermite\u6c42\u79ef\u89c4\u5219\u7684\u8bef\u5dee\u8868\u793a\u4e0e\u754c\u3002\u5b8c\u6574\u8bb0\u5f55\u7814\u7a76\u6d41\u7a0b\uff0c\u5305\u62ec\u5b9a\u7406\u7684\u516c\u5f0f\u5316\u548c\u8bc1\u660e\u8fc7\u7a0b\u3002", "result": "\u6210\u529f\u53d1\u73b0\u4e86\u65b0\u9896\u7684\u8bef\u5dee\u8868\u793a\u548c\u754c\uff0c\u8d85\u8d8a\u4e86\u7eaf\u4eba\u5de5\u5de5\u4f5c\u7684\u6210\u679c\u3002AI\u5728\u4ee3\u6570\u64cd\u4f5c\u3001\u7cfb\u7edf\u5316\u8bc1\u660e\u63a2\u7d22\u3001\u6587\u732e\u7efc\u5408\u548cLaTeX\u51c6\u5907\u65b9\u9762\u8868\u73b0\u51fa\u8272\uff0c\u4f46\u6bcf\u4e00\u6b65\u90fd\u9700\u8981\u4eba\u7c7b\u4e25\u683c\u9a8c\u8bc1\u3001\u6570\u5b66\u76f4\u89c9\u548c\u6218\u7565\u6307\u5bfc\u3002", "conclusion": "\u5f53\u7ed3\u5408\u9002\u5f53\u7684\u6000\u7591\u6001\u5ea6\u548c\u9a8c\u8bc1\u534f\u8bae\u4f7f\u7528\u65f6\uff0cAI\u5de5\u5177\u80fd\u591f\u6709\u610f\u4e49\u5730\u52a0\u901f\u6570\u5b66\u53d1\u73b0\uff0c\u4f46\u9700\u8981\u4ed4\u7ec6\u7684\u4eba\u7c7b\u76d1\u7763\u548c\u6df1\u539a\u7684\u9886\u57df\u4e13\u4e1a\u77e5\u8bc6\u3002\u6210\u529f\u7684\u4eba\u673a\u534f\u4f5c\u9700\u8981\u4eba\u7c7b\u4fdd\u6301\u6570\u5b66\u76f4\u89c9\u548c\u6218\u7565\u65b9\u5411\u3002"}}
{"id": "2602.23079", "categories": ["cs.CL", "cs.CR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23079", "abs": "https://arxiv.org/abs/2602.23079", "authors": ["Boyang Zhang", "Yang Zhang"], "title": "Assessing Deanonymization Risks with Stylometry-Assisted LLM Agent", "comment": null, "summary": "The rapid advancement of large language models (LLMs) has enabled powerful authorship inference capabilities, raising growing concerns about unintended deanonymization risks in textual data such as news articles. In this work, we introduce an LLM agent designed to evaluate and mitigate such risks through a structured, interpretable pipeline. Central to our framework is the proposed $\\textit{SALA}$ (Stylometry-Assisted LLM Analysis) method, which integrates quantitative stylometric features with LLM reasoning for robust and transparent authorship attribution. Experiments on large-scale news datasets demonstrate that $\\textit{SALA}$, particularly when augmented with a database module, achieves high inference accuracy in various scenarios. Finally, we propose a guided recomposition strategy that leverages the agent's reasoning trace to generate rewriting prompts, effectively reducing authorship identifiability while preserving textual meaning. Our findings highlight both the deanonymization potential of LLM agents and the importance of interpretable, proactive defenses for safeguarding author privacy.", "AI": {"tldr": "\u672c\u6587\u63d0\u51faSALA\u65b9\u6cd5\uff0c\u7ed3\u5408\u98ce\u683c\u8ba1\u91cf\u7279\u5f81\u4e0eLLM\u63a8\u7406\uff0c\u8bc4\u4f30\u548c\u51cf\u8f7b\u65b0\u95fb\u6587\u7ae0\u7684\u4f5c\u8005\u8eab\u4efd\u63a8\u65ad\u98ce\u9669\uff0c\u5e76\u63d0\u51fa\u5f15\u5bfc\u91cd\u5199\u7b56\u7565\u4fdd\u62a4\u4f5c\u8005\u9690\u79c1\u3002", "motivation": "\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u5feb\u901f\u53d1\u5c55\u589e\u5f3a\u4e86\u4f5c\u8005\u8eab\u4efd\u63a8\u65ad\u80fd\u529b\uff0c\u5f15\u53d1\u4e86\u65b0\u95fb\u6587\u7ae0\u7b49\u6587\u672c\u6570\u636e\u4e2d\u610f\u5916\u53bb\u533f\u540d\u5316\u7684\u9690\u79c1\u98ce\u9669\u62c5\u5fe7\u3002", "method": "\u63d0\u51faSALA\uff08\u98ce\u683c\u8ba1\u91cf\u8f85\u52a9\u7684LLM\u5206\u6790\uff09\u65b9\u6cd5\uff0c\u6574\u5408\u5b9a\u91cf\u98ce\u683c\u8ba1\u91cf\u7279\u5f81\u4e0eLLM\u63a8\u7406\uff0c\u6784\u5efa\u7ed3\u6784\u5316\u3001\u53ef\u89e3\u91ca\u7684\u7ba1\u9053\u3002\u540c\u65f6\u63d0\u51fa\u5f15\u5bfc\u91cd\u5199\u7b56\u7565\uff0c\u5229\u7528\u4ee3\u7406\u7684\u63a8\u7406\u8f68\u8ff9\u751f\u6210\u91cd\u5199\u63d0\u793a\uff0c\u964d\u4f4e\u4f5c\u8005\u53ef\u8bc6\u522b\u6027\u3002", "result": "\u5728\u5927\u89c4\u6a21\u65b0\u95fb\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cSALA\u65b9\u6cd5\uff08\u7279\u522b\u662f\u589e\u5f3a\u6570\u636e\u5e93\u6a21\u5757\u540e\uff09\u5728\u5404\u79cd\u573a\u666f\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u63a8\u7406\u51c6\u786e\u7387\u3002\u5f15\u5bfc\u91cd\u5199\u7b56\u7565\u80fd\u6709\u6548\u964d\u4f4e\u4f5c\u8005\u53ef\u8bc6\u522b\u6027\u540c\u65f6\u4fdd\u6301\u6587\u672c\u610f\u4e49\u3002", "conclusion": "\u7814\u7a76\u7ed3\u679c\u5f3a\u8c03\u4e86LLM\u4ee3\u7406\u7684\u53bb\u533f\u540d\u5316\u6f5c\u529b\uff0c\u4ee5\u53ca\u53ef\u89e3\u91ca\u3001\u4e3b\u52a8\u9632\u5fa1\u673a\u5236\u5bf9\u4e8e\u4fdd\u62a4\u4f5c\u8005\u9690\u79c1\u7684\u91cd\u8981\u6027\u3002"}}
{"id": "2602.22879", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22879", "abs": "https://arxiv.org/abs/2602.22879", "authors": ["Xingcheng Fu", "Shengpeng Wang", "Yisen Gao", "Xianxian Li", "Chunpei Li", "Qingyun Sun", "Dongran Yu"], "title": "Towards LLM-Empowered Knowledge Tracing via LLM-Student Hierarchical Behavior Alignment in Hyperbolic Space", "comment": "9 pages, 6 figures, Accepted to AAAI 2026", "summary": "Knowledge Tracing (KT) diagnoses students' concept mastery through continuous learning state monitoring in education.Existing methods primarily focus on studying behavioral sequences based on ID or textual information.While existing methods rely on ID-based sequences or shallow textual features, they often fail to capture (1) the hierarchical evolution of cognitive states and (2) individualized problem difficulty perception due to limited semantic modeling. Therefore, this paper proposes a Large Language Model Hyperbolic Aligned Knowledge Tracing(L-HAKT). First, the teacher agent deeply parses question semantics and explicitly constructs hierarchical dependencies of knowledge points; the student agent simulates learning behaviors to generate synthetic data. Then, contrastive learning is performed between synthetic and real data in hyperbolic space to reduce distribution differences in key features such as question difficulty and forgetting patterns. Finally, by optimizing hyperbolic curvature, we explicitly model the tree-like hierarchical structure of knowledge points, precisely characterizing differences in learning curve morphology for knowledge points at different levels. Extensive experiments on four real-world educational datasets validate the effectiveness of our Large Language Model Hyperbolic Aligned Knowledge Tracing (L-HAKT) framework.", "AI": {"tldr": "\u63d0\u51faL-HAKT\u6846\u67b6\uff0c\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u548c\u53cc\u66f2\u7a7a\u95f4\u5bf9\u9f50\u6765\u89e3\u51b3\u77e5\u8bc6\u8ffd\u8e2a\u4e2d\u8ba4\u77e5\u72b6\u6001\u5c42\u6b21\u6f14\u5316\u548c\u4e2a\u6027\u5316\u96be\u5ea6\u611f\u77e5\u95ee\u9898\u3002", "motivation": "\u73b0\u6709\u77e5\u8bc6\u8ffd\u8e2a\u65b9\u6cd5\u4e3b\u8981\u57fa\u4e8eID\u5e8f\u5217\u6216\u6d45\u5c42\u6587\u672c\u7279\u5f81\uff0c\u65e0\u6cd5\u6355\u6349\u8ba4\u77e5\u72b6\u6001\u7684\u5c42\u6b21\u6f14\u5316\u4ee5\u53ca\u5b66\u751f\u4e2a\u4f53\u5bf9\u95ee\u9898\u96be\u5ea6\u7684\u611f\u77e5\u5dee\u5f02\u3002", "method": "1. \u6559\u5e08\u4ee3\u7406\u6df1\u5ea6\u89e3\u6790\u95ee\u9898\u8bed\u4e49\u5e76\u6784\u5efa\u77e5\u8bc6\u70b9\u5c42\u6b21\u4f9d\u8d56\uff1b\u5b66\u751f\u4ee3\u7406\u6a21\u62df\u5b66\u4e60\u884c\u4e3a\u751f\u6210\u5408\u6210\u6570\u636e\u30022. \u5728\u53cc\u66f2\u7a7a\u95f4\u4e2d\u5bf9\u5408\u6210\u6570\u636e\u548c\u771f\u5b9e\u6570\u636e\u8fdb\u884c\u5bf9\u6bd4\u5b66\u4e60\uff0c\u51cf\u5c11\u95ee\u9898\u96be\u5ea6\u548c\u9057\u5fd8\u6a21\u5f0f\u7b49\u5173\u952e\u7279\u5f81\u7684\u5206\u5e03\u5dee\u5f02\u30023. \u901a\u8fc7\u4f18\u5316\u53cc\u66f2\u66f2\u7387\uff0c\u663e\u5f0f\u5efa\u6a21\u77e5\u8bc6\u70b9\u7684\u6811\u72b6\u5c42\u6b21\u7ed3\u6784\uff0c\u7cbe\u786e\u523b\u753b\u4e0d\u540c\u5c42\u6b21\u77e5\u8bc6\u70b9\u7684\u5b66\u4e60\u66f2\u7ebf\u5f62\u6001\u5dee\u5f02\u3002", "result": "\u5728\u56db\u4e2a\u771f\u5b9e\u6559\u80b2\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u9a8c\u8bc1\u4e86L-HAKT\u6846\u67b6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684L-HAKT\u6846\u67b6\u901a\u8fc7\u5927\u8bed\u8a00\u6a21\u578b\u548c\u53cc\u66f2\u7a7a\u95f4\u5bf9\u9f50\uff0c\u80fd\u591f\u66f4\u597d\u5730\u5efa\u6a21\u77e5\u8bc6\u70b9\u7684\u5c42\u6b21\u7ed3\u6784\u548c\u4e2a\u4f53\u5316\u5b66\u4e60\u7279\u5f81\uff0c\u63d0\u5347\u4e86\u77e5\u8bc6\u8ffd\u8e2a\u7684\u6027\u80fd\u3002"}}
{"id": "2602.22717", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22717", "abs": "https://arxiv.org/abs/2602.22717", "authors": ["Shuoqi Chen", "Yujia Wu", "Geoffrey P. Luke"], "title": "IRSDE-Despeckle: A Physics-Grounded Diffusion Model for Generalizable Ultrasound Despeckling", "comment": "12 pages main text + 6 pages appendix, 7 figures main + 3 figures appendix, 3 tables main + 1 table appendix. Preprint", "summary": "Ultrasound imaging is widely used for real-time, noninvasive diagnosis, but speckle and related artifacts reduce image quality and can hinder interpretation. We present a diffusion-based ultrasound despeckling method built on the Image Restoration Stochastic Differential Equations framework. To enable supervised training, we curate large paired datasets by simulating ultrasound images from speckle-free magnetic resonance images using the Matlab UltraSound Toolbox. The proposed model reconstructs speckle-suppressed images while preserving anatomically meaningful edges and contrast. On a held-out simulated test set, our approach consistently outperforms classical filters and recent learning-based despeckling baselines. We quantify prediction uncertainty via cross-model variance and show that higher uncertainty correlates with higher reconstruction error, providing a practical indicator of difficult or failure-prone regions. Finally, we evaluate sensitivity to simulation probe settings and observe domain shift, motivating diversified training and adaptation for robust clinical deployment.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u8d85\u58f0\u56fe\u50cf\u53bb\u6591\u65b9\u6cd5\uff0c\u901a\u8fc7\u6a21\u62df\u914d\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\uff0c\u5728\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\u7684\u540c\u65f6\u6291\u5236\u6591\u70b9\u566a\u58f0\uff0c\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u548c\u73b0\u6709\u5b66\u4e60\u57fa\u7ebf\u3002", "motivation": "\u8d85\u58f0\u6210\u50cf\u5e7f\u6cdb\u7528\u4e8e\u5b9e\u65f6\u65e0\u521b\u8bca\u65ad\uff0c\u4f46\u6591\u70b9\u566a\u58f0\u548c\u76f8\u5173\u4f2a\u5f71\u4f1a\u964d\u4f4e\u56fe\u50cf\u8d28\u91cf\u5e76\u59a8\u788d\u89e3\u8bfb\uff0c\u9700\u8981\u6709\u6548\u7684\u53bb\u6591\u65b9\u6cd5\u3002", "method": "\u57fa\u4e8e\u56fe\u50cf\u6062\u590d\u968f\u673a\u5fae\u5206\u65b9\u7a0b\u6846\u67b6\u6784\u5efa\u6269\u6563\u6a21\u578b\uff0c\u4f7f\u7528Matlab\u8d85\u58f0\u5de5\u5177\u7bb1\u4ece\u65e0\u6591\u70b9\u7684\u78c1\u5171\u632f\u56fe\u50cf\u6a21\u62df\u8d85\u58f0\u56fe\u50cf\u521b\u5efa\u914d\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u76d1\u7763\u8bad\u7ec3\u3002", "result": "\u5728\u6a21\u62df\u6d4b\u8bd5\u96c6\u4e0a\u4f18\u4e8e\u7ecf\u5178\u6ee4\u6ce2\u5668\u548c\u8fd1\u671f\u5b66\u4e60\u57fa\u7ebf\u7684\u53bb\u6591\u65b9\u6cd5\uff0c\u80fd\u91cd\u5efa\u6291\u5236\u6591\u70b9\u540c\u65f6\u4fdd\u7559\u89e3\u5256\u8fb9\u7f18\u548c\u5bf9\u6bd4\u5ea6\u7684\u56fe\u50cf\uff0c\u901a\u8fc7\u8de8\u6a21\u578b\u65b9\u5dee\u91cf\u5316\u9884\u6d4b\u4e0d\u786e\u5b9a\u6027\u3002", "conclusion": "\u63d0\u51fa\u7684\u6269\u6563\u6a21\u578b\u80fd\u6709\u6548\u53bb\u6591\u5e76\u4fdd\u6301\u89e3\u5256\u7ed3\u6784\uff0c\u4e0d\u786e\u5b9a\u6027\u5206\u6790\u53ef\u8bc6\u522b\u56f0\u96be\u533a\u57df\uff0c\u5bf9\u6a21\u62df\u63a2\u5934\u8bbe\u7f6e\u7684\u654f\u611f\u6027\u8868\u660e\u9700\u8981\u591a\u6837\u5316\u8bad\u7ec3\u548c\u9002\u5e94\u4ee5\u5b9e\u73b0\u7a33\u5065\u4e34\u5e8a\u90e8\u7f72\u3002"}}
{"id": "2602.22727", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22727", "abs": "https://arxiv.org/abs/2602.22727", "authors": ["Yangguang Lin", "Quan Fang", "Yufei Li", "Jiachen Sun", "Junyu Gao", "Jitao Sang"], "title": "HulluEdit: Single-Pass Evidence-Consistent Subspace Editing for Mitigating Hallucinations in Large Vision-Language Models", "comment": "accepted at CVPR 2026", "summary": "Object hallucination in Large Vision-Language Models (LVLMs) significantly hinders their reliable deployment. Existing methods struggle to balance efficiency and accuracy: they often require expensive reference models and multiple forward passes, or apply static edits that risk suppressing genuine visual evidence. To address this, we introduce HulluEdit, a single-pass, reference-free intervention framework. Our core innovation is orthogonal subspace editing: we decompose the hidden states of the model into orthogonal subspaces - visual evidence, conflicting priors, and residual uncertainty - enabling selective suppression of hallucinatory patterns without interfering with visual grounding. This approach mathematically guarantees that edits applied to the prior subspace leave the visual component entirely unaffected. Extensive experiments show that HulluEdit achieves state-of-the-art hallucination reduction on benchmarks including POPE and CHAIR across diverse architectures, while preserving general capabilities on MME and maintaining efficient inference. Our method consistently outperforms contrastive decoding and static subspace editing baselines, offering a new pathway toward more trustworthy LVLMs.", "AI": {"tldr": "HulluEdit\uff1a\u4e00\u79cd\u5355\u6b21\u524d\u5411\u4f20\u64ad\u3001\u65e0\u9700\u53c2\u8003\u6a21\u578b\u7684LVLM\u5e7b\u89c9\u5e72\u9884\u6846\u67b6\uff0c\u901a\u8fc7\u6b63\u4ea4\u5b50\u7a7a\u95f4\u7f16\u8f91\u9009\u62e9\u6027\u6291\u5236\u5e7b\u89c9\u6a21\u5f0f\uff0c\u540c\u65f6\u4fdd\u6301\u89c6\u89c9\u57fa\u7840\u80fd\u529b\u3002", "motivation": "\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e2d\u7684\u7269\u4f53\u5e7b\u89c9\u95ee\u9898\u4e25\u91cd\u5f71\u54cd\u5176\u53ef\u9760\u90e8\u7f72\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u6548\u7387\u548c\u51c6\u786e\u6027\u4e4b\u95f4\u96be\u4ee5\u5e73\u8861\uff1a\u8981\u4e48\u9700\u8981\u6602\u8d35\u7684\u53c2\u8003\u6a21\u578b\u548c\u591a\u6b21\u524d\u5411\u4f20\u64ad\uff0c\u8981\u4e48\u91c7\u7528\u9759\u6001\u7f16\u8f91\u53ef\u80fd\u6291\u5236\u771f\u5b9e\u7684\u89c6\u89c9\u8bc1\u636e\u3002", "method": "\u63d0\u51fa\u6b63\u4ea4\u5b50\u7a7a\u95f4\u7f16\u8f91\u65b9\u6cd5\uff0c\u5c06\u6a21\u578b\u9690\u85cf\u72b6\u6001\u5206\u89e3\u4e3a\u4e09\u4e2a\u6b63\u4ea4\u5b50\u7a7a\u95f4\uff1a\u89c6\u89c9\u8bc1\u636e\u3001\u51b2\u7a81\u5148\u9a8c\u548c\u6b8b\u5dee\u4e0d\u786e\u5b9a\u6027\u3002\u901a\u8fc7\u9009\u62e9\u6027\u6291\u5236\u5e7b\u89c9\u6a21\u5f0f\u800c\u4e0d\u5e72\u6270\u89c6\u89c9\u57fa\u7840\uff0c\u6570\u5b66\u4e0a\u4fdd\u8bc1\u5bf9\u5148\u9a8c\u5b50\u7a7a\u95f4\u7684\u7f16\u8f91\u5b8c\u5168\u4e0d\u5f71\u54cd\u89c6\u89c9\u6210\u5206\u3002", "result": "\u5728POPE\u548cCHAIR\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u5e7b\u89c9\u51cf\u5c11\u6548\u679c\uff0c\u540c\u65f6\u5728MME\u4e0a\u4fdd\u6301\u901a\u7528\u80fd\u529b\uff0c\u63a8\u7406\u6548\u7387\u9ad8\u3002\u5728\u5404\u79cd\u67b6\u6784\u4e0a\u4e00\u81f4\u4f18\u4e8e\u5bf9\u6bd4\u89e3\u7801\u548c\u9759\u6001\u5b50\u7a7a\u95f4\u7f16\u8f91\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "HulluEdit\u4e3a\u6784\u5efa\u66f4\u53ef\u4fe1\u7684\u5927\u578b\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u4e86\u65b0\u9014\u5f84\uff0c\u901a\u8fc7\u6b63\u4ea4\u5b50\u7a7a\u95f4\u7f16\u8f91\u5728\u4fdd\u6301\u89c6\u89c9\u57fa\u7840\u7684\u540c\u65f6\u6709\u6548\u51cf\u5c11\u5e7b\u89c9\uff0c\u5e73\u8861\u4e86\u6548\u7387\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2602.23184", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23184", "abs": "https://arxiv.org/abs/2602.23184", "authors": ["Sara Rosenthal", "Yannis Katsis", "Vraj Shah", "Lihong He", "Lucian Popa", "Marina Danilevsky"], "title": "MTRAG-UN: A Benchmark for Open Challenges in Multi-Turn RAG Conversations", "comment": "5 pages, 3 figures", "summary": "We present MTRAG-UN, a benchmark for exploring open challenges in multi-turn retrieval augmented generation, a popular use of large language models. We release a benchmark of 666 tasks containing over 2,800 conversation turns across 6 domains with accompanying corpora. Our experiments show that retrieval and generation models continue to struggle on conversations with UNanswerable, UNderspecified, and NONstandalone questions and UNclear responses. Our benchmark is available at https://github.com/IBM/mt-rag-benchmark", "AI": {"tldr": "MTRAG-UN\u662f\u4e00\u4e2a\u591a\u8f6e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b666\u4e2a\u4efb\u52a1\u30012800+\u5bf9\u8bdd\u8f6e\u6b21\uff0c\u8986\u76d66\u4e2a\u9886\u57df\uff0c\u7528\u4e8e\u8bc4\u4f30\u6a21\u578b\u5728\u4e0d\u53ef\u56de\u7b54\u3001\u672a\u660e\u786e\u6307\u5b9a\u3001\u975e\u72ec\u7acb\u95ee\u9898\u548c\u4e0d\u6e05\u6670\u56de\u7b54\u7b49\u6311\u6218\u6027\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u6a21\u578b\u5728\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u9762\u4e34\u8bf8\u591a\u6311\u6218\uff0c\u7279\u522b\u662f\u5728\u5904\u7406\u4e0d\u53ef\u56de\u7b54\u3001\u672a\u660e\u786e\u6307\u5b9a\u3001\u975e\u72ec\u7acb\u95ee\u9898\u548c\u4e0d\u6e05\u6670\u56de\u7b54\u7b49\u590d\u6742\u573a\u666f\u65f6\u8868\u73b0\u4e0d\u4f73\u3002\u9700\u8981\u5efa\u7acb\u4e00\u4e2a\u4e13\u95e8\u7684\u57fa\u51c6\u6d4b\u8bd5\u6765\u7cfb\u7edf\u8bc4\u4f30\u548c\u63a8\u52a8\u8fd9\u4e9b\u6311\u6218\u7684\u89e3\u51b3\u3002", "method": "\u521b\u5efa\u4e86MTRAG-UN\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5305\u542b666\u4e2a\u4efb\u52a1\uff0c\u8d85\u8fc72800\u4e2a\u5bf9\u8bdd\u8f6e\u6b21\uff0c\u8986\u76d66\u4e2a\u4e0d\u540c\u9886\u57df\u3002\u6bcf\u4e2a\u4efb\u52a1\u90fd\u914d\u5907\u4e86\u76f8\u5e94\u7684\u8bed\u6599\u5e93\uff0c\u4e13\u95e8\u8bbe\u8ba1\u7528\u4e8e\u6d4b\u8bd5\u6a21\u578b\u5728UNanswerable\uff08\u4e0d\u53ef\u56de\u7b54\uff09\u3001UNderspecified\uff08\u672a\u660e\u786e\u6307\u5b9a\uff09\u3001NONstandalone\uff08\u975e\u72ec\u7acb\uff09\u548cUNclear\uff08\u4e0d\u6e05\u6670\uff09\u7b49\u6311\u6218\u6027\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u8868\u73b0\u3002", "result": "\u5b9e\u9a8c\u7ed3\u679c\u8868\u660e\uff0c\u5f53\u524d\u7684\u68c0\u7d22\u548c\u751f\u6210\u6a21\u578b\u5728\u5904\u7406\u5305\u542b\u4e0d\u53ef\u56de\u7b54\u3001\u672a\u660e\u786e\u6307\u5b9a\u3001\u975e\u72ec\u7acb\u95ee\u9898\u548c\u4e0d\u6e05\u6670\u56de\u7b54\u7684\u5bf9\u8bdd\u65f6\u4ecd\u7136\u5b58\u5728\u56f0\u96be\u3002\u8fd9\u4e9b\u6311\u6218\u6027\u573a\u666f\u663e\u8457\u5f71\u54cd\u4e86\u6a21\u578b\u7684\u6027\u80fd\u8868\u73b0\u3002", "conclusion": "MTRAG-UN\u57fa\u51c6\u6d4b\u8bd5\u4e3a\u591a\u8f6e\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u7814\u7a76\u63d0\u4f9b\u4e86\u91cd\u8981\u7684\u8bc4\u4f30\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5f53\u524d\u6a21\u578b\u5728\u5904\u7406\u590d\u6742\u5bf9\u8bdd\u573a\u666f\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u6307\u660e\u4e86\u6539\u8fdb\u65b9\u5411\u3002\u8be5\u57fa\u51c6\u5df2\u5f00\u6e90\u4f9b\u7814\u7a76\u793e\u533a\u4f7f\u7528\u3002"}}
{"id": "2602.22734", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22734", "abs": "https://arxiv.org/abs/2602.22734", "authors": ["Muzi Tao", "Chufan Shi", "Huijuan Wang", "Shengbang Tong", "Xuezhe Ma"], "title": "Asymmetric Idiosyncrasies in Multimodal Models", "comment": "Project page: https://muzi-tao.github.io/asymmetric-idiosyncrasies/", "summary": "In this work, we study idiosyncrasies in the caption models and their downstream impact on text-to-image models. We design a systematic analysis: given either a generated caption or the corresponding image, we train neural networks to predict the originating caption model. Our results show that text classification yields very high accuracy (99.70\\%), indicating that captioning models embed distinctive stylistic signatures. In contrast, these signatures largely disappear in the generated images, with classification accuracy dropping to at most 50\\% even for the state-of-the-art Flux model. To better understand this cross-modal discrepancy, we further analyze the data and find that the generated images fail to preserve key variations present in captions, such as differences in the level of detail, emphasis on color and texture, and the distribution of objects within a scene. Overall, our classification-based framework provides a novel methodology for quantifying both the stylistic idiosyncrasies of caption models and the prompt-following ability of text-to-image systems.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e00\u4e2a\u5206\u7c7b\u6846\u67b6\u6765\u91cf\u5316\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b\uff08caption models\uff09\u7684\u98ce\u683c\u7279\u5f81\u53ca\u5176\u5bf9\u6587\u751f\u56fe\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u63cf\u8ff0\u6a21\u578b\u6709\u663e\u8457\u98ce\u683c\u7279\u5f81\uff08\u5206\u7c7b\u51c6\u786e\u738799.7%\uff09\uff0c\u4f46\u8fd9\u4e9b\u7279\u5f81\u5728\u751f\u6210\u7684\u56fe\u50cf\u4e2d\u57fa\u672c\u6d88\u5931\uff08\u51c6\u786e\u7387\u6700\u591a50%\uff09\u3002", "motivation": "\u7814\u7a76\u56fe\u50cf\u63cf\u8ff0\u6a21\u578b\u7684\u98ce\u683c\u7279\u5f81\uff08idiosyncrasies\uff09\u53ca\u5176\u5bf9\u4e0b\u6e38\u6587\u751f\u56fe\u6a21\u578b\u7684\u5f71\u54cd\uff0c\u91cf\u5316\u4e0d\u540c\u63cf\u8ff0\u6a21\u578b\u7684\u98ce\u683c\u5dee\u5f02\uff0c\u5e76\u8bc4\u4f30\u6587\u751f\u56fe\u6a21\u578b\u662f\u5426\u80fd\u5728\u751f\u6210\u7684\u56fe\u50cf\u4e2d\u4fdd\u7559\u8fd9\u4e9b\u98ce\u683c\u7279\u5f81\u3002", "method": "\u8bbe\u8ba1\u7cfb\u7edf\u6027\u5206\u6790\u6846\u67b6\uff1a\u7ed9\u5b9a\u751f\u6210\u7684\u63cf\u8ff0\u6216\u5bf9\u5e94\u56fe\u50cf\uff0c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u9884\u6d4b\u5176\u6765\u6e90\u7684\u63cf\u8ff0\u6a21\u578b\u3002\u901a\u8fc7\u6587\u672c\u5206\u7c7b\uff08\u57fa\u4e8e\u63cf\u8ff0\uff09\u548c\u56fe\u50cf\u5206\u7c7b\uff08\u57fa\u4e8e\u751f\u6210\u7684\u56fe\u50cf\uff09\u6765\u8bc4\u4f30\u98ce\u683c\u7279\u5f81\u7684\u4fdd\u7559\u7a0b\u5ea6\u3002", "result": "\u6587\u672c\u5206\u7c7b\u51c6\u786e\u7387\u9ad8\u8fbe99.70%\uff0c\u8868\u660e\u63cf\u8ff0\u6a21\u578b\u6709\u72ec\u7279\u7684\u98ce\u683c\u7279\u5f81\uff1b\u4f46\u56fe\u50cf\u5206\u7c7b\u51c6\u786e\u7387\u6700\u591a\u53ea\u670950%\uff08\u5373\u4f7f\u662f\u5148\u8fdb\u7684Flux\u6a21\u578b\uff09\uff0c\u8bf4\u660e\u8fd9\u4e9b\u98ce\u683c\u7279\u5f81\u5728\u751f\u6210\u7684\u56fe\u50cf\u4e2d\u57fa\u672c\u6d88\u5931\u3002\u8fdb\u4e00\u6b65\u5206\u6790\u53d1\u73b0\u751f\u6210\u7684\u56fe\u50cf\u672a\u80fd\u4fdd\u7559\u63cf\u8ff0\u4e2d\u7684\u5173\u952e\u53d8\u5316\uff0c\u5982\u7ec6\u8282\u7a0b\u5ea6\u3001\u989c\u8272\u7eb9\u7406\u5f3a\u8c03\u3001\u573a\u666f\u4e2d\u7269\u4f53\u5206\u5e03\u7b49\u5dee\u5f02\u3002", "conclusion": "\u63d0\u51fa\u7684\u5206\u7c7b\u6846\u67b6\u4e3a\u91cf\u5316\u63cf\u8ff0\u6a21\u578b\u7684\u98ce\u683c\u7279\u5f81\u548c\u6587\u751f\u56fe\u7cfb\u7edf\u7684\u63d0\u793a\u8ddf\u968f\u80fd\u529b\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\u3002\u7814\u7a76\u53d1\u73b0\u63cf\u8ff0\u6a21\u578b\u6709\u663e\u8457\u98ce\u683c\u7279\u5f81\uff0c\u4f46\u6587\u751f\u56fe\u6a21\u578b\u96be\u4ee5\u5728\u751f\u6210\u7684\u56fe\u50cf\u4e2d\u4fdd\u7559\u8fd9\u4e9b\u7279\u5f81\uff0c\u63ed\u793a\u4e86\u8de8\u6a21\u6001\u5dee\u5f02\u95ee\u9898\u3002"}}
{"id": "2602.23197", "categories": ["cs.CL", "cs.LG", "stat.ML"], "pdf": "https://arxiv.org/pdf/2602.23197", "abs": "https://arxiv.org/abs/2602.23197", "authors": ["Chungpa Lee", "Jy-yong Sohn", "Kangwook Lee"], "title": "Fine-Tuning Without Forgetting In-Context Learning: A Theoretical Analysis of Linear Attention Models", "comment": null, "summary": "Transformer-based large language models exhibit in-context learning, enabling adaptation to downstream tasks via few-shot prompting with demonstrations. In practice, such models are often fine-tuned to improve zero-shot performance on downstream tasks, allowing them to solve tasks without examples and thereby reducing inference costs. However, fine-tuning can degrade in-context learning, limiting the performance of fine-tuned models on tasks not seen during fine-tuning. Using linear attention models, we provide a theoretical analysis that characterizes how fine-tuning objectives modify attention parameters and identifies conditions under which this leads to degraded few-shot performance. We show that fine-tuning all attention parameters can harm in-context learning, whereas restricting updates to the value matrix improves zero-shot performance while preserving in-context learning. We further show that incorporating an auxiliary few-shot loss enhances in-context learning primarily on the target task, at the expense of degraded in-context learning ability on tasks not seen during fine-tuning. We empirically validate our theoretical results.", "AI": {"tldr": "\u8bba\u6587\u5206\u6790\u4e86\u5fae\u8c03\u5bf9Transformer\u5927\u8bed\u8a00\u6a21\u578b\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u7684\u5f71\u54cd\uff0c\u53d1\u73b0\u5168\u53c2\u6570\u5fae\u8c03\u4f1a\u635f\u5bb3\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u800c\u4ec5\u66f4\u65b0\u503c\u77e9\u9635\u53ef\u4ee5\u4fdd\u6301\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u540c\u65f6\u63d0\u5347\u96f6\u6837\u672c\u6027\u80fd\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u901a\u5e38\u901a\u8fc7\u5fae\u8c03\u6765\u63d0\u5347\u4e0b\u6e38\u4efb\u52a1\u7684\u96f6\u6837\u672c\u6027\u80fd\uff0c\u4f46\u5fae\u8c03\u53ef\u80fd\u4f1a\u635f\u5bb3\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff0c\u9650\u5236\u5176\u5728\u5fae\u8c03\u672a\u89c1\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\u3002\u9700\u8981\u7406\u89e3\u5fae\u8c03\u5982\u4f55\u5f71\u54cd\u4e0a\u4e0b\u6587\u5b66\u4e60\u673a\u5236\u3002", "method": "\u4f7f\u7528\u7ebf\u6027\u6ce8\u610f\u529b\u6a21\u578b\u8fdb\u884c\u7406\u8bba\u5206\u6790\uff0c\u7814\u7a76\u5fae\u8c03\u76ee\u6807\u5982\u4f55\u4fee\u6539\u6ce8\u610f\u529b\u53c2\u6570\uff0c\u8bc6\u522b\u5bfc\u81f4\u4e0a\u4e0b\u6587\u5b66\u4e60\u6027\u80fd\u4e0b\u964d\u7684\u6761\u4ef6\u3002\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u7406\u8bba\u7ed3\u679c\u3002", "result": "1. \u5fae\u8c03\u6240\u6709\u6ce8\u610f\u529b\u53c2\u6570\u4f1a\u635f\u5bb3\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff1b2. \u4ec5\u66f4\u65b0\u503c\u77e9\u9635\u53ef\u4ee5\u5728\u63d0\u5347\u96f6\u6837\u672c\u6027\u80fd\u7684\u540c\u65f6\u4fdd\u6301\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\uff1b3. \u52a0\u5165\u8f85\u52a9\u7684\u5c11\u6837\u672c\u635f\u5931\u4e3b\u8981\u589e\u5f3a\u76ee\u6807\u4efb\u52a1\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\uff0c\u4f46\u4f1a\u635f\u5bb3\u5728\u672a\u89c1\u4efb\u52a1\u4e0a\u7684\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u3002", "conclusion": "\u5fae\u8c03\u7b56\u7565\u9700\u8981\u8c28\u614e\u8bbe\u8ba1\uff0c\u4ec5\u66f4\u65b0\u503c\u77e9\u9635\u662f\u5e73\u8861\u96f6\u6837\u672c\u6027\u80fd\u548c\u4e0a\u4e0b\u6587\u5b66\u4e60\u80fd\u529b\u7684\u6709\u6548\u65b9\u6cd5\uff0c\u800c\u5168\u53c2\u6570\u5fae\u8c03\u6216\u52a0\u5165\u5c11\u6837\u672c\u635f\u5931\u53ef\u80fd\u4f1a\u635f\u5bb3\u6a21\u578b\u7684\u6cdb\u5316\u80fd\u529b\u3002"}}
{"id": "2602.22963", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22963", "abs": "https://arxiv.org/abs/2602.22963", "authors": ["Zehao Li", "Hongwei Yu", "Hao Jiang", "Qiang Sheng", "Yilong Xu", "Baolong Bi", "Yang Li", "Zhenlong Yuan", "Yujun Cai", "Zhaoqi Wang"], "title": "FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning", "comment": null, "summary": "Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is sparse, fragmented, or requires external verification. To address these limitations, we propose FactGuard, an agentic framework for video misinformation detection that formulates verification as an iterative reasoning process built upon MLLMs. FactGuard explicitly assesses task ambiguity and selectively invokes external tools to acquire critical evidence, enabling progressive refinement of reasoning trajectories. To further strengthen this capability, we introduce a two-stage training strategy that combines domain-specific agentic supervised fine-tuning with decision-aware reinforcement learning to optimize tool usage and calibrate risk-sensitive decision making. Extensive experiments on FakeSV, FakeTT, and FakeVV demonstrate FactGuard's state-of-the-art performance and validate its excellent robustness and generalization capacity.", "AI": {"tldr": "FactGuard\uff1a\u57fa\u4e8eMLLM\u7684\u4ee3\u7406\u6846\u67b6\uff0c\u901a\u8fc7\u8fed\u4ee3\u63a8\u7406\u548c\u5916\u90e8\u5de5\u5177\u8c03\u7528\u68c0\u6d4b\u89c6\u9891\u865a\u5047\u4fe1\u606f\uff0c\u89e3\u51b3\u56fa\u5b9a\u6df1\u5ea6\u63a8\u7406\u548c\u8fc7\u5ea6\u4f9d\u8d56\u5185\u90e8\u5047\u8bbe\u7684\u95ee\u9898", "motivation": "\u73b0\u6709MLLM\u5728\u89c6\u9891\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4e2d\u4f9d\u8d56\u56fa\u5b9a\u6df1\u5ea6\u63a8\u7406\uff0c\u8fc7\u5ea6\u4fe1\u4efb\u5185\u90e8\u751f\u6210\u7684\u5047\u8bbe\uff0c\u5728\u5173\u952e\u8bc1\u636e\u7a00\u758f\u3001\u788e\u7247\u5316\u6216\u9700\u8981\u5916\u90e8\u9a8c\u8bc1\u7684\u573a\u666f\u4e0b\u8868\u73b0\u4e0d\u4f73", "method": "FactGuard\u6846\u67b6\u5c06\u9a8c\u8bc1\u6784\u5efa\u4e3a\u57fa\u4e8eMLLM\u7684\u8fed\u4ee3\u63a8\u7406\u8fc7\u7a0b\uff0c\u663e\u5f0f\u8bc4\u4f30\u4efb\u52a1\u6a21\u7cca\u6027\u5e76\u9009\u62e9\u6027\u8c03\u7528\u5916\u90e8\u5de5\u5177\u83b7\u53d6\u5173\u952e\u8bc1\u636e\uff0c\u5b9e\u73b0\u63a8\u7406\u8f68\u8ff9\u7684\u6e10\u8fdb\u5f0f\u4f18\u5316\uff1b\u91c7\u7528\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff1a\u9886\u57df\u7279\u5b9a\u4ee3\u7406\u76d1\u7763\u5fae\u8c03 + \u51b3\u7b56\u611f\u77e5\u5f3a\u5316\u5b66\u4e60", "result": "\u5728FakeSV\u3001FakeTT\u548cFakeVV\u6570\u636e\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660eFactGuard\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd\uff0c\u9a8c\u8bc1\u4e86\u5176\u4f18\u79c0\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b", "conclusion": "FactGuard\u901a\u8fc7\u4ee3\u7406\u6846\u67b6\u548c\u8fed\u4ee3\u63a8\u7406\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86MLLM\u5728\u89c6\u9891\u865a\u5047\u4fe1\u606f\u68c0\u6d4b\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u63d0\u5347\u4e86\u68c0\u6d4b\u7684\u51c6\u786e\u6027\u548c\u53ef\u9760\u6027"}}
{"id": "2602.22742", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22742", "abs": "https://arxiv.org/abs/2602.22742", "authors": ["Akihisa Watanabe", "Qing Yu", "Edgar Simo-Serra", "Kent Fujiwara"], "title": "ProjFlow: Projection Sampling with Flow Matching for Zero-Shot Exact Spatial Motion Control", "comment": null, "summary": "Generating human motion with precise spatial control is a challenging problem. Existing approaches often require task-specific training or slow optimization, and enforcing hard constraints frequently disrupts motion naturalness. Building on the observation that many animation tasks can be formulated as a linear inverse problem, we introduce ProjFlow, a training-free sampler that achieves zero-shot, exact satisfaction of linear spatial constraints while preserving motion realism. Our key advance is a novel kinematics-aware metric that encodes skeletal topology. This metric allows the sampler to enforce hard constraints by distributing corrections coherently across the entire skeleton, avoiding the unnatural artifacts of naive projection. Furthermore, for sparse inputs, such as filling in long gaps between a few keyframes, we introduce a time-varying formulation using pseudo-observations that fade during sampling. Extensive experiments on representative applications, motion inpainting, and 2D-to-3D lifting, demonstrate that ProjFlow achieves exact constraint satisfaction and matches or improves realism over zero-shot baselines, while remaining competitive with training-based controllers.", "AI": {"tldr": "ProjFlow\uff1a\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u3001\u96f6\u6837\u672c\u7684\u91c7\u6837\u5668\uff0c\u901a\u8fc7\u65b0\u9896\u7684\u9aa8\u9abc\u611f\u77e5\u5ea6\u91cf\u5b9e\u73b0\u7cbe\u786e\u7684\u7ebf\u6027\u7a7a\u95f4\u7ea6\u675f\uff0c\u540c\u65f6\u4fdd\u6301\u8fd0\u52a8\u81ea\u7136\u6027\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u9700\u8981\u4efb\u52a1\u7279\u5b9a\u8bad\u7ec3\u6216\u7f13\u6162\u4f18\u5316\uff0c\u4e14\u786c\u7ea6\u675f\u5e38\u7834\u574f\u8fd0\u52a8\u81ea\u7136\u6027\u3002\u8bb8\u591a\u52a8\u753b\u4efb\u52a1\u53ef\u8868\u8ff0\u4e3a\u7ebf\u6027\u9006\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u80fd\u7cbe\u786e\u6ee1\u8db3\u7a7a\u95f4\u7ea6\u675f\u540c\u65f6\u4fdd\u6301\u771f\u5b9e\u611f\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faProjFlow\u8bad\u7ec3\u514d\u8d39\u91c7\u6837\u5668\uff0c\u6838\u5fc3\u662f\u65b0\u9896\u7684\u9aa8\u9abc\u62d3\u6251\u611f\u77e5\u5ea6\u91cf\uff0c\u4f7f\u91c7\u6837\u5668\u80fd\u901a\u8fc7\u6574\u4e2a\u9aa8\u9abc\u7684\u8fde\u8d2f\u4fee\u6b63\u6765\u5f3a\u5236\u6267\u884c\u786c\u7ea6\u675f\u3002\u5bf9\u4e8e\u7a00\u758f\u8f93\u5165\uff08\u5982\u586b\u8865\u5173\u952e\u5e27\u95f4\u7684\u957f\u95f4\u9699\uff09\uff0c\u5f15\u5165\u4f7f\u7528\u4f2a\u89c2\u6d4b\u7684\u65f6\u95f4\u53d8\u5316\u516c\u5f0f\u3002", "result": "\u5728\u8fd0\u52a8\u4fee\u590d\u548c2D\u52303D\u63d0\u5347\u7b49\u4ee3\u8868\u6027\u5e94\u7528\u4e2d\uff0cProjFlow\u5b9e\u73b0\u4e86\u7cbe\u786e\u7ea6\u675f\u6ee1\u8db3\uff0c\u5728\u96f6\u6837\u672c\u57fa\u7ebf\u4e0a\u5339\u914d\u6216\u63d0\u9ad8\u4e86\u771f\u5b9e\u611f\uff0c\u540c\u65f6\u4e0e\u57fa\u4e8e\u8bad\u7ec3\u7684\u63a7\u5236\u5668\u4fdd\u6301\u7ade\u4e89\u529b\u3002", "conclusion": "ProjFlow\u901a\u8fc7\u9aa8\u9abc\u611f\u77e5\u5ea6\u91cf\u548c\u65f6\u95f4\u53d8\u5316\u516c\u5f0f\uff0c\u5b9e\u73b0\u4e86\u96f6\u6837\u672c\u3001\u7cbe\u786e\u7684\u7ebf\u6027\u7a7a\u95f4\u7ea6\u675f\u6ee1\u8db3\uff0c\u540c\u65f6\u4fdd\u6301\u8fd0\u52a8\u81ea\u7136\u6027\uff0c\u4e3a\u52a8\u753b\u4efb\u52a1\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u8bad\u7ec3\u514d\u8d39\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23266", "categories": ["cs.CL"], "pdf": "https://arxiv.org/pdf/2602.23266", "abs": "https://arxiv.org/abs/2602.23266", "authors": ["Siyuan Liu", "Jiahui Xu", "Feng Jiang", "Kuang Wang", "Zefeng Zhao", "Chu-Ren Huang", "Jinghang Gu", "Changqing Yin", "Haizhou Li"], "title": "Discourse-Aware Dual-Track Streaming Response for Low-Latency Spoken Dialogue Systems", "comment": null, "summary": "Achieving human-like responsiveness is a critical yet challenging goal for cascaded spoken dialogue systems. Conventional ASR-LLM-TTS pipelines follow a strictly sequential paradigm, requiring complete transcription and full reasoning before speech synthesis can begin, which results in high response latency. We propose the Discourse-Aware Dual-Track Streaming Response (DDTSR) framework, a low-latency architecture that enables listen-while-thinking and speak-while-thinking. DDTSR is built upon three key mechanisms: (1) connective-guided small-large model synergy, where an auxiliary small model generates minimal-committal discourse connectives while a large model performs knowledge-intensive reasoning in parallel; (2) streaming-based cross-modal collaboration, which dynamically overlaps ASR, LLM inference, and TTS to advance the earliest speakable moment; and (3) curriculum-learning-based discourse continuity enhancement, which maintains coherence and logical consistency between early responses and subsequent reasoning outputs. Experiments on two spoken dialogue benchmarks demonstrate that DDTSR reduces response latency by 19%-51% while preserving discourse quality. Further analysis shows that DDTSR functions as a plug-and-play module compatible with diverse LLM backbones, and remains robust across varying utterance lengths, indicating strong practicality and scalability for real-time spoken interaction.", "AI": {"tldr": "DDTSR\u6846\u67b6\u901a\u8fc7\u8fde\u63a5\u8bcd\u5f15\u5bfc\u7684\u5927\u5c0f\u6a21\u578b\u534f\u540c\u3001\u6d41\u5f0f\u8de8\u6a21\u6001\u534f\u4f5c\u548c\u8bfe\u7a0b\u5b66\u4e60\u589e\u5f3a\uff0c\u663e\u8457\u964d\u4f4e\u53e3\u8bed\u5bf9\u8bdd\u7cfb\u7edf\u54cd\u5e94\u5ef6\u8fdf19%-51%\uff0c\u5b9e\u73b0\"\u8fb9\u542c\u8fb9\u60f3\"\u548c\"\u8fb9\u8bf4\u8fb9\u60f3\"\u3002", "motivation": "\u4f20\u7edfASR-LLM-TTS\u7ea7\u8054\u5bf9\u8bdd\u7cfb\u7edf\u91c7\u7528\u4e25\u683c\u4e32\u884c\u5904\u7406\uff0c\u9700\u8981\u5b8c\u6574\u8f6c\u5f55\u548c\u63a8\u7406\u540e\u624d\u80fd\u5f00\u59cb\u8bed\u97f3\u5408\u6210\uff0c\u5bfc\u81f4\u54cd\u5e94\u5ef6\u8fdf\u9ad8\uff0c\u65e0\u6cd5\u5b9e\u73b0\u4eba\u7c7b\u822c\u7684\u5b9e\u65f6\u4ea4\u4e92\u3002", "method": "\u63d0\u51faDDTSR\u6846\u67b6\uff0c\u5305\u542b\u4e09\u4e2a\u5173\u952e\u6280\u672f\uff1a1)\u8fde\u63a5\u8bcd\u5f15\u5bfc\u7684\u5927\u5c0f\u6a21\u578b\u534f\u540c\uff0c\u5c0f\u6a21\u578b\u751f\u6210\u6700\u5c0f\u627f\u8bfa\u7684\u8fde\u63a5\u8bcd\uff0c\u5927\u6a21\u578b\u5e76\u884c\u8fdb\u884c\u77e5\u8bc6\u5bc6\u96c6\u578b\u63a8\u7406\uff1b2)\u6d41\u5f0f\u8de8\u6a21\u6001\u534f\u4f5c\uff0c\u52a8\u6001\u91cd\u53e0ASR\u3001LLM\u63a8\u7406\u548cTTS\u5904\u7406\uff1b3)\u8bfe\u7a0b\u5b66\u4e60\u589e\u5f3a\u8bdd\u8bed\u8fde\u7eed\u6027\uff0c\u4fdd\u6301\u65e9\u671f\u54cd\u5e94\u4e0e\u540e\u7eed\u63a8\u7406\u7684\u4e00\u81f4\u6027\u3002", "result": "\u5728\u4e24\u4e2a\u53e3\u8bed\u5bf9\u8bdd\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cDDTSR\u5c06\u54cd\u5e94\u5ef6\u8fdf\u964d\u4f4e19%-51%\uff0c\u540c\u65f6\u4fdd\u6301\u8bdd\u8bed\u8d28\u91cf\u3002\u8be5\u6846\u67b6\u53ef\u4f5c\u4e3a\u5373\u63d2\u5373\u7528\u6a21\u5757\u517c\u5bb9\u4e0d\u540cLLM\u9aa8\u5e72\uff0c\u5728\u4e0d\u540c\u8bdd\u8bed\u957f\u5ea6\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "conclusion": "DDTSR\u901a\u8fc7\u521b\u65b0\u7684\u53cc\u8f68\u6d41\u5f0f\u54cd\u5e94\u67b6\u6784\uff0c\u663e\u8457\u964d\u4f4e\u4e86\u53e3\u8bed\u5bf9\u8bdd\u7cfb\u7edf\u7684\u54cd\u5e94\u5ef6\u8fdf\uff0c\u5b9e\u73b0\u4e86\u66f4\u63a5\u8fd1\u4eba\u7c7b\u4ea4\u4e92\u7684\u5b9e\u65f6\u54cd\u5e94\u80fd\u529b\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2602.22971", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22971", "abs": "https://arxiv.org/abs/2602.22971", "authors": ["Peiyao Xiao", "Xiaogang Li", "Chengliang Xu", "Jiayi Wang", "Ben Wang", "Zichao Chen", "Zeyu Wang", "Kejun Yu", "Yueqian Chen", "Xulin Liu", "Wende Xiao", "Bing Zhao", "Hu Wei"], "title": "SPM-Bench: Benchmarking Large Language Models for Scanning Probe Microscopy", "comment": null, "summary": "As LLMs achieved breakthroughs in general reasoning, their proficiency in specialized scientific domains reveals pronounced gaps in existing benchmarks due to data contamination, insufficient complexity, and prohibitive human labor costs. Here we present SPM-Bench, an original, PhD-level multimodal benchmark specifically designed for scanning probe microscopy (SPM). We propose a fully automated data synthesis pipeline that ensures both high authority and low-cost. By employing Anchor-Gated Sieve (AGS) technology, we efficiently extract high-value image-text pairs from arXiv and journal papers published between 2023 and 2025. Through a hybrid cloud-local architecture where VLMs return only spatial coordinates \"llbox\" for local high-fidelity cropping, our pipeline achieves extreme token savings while maintaining high dataset purity. To accurately and objectively evaluate the performance of the LLMs, we introduce the Strict Imperfection Penalty F1 (SIP-F1) score. This metric not only establishes a rigorous capability hierarchy but also, for the first time, quantifies model \"personalities\" (Conservative, Aggressive, Gambler, or Wise). By correlating these results with model-reported confidence and perceived difficulty, we expose the true reasoning boundaries of current AI in complex physical scenarios. These insights establish SPM-Bench as a generalizable paradigm for automated scientific data synthesis.", "AI": {"tldr": "SPM-Bench\u662f\u4e00\u4e2a\u9488\u5bf9\u626b\u63cf\u63a2\u9488\u663e\u5fae\u955c\u7684\u535a\u58eb\u7ea7\u591a\u6a21\u6001\u57fa\u51c6\u6d4b\u8bd5\uff0c\u901a\u8fc7\u81ea\u52a8\u5316\u6570\u636e\u5408\u6210\u7ba1\u9053\u548c\u521b\u65b0\u7684\u8bc4\u4f30\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86AI\u5728\u590d\u6742\u79d1\u5b66\u63a8\u7406\u4e2d\u7684\u8fb9\u754c\u3002", "motivation": "\u73b0\u6709\u79d1\u5b66\u9886\u57df\u57fa\u51c6\u6d4b\u8bd5\u5b58\u5728\u6570\u636e\u6c61\u67d3\u3001\u590d\u6742\u5ea6\u4e0d\u8db3\u548c\u4eba\u5de5\u6210\u672c\u8fc7\u9ad8\u7684\u95ee\u9898\uff0c\u9700\u8981\u521b\u5efa\u4e13\u95e8\u9488\u5bf9\u626b\u63cf\u63a2\u9488\u663e\u5fae\u955c\u7684\u9ad8\u8d28\u91cf\u3001\u4f4e\u6210\u672c\u7684\u8bc4\u4f30\u57fa\u51c6\u3002", "method": "1) \u4f7f\u7528Anchor-Gated Sieve\u6280\u672f\u4ecearXiv\u548c\u671f\u520a\u8bba\u6587\u4e2d\u63d0\u53d6\u9ad8\u8d28\u91cf\u56fe\u50cf-\u6587\u672c\u5bf9\uff1b2) \u91c7\u7528\u6df7\u5408\u4e91-\u672c\u5730\u67b6\u6784\uff0cVLMs\u4ec5\u8fd4\u56de\u7a7a\u95f4\u5750\u6807\u8fdb\u884c\u672c\u5730\u9ad8\u4fdd\u771f\u88c1\u526a\uff1b3) \u5f15\u5165Strict Imperfection Penalty F1 (SIP-F1)\u8bc4\u5206\u8fdb\u884c\u5ba2\u89c2\u8bc4\u4f30\u3002", "result": "\u5efa\u7acb\u4e86SPM-Bench\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b9e\u73b0\u4e86\u6781\u7aef\u4ee4\u724c\u8282\u7701\u548c\u9ad8\u6570\u636e\u96c6\u7eaf\u5ea6\uff0c\u9996\u6b21\u91cf\u5316\u4e86\u6a21\u578b\u7684\"\u4e2a\u6027\"\u7c7b\u578b\uff08\u4fdd\u5b88\u578b\u3001\u6fc0\u8fdb\u578b\u3001\u8d4c\u5f92\u578b\u3001\u660e\u667a\u578b\uff09\uff0c\u5e76\u63ed\u793a\u4e86AI\u5728\u590d\u6742\u7269\u7406\u573a\u666f\u4e2d\u7684\u771f\u5b9e\u63a8\u7406\u8fb9\u754c\u3002", "conclusion": "SPM-Bench\u4e3a\u81ea\u52a8\u5316\u79d1\u5b66\u6570\u636e\u5408\u6210\u63d0\u4f9b\u4e86\u4e00\u4e2a\u53ef\u63a8\u5e7f\u7684\u8303\u5f0f\uff0c\u80fd\u591f\u51c6\u786e\u8bc4\u4f30LLMs\u5728\u4e13\u4e1a\u79d1\u5b66\u9886\u57df\u7684\u63a8\u7406\u80fd\u529b\uff0c\u5e76\u91cf\u5316\u6a21\u578b\u7684\u884c\u4e3a\u7279\u5f81\u3002"}}
{"id": "2602.22745", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22745", "abs": "https://arxiv.org/abs/2602.22745", "authors": ["Fengming Liu", "Tat-Jen Cham", "Chuanxia Zheng"], "title": "SPATIALALIGN: Aligning Dynamic Spatial Relationships in Video Generation", "comment": null, "summary": "Most text-to-video (T2V) generators prioritize aesthetic quality, but often ignoring the spatial constraints in the generated videos. In this work, we present SPATIALALIGN, a self-improvement framework that enhances T2V models capabilities to depict Dynamic Spatial Relationships (DSR) specified in text prompts. We present a zeroth-order regularized Direct Preference Optimization (DPO) to fine-tune T2V models towards better alignment with DSR. Specifically, we design DSR-SCORE, a geometry-based metric that quantitatively measures the alignment between generated videos and the specified DSRs in prompts, which is a step forward from prior works that rely on VLM for evaluation. We also conduct a dataset of text-video pairs with diverse DSRs to facilitate the study. Extensive experiments demonstrate that our fine-tuned model significantly out performs the baseline in spatial relationships. The code will be released in Link.", "AI": {"tldr": "SPATIALALIGN\u662f\u4e00\u4e2a\u81ea\u6539\u8fdb\u6846\u67b6\uff0c\u901a\u8fc7\u96f6\u9636\u6b63\u5219\u5316DPO\u5fae\u8c03T2V\u6a21\u578b\uff0c\u63d0\u5347\u5176\u5bf9\u6587\u672c\u63d0\u793a\u4e2d\u52a8\u6001\u7a7a\u95f4\u5173\u7cfb\uff08DSR\uff09\u7684\u63cf\u7ed8\u80fd\u529b\uff0c\u5e76\u8bbe\u8ba1\u4e86\u57fa\u4e8e\u51e0\u4f55\u7684DSR-SCORE\u8bc4\u4f30\u6307\u6807\u3002", "motivation": "\u5f53\u524d\u6587\u672c\u5230\u89c6\u9891\uff08T2V\uff09\u751f\u6210\u5668\u8fc7\u4e8e\u6ce8\u91cd\u7f8e\u5b66\u8d28\u91cf\uff0c\u800c\u5ffd\u89c6\u4e86\u751f\u6210\u89c6\u9891\u4e2d\u7684\u7a7a\u95f4\u7ea6\u675f\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u63cf\u7ed8\u6587\u672c\u63d0\u793a\u4e2d\u6307\u5b9a\u7684\u52a8\u6001\u7a7a\u95f4\u5173\u7cfb\uff08DSR\uff09\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\u3002", "method": "1. \u63d0\u51faSPATIALALIGN\u81ea\u6539\u8fdb\u6846\u67b6\uff1b2. \u4f7f\u7528\u96f6\u9636\u6b63\u5219\u5316\u76f4\u63a5\u504f\u597d\u4f18\u5316\uff08DPO\uff09\u5fae\u8c03T2V\u6a21\u578b\uff1b3. \u8bbe\u8ba1\u57fa\u4e8e\u51e0\u4f55\u7684DSR-SCORE\u8bc4\u4f30\u6307\u6807\uff1b4. \u6784\u5efa\u5305\u542b\u591a\u6837DSR\u7684\u6587\u672c-\u89c6\u9891\u5bf9\u6570\u636e\u96c6\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u7ecf\u8fc7\u5fae\u8c03\u7684\u6a21\u578b\u5728\u7a7a\u95f4\u5173\u7cfb\u63cf\u7ed8\u65b9\u9762\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002DSR-SCORE\u63d0\u4f9b\u4e86\u6bd4\u4f9d\u8d56VLM\u8bc4\u4f30\u66f4\u7cbe\u786e\u7684\u7a7a\u95f4\u5bf9\u9f50\u91cf\u5316\u6d4b\u91cf\u3002", "conclusion": "SPATIALALIGN\u6846\u67b6\u6709\u6548\u63d0\u5347\u4e86T2V\u6a21\u578b\u5bf9\u52a8\u6001\u7a7a\u95f4\u5173\u7cfb\u7684\u7406\u89e3\u4e0e\u751f\u6210\u80fd\u529b\uff0c\u4e3a\u7a7a\u95f4\u7ea6\u675f\u7684\u89c6\u9891\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u548c\u8bc4\u4f30\u65b9\u6cd5\u3002"}}
{"id": "2602.22973", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22973", "abs": "https://arxiv.org/abs/2602.22973", "authors": ["Dimitrios P. Panagoulias", "Evangelia-Aikaterini Tsichrintzi", "Georgios Savvidis", "Evridiki Tsoureli-Nikita"], "title": "Modeling Expert AI Diagnostic Alignment via Immutable Inference Snapshots", "comment": null, "summary": "Human-in-the-loop validation is essential in safety-critical clinical AI, yet the transition between initial model inference and expert correction is rarely analyzed as a structured signal. We introduce a diagnostic alignment framework in which the AI-generated image based report is preserved as an immutable inference state and systematically compared with the physician-validated outcome. The inference pipeline integrates a vision-enabled large language model, BERT- based medical entity extraction, and a Sequential Language Model Inference (SLMI) step to enforce domain-consistent refinement prior to expert review. Evaluation on 21 dermatological cases (21 complete AI physician pairs) em- ployed a four-level concordance framework comprising exact primary match rate (PMR), semantic similarity-adjusted rate (AMR), cross-category alignment, and Comprehensive Concordance Rate (CCR). Exact agreement reached 71.4% and remained unchanged under semantic similarity (t = 0.60), while structured cross-category and differential overlap analysis yielded 100% comprehensive concordance (95% CI: [83.9%, 100%]). No cases demonstrated complete diagnostic divergence. These findings show that binary lexical evaluation substantially un- derestimates clinically meaningful alignment. Modeling expert validation as a structured transformation enables signal-aware quantification of correction dynamics and supports traceable, human aligned evaluation of image based clinical decision support systems.", "AI": {"tldr": "\u63d0\u51fa\u8bca\u65ad\u5bf9\u9f50\u6846\u67b6\uff0c\u5c06AI\u751f\u6210\u7684\u5f71\u50cf\u62a5\u544a\u4f5c\u4e3a\u4e0d\u53ef\u53d8\u63a8\u7406\u72b6\u6001\uff0c\u4e0e\u533b\u751f\u9a8c\u8bc1\u7ed3\u679c\u7cfb\u7edf\u6bd4\u8f83\uff0c\u901a\u8fc7\u591a\u7ea7\u4e00\u81f4\u6027\u8bc4\u4f30\u663e\u793a\u4e34\u5e8a\u5bf9\u9f50\u4f18\u4e8e\u5355\u7eaf\u8bcd\u6c47\u5339\u914d", "motivation": "\u5728\u5b89\u5168\u5173\u952e\u7684\u4e34\u5e8aAI\u4e2d\uff0c\u4eba\u5de5\u9a8c\u8bc1\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u6a21\u578b\u63a8\u7406\u4e0e\u4e13\u5bb6\u4fee\u6b63\u4e4b\u95f4\u7684\u8fc7\u6e21\u5f88\u5c11\u88ab\u5206\u6790\u4e3a\u7ed3\u6784\u5316\u4fe1\u53f7\u3002\u9700\u8981\u4e00\u79cd\u80fd\u591f\u91cf\u5316\u4fee\u6b63\u52a8\u6001\u5e76\u652f\u6301\u53ef\u8ffd\u6eaf\u3001\u4eba\u7c7b\u5bf9\u9f50\u8bc4\u4f30\u7684\u65b9\u6cd5\u3002", "method": "\u5f15\u5165\u8bca\u65ad\u5bf9\u9f50\u6846\u67b6\uff1a1) \u5c06AI\u751f\u6210\u7684\u5f71\u50cf\u62a5\u544a\u4fdd\u5b58\u4e3a\u4e0d\u53ef\u53d8\u63a8\u7406\u72b6\u6001\uff1b2) \u7cfb\u7edf\u6bd4\u8f83\u533b\u751f\u9a8c\u8bc1\u7ed3\u679c\uff1b3) \u63a8\u7406\u7ba1\u9053\u96c6\u6210\u89c6\u89c9\u5927\u8bed\u8a00\u6a21\u578b\u3001\u57fa\u4e8eBERT\u7684\u533b\u5b66\u5b9e\u4f53\u63d0\u53d6\u548c\u5e8f\u5217\u8bed\u8a00\u6a21\u578b\u63a8\u7406(SLMI)\u6b65\u9aa4\uff1b4) \u4f7f\u7528\u56db\u7ea7\u4e00\u81f4\u6027\u6846\u67b6\u8bc4\u4f30\uff1a\u7cbe\u786e\u4e3b\u8981\u5339\u914d\u7387\u3001\u8bed\u4e49\u76f8\u4f3c\u5ea6\u8c03\u6574\u7387\u3001\u8de8\u7c7b\u522b\u5bf9\u9f50\u548c\u7efc\u5408\u4e00\u81f4\u6027\u7387\u3002", "result": "\u572821\u4e2a\u76ae\u80a4\u75c5\u6848\u4f8b\u4e2d\uff1a\u7cbe\u786e\u4e00\u81f4\u7387\u8fbe\u523071.4%\uff0c\u8bed\u4e49\u76f8\u4f3c\u5ea6\u8c03\u6574\u540e\u4fdd\u6301\u4e0d\u53d8\uff1b\u7ed3\u6784\u5316\u8de8\u7c7b\u522b\u548c\u9274\u522b\u8bca\u65ad\u91cd\u53e0\u5206\u6790\u8fbe\u5230100%\u7efc\u5408\u4e00\u81f4\u6027\uff1b\u6ca1\u6709\u6848\u4f8b\u663e\u793a\u5b8c\u5168\u8bca\u65ad\u5206\u6b67\u3002\u4e8c\u5143\u8bcd\u6c47\u8bc4\u4f30\u663e\u8457\u4f4e\u4f30\u4e86\u4e34\u5e8a\u610f\u4e49\u4e0a\u7684\u5bf9\u9f50\u3002", "conclusion": "\u5c06\u4e13\u5bb6\u9a8c\u8bc1\u5efa\u6a21\u4e3a\u7ed3\u6784\u5316\u8f6c\u6362\uff0c\u80fd\u591f\u5b9e\u73b0\u4fe1\u53f7\u611f\u77e5\u7684\u4fee\u6b63\u52a8\u6001\u91cf\u5316\uff0c\u5e76\u652f\u6301\u57fa\u4e8e\u5f71\u50cf\u7684\u4e34\u5e8a\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u53ef\u8ffd\u6eaf\u3001\u4eba\u7c7b\u5bf9\u9f50\u8bc4\u4f30\u3002\u4e34\u5e8a\u5bf9\u9f50\u8fdc\u8d85\u8fc7\u5355\u7eaf\u7684\u8bcd\u6c47\u5339\u914d\u3002"}}
{"id": "2602.22759", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22759", "abs": "https://arxiv.org/abs/2602.22759", "authors": ["Yuan-Chih Chen", "Chun-Shien Lu"], "title": "Beyond Detection: Multi-Scale Hidden-Code for Natural Image Deepfake Recovery and Factual Retrieval", "comment": null, "summary": "Recent advances in image authenticity have primarily focused on deepfake detection and localization, leaving recovery of tampered contents for factual retrieval relatively underexplored. We propose a unified hidden-code recovery framework that enables both retrieval and restoration from post-hoc and in-generation watermarking paradigms. Our method encodes semantic and perceptual information into a compact hidden-code representation, refined through multi-scale vector quantization, and enhances contextual reasoning via conditional Transformer modules. To enable systematic evaluation for natural images, we construct ImageNet-S, a benchmark that provides paired image-label factual retrieval tasks. Extensive experiments on ImageNet-S demonstrate that our method exhibits promising retrieval and reconstruction performance while remaining fully compatible with diverse watermarking pipelines. This framework establishes a foundation for general-purpose image recovery beyond detection and localization.", "AI": {"tldr": "\u63d0\u51fa\u7edf\u4e00\u9690\u85cf\u7801\u6062\u590d\u6846\u67b6\uff0c\u5b9e\u73b0\u7be1\u6539\u56fe\u50cf\u7684\u68c0\u7d22\u4e0e\u6062\u590d\uff0c\u6784\u5efaImageNet-S\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5728\u591a\u79cd\u6c34\u5370\u65b9\u6848\u4e2d\u5c55\u73b0\u826f\u597d\u6027\u80fd", "motivation": "\u5f53\u524d\u56fe\u50cf\u771f\u5b9e\u6027\u7814\u7a76\u4e3b\u8981\u96c6\u4e2d\u5728\u6df1\u5ea6\u4f2a\u9020\u68c0\u6d4b\u548c\u5b9a\u4f4d\uff0c\u800c\u7be1\u6539\u5185\u5bb9\u7684\u6062\u590d\u548c\u4e8b\u5b9e\u68c0\u7d22\u76f8\u5bf9\u672a\u88ab\u5145\u5206\u63a2\u7d22\u3002\u9700\u8981\u5efa\u7acb\u8d85\u8d8a\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7684\u901a\u7528\u56fe\u50cf\u6062\u590d\u57fa\u7840", "method": "\u63d0\u51fa\u7edf\u4e00\u9690\u85cf\u7801\u6062\u590d\u6846\u67b6\uff0c\u5c06\u8bed\u4e49\u548c\u611f\u77e5\u4fe1\u606f\u7f16\u7801\u4e3a\u7d27\u51d1\u9690\u85cf\u7801\u8868\u793a\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u5411\u91cf\u91cf\u5316\u8fdb\u884c\u7cbe\u70bc\uff0c\u5e76\u5229\u7528\u6761\u4ef6Transformer\u6a21\u5757\u589e\u5f3a\u4e0a\u4e0b\u6587\u63a8\u7406\u80fd\u529b", "result": "\u5728ImageNet-S\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5c55\u73b0\u51fa\u6709\u524d\u666f\u7684\u68c0\u7d22\u548c\u91cd\u5efa\u6027\u80fd\uff0c\u540c\u65f6\u5b8c\u5168\u517c\u5bb9\u591a\u79cd\u6c34\u5370\u7ba1\u9053", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u8d85\u8d8a\u68c0\u6d4b\u548c\u5b9a\u4f4d\u7684\u901a\u7528\u56fe\u50cf\u6062\u590d\u5960\u5b9a\u4e86\u57fa\u7840\uff0c\u4e3a\u540e\u5904\u7406\u548c\u751f\u6210\u8fc7\u7a0b\u4e2d\u7684\u6c34\u5370\u8303\u5f0f\u63d0\u4f9b\u4e86\u7edf\u4e00\u7684\u6062\u590d\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.23300", "categories": ["cs.CL", "eess.AS"], "pdf": "https://arxiv.org/pdf/2602.23300", "abs": "https://arxiv.org/abs/2602.23300", "authors": ["Soumya Dutta", "Smruthi Balaji", "Sriram Ganapathy"], "title": "A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations", "comment": "Accepted to Elsevier Computer Speech and Language. 30 pages, 9 figures, 5 tables", "summary": "Emotion Recognition in Conversations (ERC) presents unique challenges, requiring models to capture the temporal flow of multi-turn dialogues and to effectively integrate cues from multiple modalities. We propose Mixture of Speech-Text Experts for Recognition of Emotions (MiSTER-E), a modular Mixture-of-Experts (MoE) framework designed to decouple two core challenges in ERC: modality-specific context modeling and multimodal information fusion. MiSTER-E leverages large language models (LLMs) fine-tuned for both speech and text to provide rich utterance-level embeddings, which are then enhanced through a convolutional-recurrent context modeling layer. The system integrates predictions from three experts-speech-only, text-only, and cross-modal-using a learned gating mechanism that dynamically weighs their outputs. To further encourage consistency and alignment across modalities, we introduce a supervised contrastive loss between paired speech-text representations and a KL-divergence-based regulariza-tion across expert predictions. Importantly, MiSTER-E does not rely on speaker identity at any stage. Experiments on three benchmark datasets-IEMOCAP, MELD, and MOSI-show that our proposal achieves 70.9%, 69.5%, and 87.9% weighted F1-scores respectively, outperforming several baseline speech-text ERC systems. We also provide various ablations to highlight the contributions made in the proposed approach.", "AI": {"tldr": "MiSTER-E\u662f\u4e00\u4e2a\u7528\u4e8e\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\u7684\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\uff0c\u901a\u8fc7\u5206\u79bb\u6a21\u6001\u7279\u5b9a\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u4e24\u4e2a\u6838\u5fc3\u6311\u6218\uff0c\u5728\u4e0d\u4f9d\u8d56\u8bf4\u8bdd\u8005\u8eab\u4efd\u7684\u60c5\u51b5\u4e0b\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u53d6\u5f97\u4e86\u4f18\u4e8e\u57fa\u7ebf\u7cfb\u7edf\u7684\u6027\u80fd\u3002", "motivation": "\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\u9762\u4e34\u72ec\u7279\u6311\u6218\uff0c\u9700\u8981\u6a21\u578b\u6355\u6349\u591a\u8f6e\u5bf9\u8bdd\u7684\u65f6\u95f4\u6d41\u5e76\u6709\u6548\u6574\u5408\u591a\u6a21\u6001\u7ebf\u7d22\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u6a21\u6001\u7279\u5b9a\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u591a\u6a21\u6001\u4fe1\u606f\u878d\u5408\u65b9\u9762\u5b58\u5728\u8026\u5408\u95ee\u9898\u3002", "method": "\u63d0\u51faMiSTER-E\u6846\u67b6\uff1a1) \u4f7f\u7528\u9488\u5bf9\u8bed\u97f3\u548c\u6587\u672c\u5fae\u8c03\u7684\u5927\u8bed\u8a00\u6a21\u578b\u83b7\u53d6\u4e30\u5bcc\u7684\u8bed\u53e5\u7ea7\u5d4c\u5165\uff1b2) \u901a\u8fc7\u5377\u79ef-\u5faa\u73af\u4e0a\u4e0b\u6587\u5efa\u6a21\u5c42\u589e\u5f3a\u8868\u793a\uff1b3) \u96c6\u6210\u4e09\u4e2a\u4e13\u5bb6\uff08\u8bed\u97f3\u3001\u6587\u672c\u3001\u8de8\u6a21\u6001\uff09\u7684\u9884\u6d4b\uff0c\u4f7f\u7528\u5b66\u4e60\u95e8\u63a7\u673a\u5236\u52a8\u6001\u52a0\u6743\uff1b4) \u5f15\u5165\u76d1\u7763\u5bf9\u6bd4\u635f\u5931\u548cKL\u6563\u5ea6\u6b63\u5219\u5316\u6765\u589e\u5f3a\u6a21\u6001\u95f4\u4e00\u81f4\u6027\u548c\u5bf9\u9f50\u3002", "result": "\u5728IEMOCAP\u3001MELD\u548cMOSI\u4e09\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u5206\u522b\u83b7\u5f9770.9%\u300169.5%\u548c87.9%\u7684\u52a0\u6743F1\u5206\u6570\uff0c\u4f18\u4e8e\u591a\u4e2a\u57fa\u7ebf\u8bed\u97f3-\u6587\u672cERC\u7cfb\u7edf\u3002\u6d88\u878d\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u6240\u63d0\u65b9\u6cd5\u7684\u8d21\u732e\u3002", "conclusion": "MiSTER-E\u901a\u8fc7\u6a21\u5757\u5316\u7684\u6df7\u5408\u4e13\u5bb6\u6846\u67b6\u6210\u529f\u5206\u79bb\u4e86ERC\u4e2d\u7684\u4e24\u4e2a\u6838\u5fc3\u6311\u6218\uff0c\u5728\u4e0d\u4f9d\u8d56\u8bf4\u8bdd\u8005\u8eab\u4efd\u7684\u60c5\u51b5\u4e0b\u5b9e\u73b0\u4e86\u4f18\u5f02\u7684\u6027\u80fd\uff0c\u4e3a\u591a\u6a21\u6001\u5bf9\u8bdd\u60c5\u611f\u8bc6\u522b\u63d0\u4f9b\u4e86\u6709\u6548\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22981", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.22981", "abs": "https://arxiv.org/abs/2602.22981", "authors": ["Haohui Jia", "Zheng Chen", "Lingwei Zhu", "Xu Cao", "Yasuko Matsubara", "Takashi Matsubara", "Yasushi Sakurai"], "title": "RepSPD: Enhancing SPD Manifold Representation in EEGs via Dynamic Graphs", "comment": null, "summary": "Decoding brain activity from electroencephalography (EEG) is crucial for neuroscience and clinical applications. Among recent advances in deep learning for EEG, geometric learning stands out as its theoretical underpinnings on symmetric positive definite (SPD) allows revealing structural connectivity analysis in a physics-grounded manner. However, current SPD-based methods focus predominantly on statistical aggregation of EEGs, with frequency-specific synchronization and local topological structures of brain regions neglected. Given this, we propose RepSPD, a novel geometric deep learning (GDL)-based model. RepSPD implements a cross-attention mechanism on the Riemannian manifold to modulate the geometric attributes of SPD with graph-derived functional connectivity features. On top of this, we introduce a global bidirectional alignment strategy to reshape tangent-space embeddings, mitigating geometric distortions caused by curvature and thereby enhancing geometric consistency. Extensive experiments demonstrate that our proposed framework significantly outperforms existing EEG representation methods, exhibiting superior robustness and generalization capabilities.", "AI": {"tldr": "\u63d0\u51faRepSPD\u6a21\u578b\uff0c\u901a\u8fc7\u9ece\u66fc\u6d41\u5f62\u4e0a\u7684\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\u548c\u5168\u5c40\u53cc\u5411\u5bf9\u9f50\u7b56\u7565\uff0c\u6539\u8fdb\u57fa\u4e8e\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635\u7684\u8111\u7535\u4fe1\u53f7\u89e3\u7801\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u6027\u80fd", "motivation": "\u5f53\u524d\u57fa\u4e8e\u5bf9\u79f0\u6b63\u5b9a\u77e9\u9635\u7684\u8111\u7535\u5206\u6790\u65b9\u6cd5\u4e3b\u8981\u5173\u6ce8\u7edf\u8ba1\u805a\u5408\uff0c\u5ffd\u7565\u4e86\u9891\u7387\u7279\u5f02\u6027\u540c\u6b65\u548c\u8111\u533a\u5c40\u90e8\u62d3\u6251\u7ed3\u6784\uff0c\u9700\u8981\u66f4\u5168\u9762\u7684\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u65b9\u6cd5\u6765\u6539\u8fdb\u8111\u7535\u89e3\u7801", "method": "\u63d0\u51faRepSPD\u6a21\u578b\uff1a1\uff09\u5728\u9ece\u66fc\u6d41\u5f62\u4e0a\u5b9e\u73b0\u4ea4\u53c9\u6ce8\u610f\u529b\u673a\u5236\uff0c\u7528\u56fe\u5bfc\u51fa\u7684\u529f\u80fd\u8fde\u63a5\u7279\u5f81\u8c03\u5236SPD\u7684\u51e0\u4f55\u5c5e\u6027\uff1b2\uff09\u5f15\u5165\u5168\u5c40\u53cc\u5411\u5bf9\u9f50\u7b56\u7565\u91cd\u5851\u5207\u7a7a\u95f4\u5d4c\u5165\uff0c\u51cf\u8f7b\u66f2\u7387\u5f15\u8d77\u7684\u51e0\u4f55\u5931\u771f", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u6846\u67b6\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u7684\u8111\u7535\u8868\u793a\u65b9\u6cd5\uff0c\u5c55\u73b0\u51fa\u5353\u8d8a\u7684\u9c81\u68d2\u6027\u548c\u6cdb\u5316\u80fd\u529b", "conclusion": "RepSPD\u901a\u8fc7\u7ed3\u5408\u51e0\u4f55\u6df1\u5ea6\u5b66\u4e60\u548c\u529f\u80fd\u8fde\u63a5\u7279\u5f81\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u5f53\u524dSPD\u65b9\u6cd5\u7684\u5c40\u9650\u6027\uff0c\u4e3a\u8111\u7535\u89e3\u7801\u63d0\u4f9b\u4e86\u66f4\u5f3a\u5927\u7684\u51e0\u4f55\u4e00\u81f4\u6027\u8868\u793a"}}
{"id": "2602.22779", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22779", "abs": "https://arxiv.org/abs/2602.22779", "authors": ["Chenhao Zheng", "Jieyu Zhang", "Jianing Zhang", "Weikai Huang", "Ashutosh Kumar", "Quan Kong", "Oncel Tuzel", "Chun-Liang Li", "Ranjay Krishna"], "title": "TrajTok: Learning Trajectory Tokens enables better Video Understanding", "comment": "CVPR 2026", "summary": "Tokenization in video models, typically through patchification, generates an excessive and redundant number of tokens. This severely limits video efficiency and scalability. While recent trajectory-based tokenizers offer a promising solution by decoupling video duration from token count, they rely on complex external segmentation and tracking pipelines that are slow and task-agnostic. We propose TrajTok, an end-to-end video tokenizer module that is fully integrated and co-trained with video models for a downstream objective, dynamically adapting its token granularity to semantic complexity, independent of video duration. TrajTok contains a unified segmenter that performs implicit clustering over pixels in both space and time to directly produce object trajectories in a single forward pass. By prioritizing downstream adaptability over pixel-perfect segmentation fidelity, TrajTok is lightweight and efficient, yet empirically improves video understanding performance. With TrajTok, we implement a video CLIP model trained from scratch (TrajViT2). It achieves the best accuracy at scale across both classification and retrieval benchmarks, while maintaining efficiency comparable to the best token-merging methods. TrajTok also proves to be a versatile component beyond its role as a tokenizer. We show that it can be seamlessly integrated as either a probing head for pretrained visual features (TrajAdapter) or an alignment connector in vision-language models (TrajVLM) with especially strong performance in long-video reasoning.", "AI": {"tldr": "TrajTok\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u7684\u89c6\u9891\u5206\u8bcd\u5668\u6a21\u5757\uff0c\u901a\u8fc7\u7edf\u4e00\u7684\u7247\u6bb5\u5206\u5272\u5668\u5728\u65f6\u7a7a\u7ef4\u5ea6\u4e0a\u9690\u5f0f\u805a\u7c7b\u50cf\u7d20\uff0c\u76f4\u63a5\u751f\u6210\u7269\u4f53\u8f68\u8ff9\uff0c\u5b9e\u73b0\u4e0e\u89c6\u9891\u65f6\u957f\u65e0\u5173\u7684\u52a8\u6001\u5206\u8bcd\u7c92\u5ea6\uff0c\u63d0\u5347\u89c6\u9891\u7406\u89e3\u6548\u7387\u4e0e\u6027\u80fd\u3002", "motivation": "\u4f20\u7edf\u89c6\u9891\u6a21\u578b\u901a\u8fc7\u5206\u5757\u5316\u8fdb\u884c\u5206\u8bcd\u4f1a\u4ea7\u751f\u5927\u91cf\u5197\u4f59token\uff0c\u4e25\u91cd\u9650\u5236\u89c6\u9891\u5904\u7406\u6548\u7387\u548c\u53ef\u6269\u5c55\u6027\u3002\u73b0\u6709\u7684\u8f68\u8ff9\u5206\u8bcd\u5668\u867d\u7136\u80fd\u89e3\u8026\u89c6\u9891\u65f6\u957f\u4e0etoken\u6570\u91cf\uff0c\u4f46\u4f9d\u8d56\u590d\u6742\u7684\u5916\u90e8\u5206\u5272\u548c\u8ddf\u8e2a\u6d41\u7a0b\uff0c\u901f\u5ea6\u6162\u4e14\u4e0e\u4efb\u52a1\u65e0\u5173\u3002", "method": "\u63d0\u51faTrajTok\u7aef\u5230\u7aef\u89c6\u9891\u5206\u8bcd\u5668\u6a21\u5757\uff0c\u5305\u542b\u7edf\u4e00\u7684\u7247\u6bb5\u5206\u5272\u5668\uff0c\u5728\u65f6\u7a7a\u7ef4\u5ea6\u4e0a\u5bf9\u50cf\u7d20\u8fdb\u884c\u9690\u5f0f\u805a\u7c7b\uff0c\u76f4\u63a5\u751f\u6210\u7269\u4f53\u8f68\u8ff9\u3002\u8be5\u6a21\u5757\u4e0e\u89c6\u9891\u6a21\u578b\u5b8c\u5168\u96c6\u6210\u5e76\u8054\u5408\u8bad\u7ec3\uff0c\u6839\u636e\u8bed\u4e49\u590d\u6742\u5ea6\u52a8\u6001\u8c03\u6574\u5206\u8bcd\u7c92\u5ea6\u3002", "result": "TrajTok\u5b9e\u73b0\u4e86\u89c6\u9891CLIP\u6a21\u578b(TrajViT2)\uff0c\u5728\u5206\u7c7b\u548c\u68c0\u7d22\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8fbe\u5230\u6700\u4f73\u51c6\u786e\u7387\uff0c\u540c\u65f6\u4fdd\u6301\u4e0e\u6700\u4f73token\u5408\u5e76\u65b9\u6cd5\u76f8\u5f53\u7684\u6548\u7387\u3002TrajTok\u8fd8\u53ef\u4f5c\u4e3a\u9884\u8bad\u7ec3\u89c6\u89c9\u7279\u5f81\u7684\u63a2\u6d4b\u5934(TrajAdapter)\u6216\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5bf9\u9f50\u8fde\u63a5\u5668(TrajVLM)\uff0c\u5728\u957f\u89c6\u9891\u63a8\u7406\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "TrajTok\u662f\u4e00\u4e2a\u8f7b\u91cf\u9ad8\u6548\u7684\u7aef\u5230\u7aef\u89c6\u9891\u5206\u8bcd\u5668\uff0c\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u8bed\u4e49\u590d\u6742\u5ea6\u7684\u8f68\u8ff9\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u89c6\u9891\u5206\u8bcd\u4e2d\u7684\u5197\u4f59\u95ee\u9898\uff0c\u663e\u8457\u63d0\u5347\u4e86\u89c6\u9891\u7406\u89e3\u6027\u80fd\uff0c\u5e76\u5c55\u73b0\u51fa\u4f5c\u4e3a\u591a\u529f\u80fd\u7ec4\u4ef6\u7684\u6f5c\u529b\u3002"}}
{"id": "2602.22983", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2602.22983", "abs": "https://arxiv.org/abs/2602.22983", "authors": ["Xun Huang", "Simeng Qin", "Xiaoshuang Jia", "Ranjie Duan", "Huanqian Yan", "Zhitao Zeng", "Fei Yang", "Yang Liu", "Xiaojun Jia"], "title": "Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search", "comment": null, "summary": "As Large Language Models (LLMs) are increasingly used, their security risks have drawn increasing attention. Existing research reveals that LLMs are highly susceptible to jailbreak attacks, with effectiveness varying across language contexts. This paper investigates the role of classical Chinese in jailbreak attacks. Owing to its conciseness and obscurity, classical Chinese can partially bypass existing safety constraints, exposing notable vulnerabilities in LLMs. Based on this observation, this paper proposes a framework, CC-BOS, for the automatic generation of classical Chinese adversarial prompts based on multi-dimensional fruit fly optimization, facilitating efficient and automated jailbreak attacks in black-box settings. Prompts are encoded into eight policy dimensions-covering role, behavior, mechanism, metaphor, expression, knowledge, trigger pattern and context; and iteratively refined via smell search, visual search, and cauchy mutation. This design enables efficient exploration of the search space, thereby enhancing the effectiveness of black-box jailbreak attacks. To enhance readability and evaluation accuracy, we further design a classical Chinese to English translation module. Extensive experiments demonstrate that effectiveness of the proposed CC-BOS, consistently outperforming state-of-the-art jailbreak attack methods.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51faCC-BOS\u6846\u67b6\uff0c\u5229\u7528\u53e4\u5178\u4e2d\u6587\u7684\u7b80\u6d01\u6027\u548c\u6a21\u7cca\u6027\u81ea\u52a8\u751f\u6210\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u901a\u8fc7\u679c\u8747\u4f18\u5316\u7b97\u6cd5\u5728\u516b\u4e2a\u7b56\u7565\u7ef4\u5ea6\u4e0a\u8fed\u4ee3\u4f18\u5316\uff0c\u6709\u6548\u5b9e\u73b0\u9ed1\u76d2\u8d8a\u72f1\u653b\u51fb\u3002", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u98ce\u9669\u65e5\u76ca\u53d7\u5230\u5173\u6ce8\uff0c\u73b0\u6709\u7814\u7a76\u8868\u660eLLMs\u5bb9\u6613\u53d7\u5230\u8d8a\u72f1\u653b\u51fb\uff0c\u4e14\u653b\u51fb\u6548\u679c\u5728\u4e0d\u540c\u8bed\u8a00\u8bed\u5883\u4e2d\u5b58\u5728\u5dee\u5f02\u3002\u53e4\u5178\u4e2d\u6587\u56e0\u5176\u7b80\u6d01\u6027\u548c\u6a21\u7cca\u6027\uff0c\u80fd\u591f\u90e8\u5206\u7ed5\u8fc7\u73b0\u6709\u5b89\u5168\u7ea6\u675f\uff0c\u66b4\u9732LLMs\u7684\u663e\u8457\u6f0f\u6d1e\u3002", "method": "\u63d0\u51faCC-BOS\u6846\u67b6\uff1a1\uff09\u5c06\u63d0\u793a\u7f16\u7801\u4e3a\u516b\u4e2a\u7b56\u7565\u7ef4\u5ea6\uff08\u89d2\u8272\u3001\u884c\u4e3a\u3001\u673a\u5236\u3001\u9690\u55bb\u3001\u8868\u8fbe\u3001\u77e5\u8bc6\u3001\u89e6\u53d1\u6a21\u5f0f\u3001\u4e0a\u4e0b\u6587\uff09\uff1b2\uff09\u91c7\u7528\u591a\u7ef4\u679c\u8747\u4f18\u5316\u7b97\u6cd5\uff0c\u901a\u8fc7\u55c5\u89c9\u641c\u7d22\u3001\u89c6\u89c9\u641c\u7d22\u548c\u67ef\u897f\u53d8\u5f02\u8fed\u4ee3\u4f18\u5316\uff1b3\uff09\u8bbe\u8ba1\u53e4\u5178\u4e2d\u6587\u5230\u82f1\u6587\u7684\u7ffb\u8bd1\u6a21\u5757\u4ee5\u63d0\u9ad8\u53ef\u8bfb\u6027\u548c\u8bc4\u4f30\u51c6\u786e\u6027\u3002", "result": "\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCC-BOS\u6846\u67b6\u5728\u8d8a\u72f1\u653b\u51fb\u4e2d\u59cb\u7ec8\u4f18\u4e8e\u73b0\u6709\u7684\u6700\u5148\u8fdb\u65b9\u6cd5\uff0c\u8bc1\u660e\u4e86\u5176\u6709\u6548\u6027\u3002", "conclusion": "\u53e4\u5178\u4e2d\u6587\u5728\u8d8a\u72f1\u653b\u51fb\u4e2d\u5177\u6709\u72ec\u7279\u4f18\u52bf\uff0cCC-BOS\u6846\u67b6\u80fd\u591f\u9ad8\u6548\u81ea\u52a8\u751f\u6210\u5bf9\u6297\u6027\u63d0\u793a\uff0c\u663e\u8457\u63d0\u5347\u9ed1\u76d2\u8d8a\u72f1\u653b\u51fb\u7684\u6548\u679c\uff0c\u66b4\u9732\u4e86LLMs\u5728\u53e4\u5178\u4e2d\u6587\u8bed\u5883\u4e0b\u7684\u5b89\u5168\u6f0f\u6d1e\u3002"}}
{"id": "2602.22785", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22785", "abs": "https://arxiv.org/abs/2602.22785", "authors": ["Ling Wang", "Hao-Xiang Guo", "Xinzhou Wang", "Fuchun Sun", "Kai Sun", "Pengkun Liu", "Hang Xiao", "Zhong Wang", "Guangyuan Fu", "Eric Li", "Yang Liu", "Yikai Wang"], "title": "SceneTransporter: Optimal Transport-Guided Compositional Latent Diffusion for Single-Image Structured 3D Scene Generation", "comment": "published at iclr 2026", "summary": "We introduce SceneTransporter, an end-to-end framework for structured 3D scene generation from a single image. While existing methods generate part-level 3D objects, they often fail to organize these parts into distinct instances in open-world scenes. Through a debiased clustering probe, we reveal a critical insight: this failure stems from the lack of structural constraints within the model's internal assignment mechanism. Based on this finding, we reframe the task of structured 3D scene generation as a global correlation assignment problem. To solve this, SceneTransporter formulates and solves an entropic Optimal Transport (OT) objective within the denoising loop of the compositional DiT model. This formulation imposes two powerful structural constraints. First, the resulting transport plan gates cross-attention to enforce an exclusive, one-to-one routing of image patches to part-level 3D latents, preventing entanglement. Second, the competitive nature of the transport encourages the grouping of similar patches, a process that is further regularized by an edge-based cost, to form coherent objects and prevent fragmentation. Extensive experiments show that SceneTransporter outperforms existing methods on open-world scene generation, significantly improving instance-level coherence and geometric fidelity. Code and models will be publicly available at https://2019epwl.github.io/SceneTransporter/.", "AI": {"tldr": "SceneTransporter\uff1a\u4ece\u5355\u56fe\u50cf\u751f\u6210\u7ed3\u6784\u53163D\u573a\u666f\u7684\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u89e3\u51b3\u5b9e\u4f8b\u5206\u5272\u95ee\u9898", "motivation": "\u73b0\u6709\u65b9\u6cd5\u751f\u6210\u90e8\u4ef6\u7ea73D\u5bf9\u8c61\uff0c\u4f46\u65e0\u6cd5\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u4e2d\u5c06\u90e8\u4ef6\u7ec4\u7ec7\u6210\u4e0d\u540c\u7684\u5b9e\u4f8b\u3002\u7814\u7a76\u53d1\u73b0\u8fd9\u79cd\u5931\u8d25\u6e90\u4e8e\u6a21\u578b\u5185\u90e8\u5206\u914d\u673a\u5236\u7f3a\u4e4f\u7ed3\u6784\u7ea6\u675f\u3002", "method": "\u5c06\u7ed3\u6784\u53163D\u573a\u666f\u751f\u6210\u91cd\u65b0\u5b9a\u4e49\u4e3a\u5168\u5c40\u76f8\u5173\u5206\u914d\u95ee\u9898\uff0c\u5728\u7ec4\u5408DiT\u6a21\u578b\u7684\u53bb\u566a\u5faa\u73af\u4e2d\u5236\u5b9a\u5e76\u6c42\u89e3\u71b5\u6700\u4f18\u4f20\u8f93\u76ee\u6807\uff0c\u65bd\u52a0\u4e24\u4e2a\u7ed3\u6784\u7ea6\u675f\uff1a\u4f20\u8f93\u8ba1\u5212\u95e8\u63a7\u4ea4\u53c9\u6ce8\u610f\u529b\u5b9e\u73b0\u56fe\u50cf\u5757\u52303D\u6f5c\u5728\u8868\u793a\u7684\u4e00\u5bf9\u4e00\u72ec\u5360\u8def\u7531\uff1b\u57fa\u4e8e\u8fb9\u7f18\u7684\u6210\u672c\u6b63\u5219\u5316\u7ade\u4e89\u6027\u4f20\u8f93\u4ee5\u5f62\u6210\u8fde\u8d2f\u5bf9\u8c61\u3002", "result": "\u5728\u5f00\u653e\u4e16\u754c\u573a\u666f\u751f\u6210\u4efb\u52a1\u4e0a\u8d85\u8d8a\u73b0\u6709\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u5b9e\u4f8b\u7ea7\u8fde\u8d2f\u6027\u548c\u51e0\u4f55\u4fdd\u771f\u5ea6\u3002", "conclusion": "\u901a\u8fc7\u6700\u4f18\u4f20\u8f93\u5f15\u5165\u7ed3\u6784\u7ea6\u675f\uff0c\u6709\u6548\u89e3\u51b3\u4e863D\u573a\u666f\u751f\u6210\u4e2d\u7684\u5b9e\u4f8b\u5206\u5272\u95ee\u9898\uff0c\u4e3a\u7ed3\u6784\u5316\u573a\u666f\u751f\u6210\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\u3002"}}
{"id": "2602.23056", "categories": ["cs.AI", "eess.SY"], "pdf": "https://arxiv.org/pdf/2602.23056", "abs": "https://arxiv.org/abs/2602.23056", "authors": ["Giona Fieni", "Joschua W\u00fcthrich", "Marc-Philippe Neumann", "Christopher H. Onder"], "title": "Learning-based Multi-agent Race Strategies in Formula 1", "comment": null, "summary": "In Formula 1, race strategies are adapted according to evolving race conditions and competitors' actions. This paper proposes a reinforcement learning approach for multi-agent race strategy optimization. Agents learn to balance energy management, tire degradation, aerodynamic interaction, and pit-stop decisions. Building on a pre-trained single-agent policy, we introduce an interaction module that accounts for the behavior of competitors. The combination of the interaction module and a self-play training scheme generates competitive policies, and agents are ranked based on their relative performance. Results show that the agents adapt pit timing, tire selection, and energy allocation in response to opponents, achieving robust and consistent race performance. Because the framework relies only on information available during real races, it can support race strategists' decisions before and during races.", "AI": {"tldr": "\u63d0\u51fa\u57fa\u4e8e\u5f3a\u5316\u5b66\u4e60\u7684\u591a\u667a\u80fd\u4f53F1\u8d5b\u8f66\u7b56\u7565\u4f18\u5316\u6846\u67b6\uff0c\u901a\u8fc7\u4ea4\u4e92\u6a21\u5757\u548c\u81ea\u535a\u5f08\u8bad\u7ec3\u751f\u6210\u7ade\u4e89\u6027\u7b56\u7565\uff0c\u53ef\u652f\u6301\u5b9e\u9645\u6bd4\u8d5b\u51b3\u7b56", "motivation": "F1\u6bd4\u8d5b\u4e2d\u7b56\u7565\u9700\u8981\u6839\u636e\u5b9e\u65f6\u6bd4\u8d5b\u6761\u4ef6\u548c\u5bf9\u624b\u884c\u52a8\u52a8\u6001\u8c03\u6574\uff0c\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u590d\u6742\u7684\u591a\u667a\u80fd\u4f53\u4ea4\u4e92\u548c\u5b9e\u65f6\u51b3\u7b56", "method": "\u57fa\u4e8e\u9884\u8bad\u7ec3\u7684\u5355\u667a\u80fd\u4f53\u7b56\u7565\uff0c\u5f15\u5165\u4ea4\u4e92\u6a21\u5757\u8003\u8651\u5bf9\u624b\u884c\u4e3a\uff0c\u7ed3\u5408\u81ea\u535a\u5f08\u8bad\u7ec3\u65b9\u6848\u751f\u6210\u7ade\u4e89\u6027\u7b56\u7565\uff0c\u6839\u636e\u76f8\u5bf9\u6027\u80fd\u5bf9\u667a\u80fd\u4f53\u6392\u540d", "result": "\u667a\u80fd\u4f53\u80fd\u6839\u636e\u5bf9\u624b\u8c03\u6574\u8fdb\u7ad9\u65f6\u673a\u3001\u8f6e\u80ce\u9009\u62e9\u548c\u80fd\u91cf\u5206\u914d\uff0c\u5b9e\u73b0\u7a33\u5065\u4e00\u81f4\u7684\u6bd4\u8d5b\u8868\u73b0\uff0c\u6846\u67b6\u4ec5\u4f7f\u7528\u5b9e\u9645\u6bd4\u8d5b\u4e2d\u53ef\u7528\u4fe1\u606f", "conclusion": "\u8be5\u5f3a\u5316\u5b66\u4e60\u6846\u67b6\u80fd\u6709\u6548\u4f18\u5316\u591a\u667a\u80fd\u4f53F1\u6bd4\u8d5b\u7b56\u7565\uff0c\u652f\u6301\u8d5b\u524d\u548c\u8d5b\u4e2d\u7b56\u7565\u5e08\u51b3\u7b56\uff0c\u5177\u6709\u5b9e\u9645\u5e94\u7528\u4ef7\u503c"}}
{"id": "2602.22791", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22791", "abs": "https://arxiv.org/abs/2602.22791", "authors": ["Taishu Arashima", "Hiroshi Kera", "Kazuhiko Kawamoto"], "title": "Robust Human Trajectory Prediction via Self-Supervised Skeleton Representation Learning", "comment": "11 pages main, 5 pages supplementary material", "summary": "Human trajectory prediction plays a crucial role in applications such as autonomous navigation and video surveillance. While recent works have explored the integration of human skeleton sequences to complement trajectory information, skeleton data in real-world environments often suffer from missing joints caused by occlusions. These disturbances significantly degrade prediction accuracy, indicating the need for more robust skeleton representations. We propose a robust trajectory prediction method that incorporates a self-supervised skeleton representation model pretrained with masked autoencoding. Experimental results in occlusion-prone scenarios show that our method improves robustness to missing skeletal data without sacrificing prediction accuracy, and consistently outperforms baseline models in clean-to-moderate missingness regimes.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u7ed3\u5408\u81ea\u76d1\u7763\u9aa8\u67b6\u8868\u793a\u5b66\u4e60\u7684\u9c81\u68d2\u8f68\u8ff9\u9884\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u63a9\u7801\u81ea\u7f16\u7801\u9884\u8bad\u7ec3\u5904\u7406\u906e\u6321\u5bfc\u81f4\u7684\u5173\u8282\u7f3a\u5931\u95ee\u9898", "motivation": "\u73b0\u5b9e\u73af\u5883\u4e2d\u7684\u4eba\u4f53\u9aa8\u67b6\u6570\u636e\u5e38\u56e0\u906e\u6321\u5bfc\u81f4\u5173\u8282\u7f3a\u5931\uff0c\u8fd9\u4f1a\u663e\u8457\u964d\u4f4e\u8f68\u8ff9\u9884\u6d4b\u7cbe\u5ea6\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u9aa8\u67b6\u8868\u793a\u65b9\u6cd5", "method": "\u91c7\u7528\u81ea\u76d1\u7763\u9aa8\u67b6\u8868\u793a\u6a21\u578b\uff0c\u901a\u8fc7\u63a9\u7801\u81ea\u7f16\u7801\u9884\u8bad\u7ec3\uff0c\u7136\u540e\u6574\u5408\u5230\u8f68\u8ff9\u9884\u6d4b\u6846\u67b6\u4e2d", "result": "\u5728\u906e\u6321\u573a\u666f\u4e0b\uff0c\u8be5\u65b9\u6cd5\u63d0\u9ad8\u4e86\u5bf9\u7f3a\u5931\u9aa8\u67b6\u6570\u636e\u7684\u9c81\u68d2\u6027\uff0c\u540c\u65f6\u4e0d\u727a\u7272\u9884\u6d4b\u7cbe\u5ea6\uff0c\u5728\u6e05\u6d01\u5230\u4e2d\u7b49\u7f3a\u5931\u7a0b\u5ea6\u4e0b\u6301\u7eed\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b", "conclusion": "\u81ea\u76d1\u7763\u9aa8\u67b6\u8868\u793a\u5b66\u4e60\u80fd\u6709\u6548\u63d0\u5347\u8f68\u8ff9\u9884\u6d4b\u5728\u906e\u6321\u73af\u5883\u4e0b\u7684\u9c81\u68d2\u6027\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u63d0\u4f9b\u4e86\u66f4\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848"}}
{"id": "2602.23092", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23092", "abs": "https://arxiv.org/abs/2602.23092", "authors": ["Zhuoliang Xie", "Fei Liu", "Zhenkun Wang", "Qingfu Zhang"], "title": "Enhancing CVRP Solver through LLM-driven Automatic Heuristic Design", "comment": null, "summary": "The Capacitated Vehicle Routing Problem (CVRP), a fundamental combinatorial optimization challenge, focuses on optimizing fleet operations under vehicle capacity constraints. While extensively studied in operational research, the NP-hard nature of CVRP continues to pose significant computational challenges, particularly for large-scale instances. This study presents AILS-AHD (Adaptive Iterated Local Search with Automatic Heuristic Design), a novel approach that leverages Large Language Models (LLMs) to revolutionize CVRP solving. Our methodology integrates an evolutionary search framework with LLMs to dynamically generate and optimize ruin heuristics within the AILS method. Additionally, we introduce an LLM-based acceleration mechanism to enhance computational efficiency. Comprehensive experimental evaluations against state-of-the-art solvers, including AILS-II and HGS, demonstrate the superior performance of AILS-AHD across both moderate and large-scale instances. Notably, our approach establishes new best-known solutions for 8 out of 10 instances in the CVRPLib large-scale benchmark, underscoring the potential of LLM-driven heuristic design in advancing the field of vehicle routing optimization.", "AI": {"tldr": "\u63d0\u51faAILS-AHD\u65b9\u6cd5\uff0c\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u81ea\u9002\u5e94\u8fed\u4ee3\u5c40\u90e8\u641c\u7d22\uff0c\u81ea\u52a8\u8bbe\u8ba1\u542f\u53d1\u5f0f\u89c4\u5219\u89e3\u51b3\u5e26\u5bb9\u91cf\u7ea6\u675f\u7684\u8f66\u8f86\u8def\u5f84\u95ee\u9898\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u6700\u4f18\u6027\u80fd\u3002", "motivation": "\u5e26\u5bb9\u91cf\u7ea6\u675f\u7684\u8f66\u8f86\u8def\u5f84\u95ee\u9898\u662f\u7ec4\u5408\u4f18\u5316\u4e2d\u7684\u57fa\u672c\u95ee\u9898\uff0c\u5177\u6709NP\u96be\u7279\u6027\uff0c\u5728\u5927\u89c4\u6a21\u5b9e\u4f8b\u4e0a\u8ba1\u7b97\u6311\u6218\u5de8\u5927\u3002\u73b0\u6709\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u95ee\u9898\u65f6\u4ecd\u6709\u6539\u8fdb\u7a7a\u95f4\uff0c\u9700\u8981\u66f4\u667a\u80fd\u7684\u542f\u53d1\u5f0f\u8bbe\u8ba1\u65b9\u6cd5\u3002", "method": "\u63d0\u51faAILS-AHD\u65b9\u6cd5\uff0c\u5c06\u8fdb\u5316\u641c\u7d22\u6846\u67b6\u4e0e\u5927\u8bed\u8a00\u6a21\u578b\u7ed3\u5408\uff0c\u52a8\u6001\u751f\u6210\u548c\u4f18\u5316\u7834\u574f\u542f\u53d1\u5f0f\u89c4\u5219\u3002\u540c\u65f6\u5f15\u5165\u57fa\u4e8eLLM\u7684\u52a0\u901f\u673a\u5236\u63d0\u5347\u8ba1\u7b97\u6548\u7387\uff0c\u5728\u81ea\u9002\u5e94\u8fed\u4ee3\u5c40\u90e8\u641c\u7d22\u6846\u67b6\u4e2d\u5b9e\u73b0\u81ea\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u3002", "result": "\u5728CVRPLib\u5927\u89c4\u6a21\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cAILS-AHD\u572810\u4e2a\u5b9e\u4f8b\u4e2d\u76848\u4e2a\u4e0a\u5efa\u7acb\u4e86\u65b0\u7684\u6700\u4f18\u5df2\u77e5\u89e3\uff0c\u6027\u80fd\u4f18\u4e8eAILS-II\u548cHGS\u7b49\u5148\u8fdb\u6c42\u89e3\u5668\uff0c\u8bc1\u660e\u4e86LLM\u9a71\u52a8\u542f\u53d1\u5f0f\u8bbe\u8ba1\u7684\u6709\u6548\u6027\u3002", "conclusion": "LLM\u9a71\u52a8\u7684\u542f\u53d1\u5f0f\u8bbe\u8ba1\u5728\u8f66\u8f86\u8def\u5f84\u4f18\u5316\u9886\u57df\u5177\u6709\u5de8\u5927\u6f5c\u529b\uff0cAILS-AHD\u65b9\u6cd5\u901a\u8fc7\u7ed3\u5408\u5927\u8bed\u8a00\u6a21\u578b\u4e0e\u8fdb\u5316\u641c\u7d22\uff0c\u4e3a\u89e3\u51b3\u5927\u89c4\u6a21\u7ec4\u5408\u4f18\u5316\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u7684\u6709\u6548\u9014\u5f84\u3002"}}
{"id": "2602.22800", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22800", "abs": "https://arxiv.org/abs/2602.22800", "authors": ["Hanliang Du", "Zhangji Lu", "Zewei Cai", "Qijian Tang", "Qifeng Yu", "Xiaoli Liu"], "title": "GSTurb: Gaussian Splatting for Atmospheric Turbulence Mitigation", "comment": null, "summary": "Atmospheric turbulence causes significant image degradation due to pixel displacement (tilt) and blur, particularly in long-range imaging applications. In this paper, we propose a novel framework for atmospheric turbulence mitigation, GSTurb, which integrates optical flow-guided tilt correction and Gaussian splatting for modeling non-isoplanatic blur. The framework employs Gaussian parameters to represent tilt and blur, and optimizes them across multiple frames to enhance restoration. Experimental results on the ATSyn-static dataset demonstrate the effectiveness of our method, achieving a peak PSNR of 27.67 dB and SSIM of 0.8735. Compared to the state-of-the-art method, GSTurb improves PSNR by 1.3 dB (a 4.5% increase) and SSIM by 0.048 (a 5.8% increase). Additionally, on real datasets, including the TSRWGAN Real-World and CLEAR datasets, GSTurb outperforms existing methods, showing significant improvements in both qualitative and quantitative performance. These results highlight that combining optical flow-guided tilt correction with Gaussian splatting effectively enhances image restoration under both synthetic and real-world turbulence conditions. The code for this method will be available at https://github.com/DuhlLiamz/3DGS_turbulence/tree/main.", "AI": {"tldr": "\u63d0\u51faGSTurb\u6846\u67b6\uff0c\u7ed3\u5408\u5149\u6d41\u5f15\u5bfc\u7684\u503e\u659c\u6821\u6b63\u548c\u9ad8\u65af\u6cfc\u6e85\u5efa\u6a21\u975e\u7b49\u6655\u6a21\u7cca\uff0c\u7528\u4e8e\u5927\u6c14\u6e4d\u6d41\u56fe\u50cf\u6062\u590d\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u5747\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\u3002", "motivation": "\u5927\u6c14\u6e4d\u6d41\u5bfc\u81f4\u957f\u8ddd\u79bb\u6210\u50cf\u4e2d\u7684\u50cf\u7d20\u4f4d\u79fb\uff08\u503e\u659c\uff09\u548c\u6a21\u7cca\uff0c\u4e25\u91cd\u5f71\u54cd\u56fe\u50cf\u8d28\u91cf\uff0c\u9700\u8981\u6709\u6548\u7684\u6062\u590d\u65b9\u6cd5\u3002", "method": "GSTurb\u6846\u67b6\u6574\u5408\u5149\u6d41\u5f15\u5bfc\u7684\u503e\u659c\u6821\u6b63\u548c\u9ad8\u65af\u6cfc\u6e85\u5efa\u6a21\u975e\u7b49\u6655\u6a21\u7cca\uff0c\u4f7f\u7528\u9ad8\u65af\u53c2\u6570\u8868\u793a\u503e\u659c\u548c\u6a21\u7cca\uff0c\u5e76\u901a\u8fc7\u591a\u5e27\u4f18\u5316\u8fdb\u884c\u6062\u590d\u3002", "result": "\u5728ATSyn-static\u6570\u636e\u96c6\u4e0a\u8fbe\u5230PSNR 27.67 dB\u548cSSIM 0.8735\uff0c\u76f8\u6bd4SOTA\u65b9\u6cd5\u63d0\u53471.3 dB PSNR\uff084.5%\uff09\u548c0.048 SSIM\uff085.8%\uff09\uff0c\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u4e5f\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u5149\u6d41\u5f15\u5bfc\u7684\u503e\u659c\u6821\u6b63\u4e0e\u9ad8\u65af\u6cfc\u6e85\u7ed3\u5408\u80fd\u6709\u6548\u63d0\u5347\u5927\u6c14\u6e4d\u6d41\u6761\u4ef6\u4e0b\u7684\u56fe\u50cf\u6062\u590d\u6548\u679c\uff0c\u5728\u5408\u6210\u548c\u771f\u5b9e\u573a\u666f\u4e2d\u5747\u8868\u73b0\u51fa\u8272\u3002"}}
{"id": "2602.23093", "categories": ["cs.AI", "cs.SI", "physics.soc-ph"], "pdf": "https://arxiv.org/pdf/2602.23093", "abs": "https://arxiv.org/abs/2602.23093", "authors": ["Dhwanil M. Mori", "Neil F. Johnson"], "title": "Three AI-agents walk into a bar . . . . `Lord of the Flies' tribalism emerges among smart AI-Agents", "comment": null, "summary": "Near-future infrastructure systems may be controlled by autonomous AI agents that repeatedly request access to limited resources such as energy, bandwidth, or computing power. We study a simplified version of this setting using a framework where N AI-agents independently decide at each round whether to request one unit from a system with fixed capacity C. An AI version of \"Lord of the Flies\" arises in which controlling tribes emerge with their own collective character and identity. The LLM agents do not reduce overload or improve resource use, and often perform worse than if they were flipping coins to make decisions. Three main tribal types emerge: Aggressive (27.3%), Conservative (24.7%), and Opportunistic (48.1%). The more capable AI-agents actually increase the rate of systemic failure. Overall, our findings show that smarter AI-agents can behave dumber as a result of forming tribes.", "AI": {"tldr": "AI\u4ee3\u7406\u5728\u8d44\u6e90\u5206\u914d\u7cfb\u7edf\u4e2d\u4f1a\u81ea\u53d1\u5f62\u6210\u90e8\u843d\uff0c\u4f46\u90e8\u843d\u5316\u53cd\u800c\u5bfc\u81f4\u51b3\u7b56\u66f4\u5dee\uff0c\u751a\u81f3\u4e0d\u5982\u968f\u673a\u51b3\u7b56\uff0c\u80fd\u529b\u66f4\u5f3a\u7684AI\u53cd\u800c\u589e\u52a0\u7cfb\u7edf\u6545\u969c\u7387", "motivation": "\u7814\u7a76\u672a\u6765\u57fa\u7840\u8bbe\u65bd\u7cfb\u7edf\u4e2d\u81ea\u4e3bAI\u4ee3\u7406\u5982\u4f55\u7ade\u4e89\u6709\u9650\u8d44\u6e90\uff0c\u63a2\u7d22AI\u4ee3\u7406\u5728\u91cd\u590d\u51b3\u7b56\u4e2d\u662f\u5426\u4f1a\u5f62\u6210\u7c7b\u4f3c\"\u8747\u738b\"\u7684\u793e\u4f1a\u7ed3\u6784\uff0c\u4ee5\u53ca\u8fd9\u79cd\u90e8\u843d\u5316\u5bf9\u7cfb\u7edf\u6027\u80fd\u7684\u5f71\u54cd", "method": "\u5efa\u7acb\u7b80\u5316\u6846\u67b6\uff1aN\u4e2aAI\u4ee3\u7406\u5728\u6bcf\u8f6e\u72ec\u7acb\u51b3\u5b9a\u662f\u5426\u8bf7\u6c421\u5355\u4f4d\u8d44\u6e90\uff0c\u7cfb\u7edf\u6709\u56fa\u5b9a\u5bb9\u91cfC\u3002\u89c2\u5bdfAI\u4ee3\u7406\u5728\u91cd\u590d\u4ea4\u4e92\u4e2d\u5982\u4f55\u5f62\u6210\u90e8\u843d\u548c\u96c6\u4f53\u7279\u5f81", "result": "AI\u4ee3\u7406\u5f62\u6210\u4e09\u79cd\u4e3b\u8981\u90e8\u843d\u7c7b\u578b\uff1a\u6fc0\u8fdb\u578b(27.3%)\u3001\u4fdd\u5b88\u578b(24.7%)\u3001\u673a\u4f1a\u4e3b\u4e49\u578b(48.1%)\u3002\u90e8\u843d\u5316\u5e76\u672a\u51cf\u5c11\u8fc7\u8f7d\u6216\u6539\u5584\u8d44\u6e90\u5229\u7528\uff0c\u51b3\u7b56\u8d28\u91cf\u751a\u81f3\u4e0d\u5982\u629b\u786c\u5e01\u3002\u80fd\u529b\u66f4\u5f3a\u7684AI\u4ee3\u7406\u53cd\u800c\u589e\u52a0\u7cfb\u7edf\u6545\u969c\u7387", "conclusion": "\u66f4\u806a\u660e\u7684AI\u4ee3\u7406\u7531\u4e8e\u5f62\u6210\u90e8\u843d\u800c\u8868\u73b0\u51fa\u66f4\u611a\u8822\u7684\u96c6\u4f53\u884c\u4e3a\uff0c\u90e8\u843d\u5316\u5bfc\u81f4\u51b3\u7b56\u8d28\u91cf\u4e0b\u964d\uff0c\u8fd9\u5bf9\u672a\u6765AI\u63a7\u5236\u7684\u57fa\u7840\u8bbe\u65bd\u7cfb\u7edf\u8bbe\u8ba1\u63d0\u51fa\u4e86\u91cd\u8981\u8b66\u793a"}}
{"id": "2602.22809", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22809", "abs": "https://arxiv.org/abs/2602.22809", "authors": ["Mingde Yao", "Zhiyuan You", "Tam-King Man", "Menglu Wang", "Tianfan Xue"], "title": "PhotoAgent: Agentic Photo Editing with Exploratory Visual Aesthetic Planning", "comment": "A fully automated, intelligent photo-editing agent that autonomously plans multi-step aesthetic enhancements, smartly chooses diverse editing tools, and enables everyday users to achieve professional-looking results without crafting complex prompts. Project page: https://github.com/mdyao/PhotoAgent", "summary": "With the recent fast development of generative models, instruction-based image editing has shown great potential in generating high-quality images. However, the quality of editing highly depends on carefully designed instructions, placing the burden of task decomposition and sequencing entirely on the user. To achieve autonomous image editing, we present PhotoAgent, a system that advances image editing through explicit aesthetic planning. Specifically, PhotoAgent formulates autonomous image editing as a long-horizon decision-making problem. It reasons over user aesthetic intent, plans multi-step editing actions via tree search, and iteratively refines results through closed-loop execution with memory and visual feedback, without requiring step-by-step user prompts. To support reliable evaluation in real-world scenarios, we introduce UGC-Edit, an aesthetic evaluation benchmark consisting of 7,000 photos and a learned aesthetic reward model. We also construct a test set containing 1,017 photos to systematically assess autonomous photo editing performance. Extensive experiments demonstrate that PhotoAgent consistently improves both instruction adherence and visual quality compared with baseline methods. The project page is https://github.com/mdyao/PhotoAgent.", "AI": {"tldr": "PhotoAgent\u662f\u4e00\u4e2a\u901a\u8fc7\u663e\u5f0f\u7f8e\u5b66\u89c4\u5212\u5b9e\u73b0\u81ea\u4e3b\u56fe\u50cf\u7f16\u8f91\u7684\u7cfb\u7edf\uff0c\u5c06\u7f16\u8f91\u4efb\u52a1\u5efa\u6a21\u4e3a\u957f\u65f6\u7a0b\u51b3\u7b56\u95ee\u9898\uff0c\u901a\u8fc7\u6811\u641c\u7d22\u89c4\u5212\u591a\u6b65\u7f16\u8f91\u52a8\u4f5c\uff0c\u65e0\u9700\u7528\u6237\u9010\u6b65\u63d0\u793a\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8e\u6307\u4ee4\u7684\u56fe\u50cf\u7f16\u8f91\u9ad8\u5ea6\u4f9d\u8d56\u7cbe\u5fc3\u8bbe\u8ba1\u7684\u6307\u4ee4\uff0c\u5c06\u4efb\u52a1\u5206\u89e3\u548c\u987a\u5e8f\u89c4\u5212\u7684\u8d1f\u62c5\u5b8c\u5168\u653e\u5728\u7528\u6237\u8eab\u4e0a\u3002\u4e3a\u4e86\u5b9e\u73b0\u81ea\u4e3b\u56fe\u50cf\u7f16\u8f91\uff0c\u9700\u8981\u7cfb\u7edf\u80fd\u591f\u7406\u89e3\u7528\u6237\u7f8e\u5b66\u610f\u56fe\u5e76\u81ea\u52a8\u89c4\u5212\u7f16\u8f91\u6b65\u9aa4\u3002", "method": "PhotoAgent\u5c06\u81ea\u4e3b\u56fe\u50cf\u7f16\u8f91\u5efa\u6a21\u4e3a\u957f\u65f6\u7a0b\u51b3\u7b56\u95ee\u9898\uff0c\u901a\u8fc7\u6811\u641c\u7d22\u89c4\u5212\u591a\u6b65\u7f16\u8f91\u52a8\u4f5c\uff0c\u5229\u7528\u8bb0\u5fc6\u548c\u89c6\u89c9\u53cd\u9988\u8fdb\u884c\u95ed\u73af\u6267\u884c\u8fed\u4ee3\u4f18\u5316\u3002\u7cfb\u7edf\u5305\u542b\u7406\u89e3\u7528\u6237\u7f8e\u5b66\u610f\u56fe\u3001\u89c4\u5212\u7f16\u8f91\u6b65\u9aa4\u548c\u6267\u884c\u7f16\u8f91\u4e09\u4e2a\u6838\u5fc3\u6a21\u5757\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cPhotoAgent\u5728\u6307\u4ee4\u9075\u5faa\u548c\u89c6\u89c9\u8d28\u91cf\u65b9\u9762\u5747\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002\u4f5c\u8005\u8fd8\u6784\u5efa\u4e86UGC-Edit\u7f8e\u5b66\u8bc4\u4f30\u57fa\u51c6\uff08\u5305\u542b7,000\u5f20\u7167\u7247\uff09\u548c\u5305\u542b1,017\u5f20\u7167\u7247\u7684\u6d4b\u8bd5\u96c6\u6765\u7cfb\u7edf\u8bc4\u4f30\u81ea\u4e3b\u7167\u7247\u7f16\u8f91\u6027\u80fd\u3002", "conclusion": "PhotoAgent\u901a\u8fc7\u663e\u5f0f\u7f8e\u5b66\u89c4\u5212\u548c\u95ed\u73af\u6267\u884c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u81ea\u4e3b\u56fe\u50cf\u7f16\u8f91\uff0c\u51cf\u8f7b\u4e86\u7528\u6237\u624b\u52a8\u89c4\u5212\u7f16\u8f91\u6b65\u9aa4\u7684\u8d1f\u62c5\uff0c\u5728\u771f\u5b9e\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u4f18\u8d8a\u7684\u6027\u80fd\u3002"}}
{"id": "2602.23123", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23123", "abs": "https://arxiv.org/abs/2602.23123", "authors": ["Keito Inoshita"], "title": "Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection", "comment": null, "summary": "In the attention economy, sensational content exposes consumers to excessive emotional stimulation, hindering calm decision-making. This study proposes Multi-Agent LLM-based Emotional deToxification (MALLET), a multi-agent information sanitization system consisting of four agents: Emotion Analysis, Emotion Adjustment, Balance Monitoring, and Personal Guide. The Emotion Analysis Agent quantifies stimulus intensity using a 6-emotion BERT classifier, and the Emotion Adjustment Agent rewrites texts into two presentation modes, BALANCED (neutralized text) and COOL (neutralized text + supplementary text), using an LLM. The Balance Monitoring Agent aggregates weekly information consumption patterns and generates personalized advice, while the Personal Guide Agent recommends a presentation mode according to consumer sensitivity. Experiments on 800 AG News articles demonstrated significant stimulus score reduction (up to 19.3%) and improved emotion balance while maintaining semantic preservation. Near-zero correlation between stimulus reduction and semantic preservation confirmed that the two are independently controllable. Category-level analysis revealed substantial reduction (17.8-33.8%) in Sports, Business, and Sci/Tech, whereas the effect was limited in the World category, where facts themselves are inherently high-stimulus. The proposed system provides a framework for supporting calm information reception of consumers without restricting access to the original text.", "AI": {"tldr": "MALLET\u662f\u4e00\u4e2a\u57fa\u4e8e\u591a\u667a\u80fd\u4f53LLM\u7684\u60c5\u611f\u51c0\u5316\u7cfb\u7edf\uff0c\u901a\u8fc7\u56db\u4e2a\u667a\u80fd\u4f53\u5206\u6790\u3001\u8c03\u6574\u3001\u76d1\u63a7\u548c\u6307\u5bfc\u4fe1\u606f\u6d88\u8d39\uff0c\u663e\u8457\u964d\u4f4e\u65b0\u95fb\u6587\u7ae0\u7684\u60c5\u611f\u523a\u6fc0\u5f3a\u5ea6\uff08\u6700\u9ad819.3%\uff09\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u5b8c\u6574\u6027\u3002", "motivation": "\u5728\u6ce8\u610f\u529b\u7ecf\u6d4e\u4e2d\uff0c\u717d\u60c5\u5185\u5bb9\u8ba9\u6d88\u8d39\u8005\u66b4\u9732\u5728\u8fc7\u5ea6\u60c5\u611f\u523a\u6fc0\u4e0b\uff0c\u963b\u788d\u51b7\u9759\u51b3\u7b56\u3002\u9700\u8981\u4e00\u79cd\u65e2\u80fd\u51cf\u5c11\u60c5\u611f\u523a\u6fc0\u53c8\u4e0d\u9650\u5236\u539f\u59cb\u6587\u672c\u8bbf\u95ee\u7684\u7cfb\u7edf\u3002", "method": "\u63d0\u51faMALLET\u7cfb\u7edf\uff0c\u5305\u542b\u56db\u4e2a\u667a\u80fd\u4f53\uff1a\u60c5\u611f\u5206\u6790\u667a\u80fd\u4f53\uff08\u4f7f\u75286\u60c5\u611fBERT\u5206\u7c7b\u5668\u91cf\u5316\u523a\u6fc0\u5f3a\u5ea6\uff09\u3001\u60c5\u611f\u8c03\u6574\u667a\u80fd\u4f53\uff08\u7528LLM\u5c06\u6587\u672c\u91cd\u5199\u4e3aBALANCED\u548cCOOL\u4e24\u79cd\u6a21\u5f0f\uff09\u3001\u5e73\u8861\u76d1\u63a7\u667a\u80fd\u4f53\uff08\u805a\u5408\u6bcf\u5468\u4fe1\u606f\u6d88\u8d39\u6a21\u5f0f\u751f\u6210\u4e2a\u6027\u5316\u5efa\u8bae\uff09\u3001\u4e2a\u4eba\u6307\u5bfc\u667a\u80fd\u4f53\uff08\u6839\u636e\u6d88\u8d39\u8005\u654f\u611f\u5ea6\u63a8\u8350\u5448\u73b0\u6a21\u5f0f\uff09\u3002", "result": "\u5728800\u7bc7AG News\u6587\u7ae0\u4e0a\u7684\u5b9e\u9a8c\u663e\u793a\uff1a\u523a\u6fc0\u5206\u6570\u663e\u8457\u964d\u4f4e\uff08\u6700\u9ad819.3%\uff09\uff0c\u60c5\u611f\u5e73\u8861\u6539\u5584\uff0c\u540c\u65f6\u4fdd\u6301\u8bed\u4e49\u4fdd\u5b58\uff1b\u523a\u6fc0\u51cf\u5c11\u4e0e\u8bed\u4e49\u4fdd\u5b58\u4e4b\u95f4\u63a5\u8fd1\u96f6\u76f8\u5173\uff0c\u8868\u660e\u4e24\u8005\u53ef\u72ec\u7acb\u63a7\u5236\uff1b\u7c7b\u522b\u5206\u6790\u663e\u793a\u4f53\u80b2\u3001\u5546\u4e1a\u3001\u79d1\u6280\u7c7b\u523a\u6fc0\u5927\u5e45\u51cf\u5c11\uff0817.8-33.8%\uff09\uff0c\u4e16\u754c\u7c7b\u6548\u679c\u6709\u9650\uff08\u4e8b\u5b9e\u672c\u8eab\u5177\u6709\u9ad8\u523a\u6fc0\u6027\uff09\u3002", "conclusion": "MALLET\u7cfb\u7edf\u4e3a\u652f\u6301\u6d88\u8d39\u8005\u51b7\u9759\u63a5\u6536\u4fe1\u606f\u63d0\u4f9b\u4e86\u6846\u67b6\uff0c\u65e0\u9700\u9650\u5236\u5bf9\u539f\u59cb\u6587\u672c\u7684\u8bbf\u95ee\uff0c\u5b9e\u73b0\u4e86\u60c5\u611f\u523a\u6fc0\u51cf\u5c11\u4e0e\u8bed\u4e49\u4fdd\u5b58\u7684\u72ec\u7acb\u63a7\u5236\u3002"}}
{"id": "2602.22819", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22819", "abs": "https://arxiv.org/abs/2602.22819", "authors": ["Purbayan Kar", "Ayush Ghadiya", "Vishal Chudasama", "Pankaj Wasnik", "C. V. Jawahar"], "title": "Face Time Traveller : Travel Through Ages Without Losing Identity", "comment": "Accepted at CVPR 2026 (Findings Track)", "summary": "Face aging, an ill-posed problem shaped by environmental and genetic factors, is vital in entertainment, forensics, and digital archiving, where realistic age transformations must preserve both identity and visual realism. However, existing works relying on numerical age representations overlook the interplay of biological and contextual cues. Despite progress in recent face aging models, they struggle with identity preservation in wide age transformations, also static attention and optimization-heavy inversion in diffusion limit adaptability, fine-grained control and background consistency. To address these challenges, we propose Face Time Traveller (FaceTT), a diffusion-based framework that achieves high-fidelity, identity-consistent age transformation. Here, we introduce a Face-Attribute-Aware Prompt Refinement strategy that encodes intrinsic (biological) and extrinsic (environmental) aging cues for context-aware conditioning. A tuning-free Angular Inversion method is proposed that efficiently maps real faces into the diffusion latent space for fast and accurate reconstruction. Moreover, an Adaptive Attention Control mechanism is introduced that dynamically balances cross-attention for semantic aging cues and self-attention for structural and identity preservation. Extensive experiments on benchmark datasets and in-the-wild testset demonstrate that FaceTT achieves superior identity retention, background preservation and aging realism over state-of-the-art (SOTA) methods.", "AI": {"tldr": "FaceTT\u662f\u4e00\u4e2a\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u9762\u90e8\u5c5e\u6027\u611f\u77e5\u63d0\u793a\u7ec6\u5316\u3001\u514d\u8c03\u8c10\u89d2\u5ea6\u53cd\u6f14\u548c\u81ea\u9002\u5e94\u6ce8\u610f\u529b\u63a7\u5236\uff0c\u5b9e\u73b0\u9ad8\u4fdd\u771f\u3001\u8eab\u4efd\u4e00\u81f4\u7684\u9762\u90e8\u5e74\u9f84\u53d8\u6362\u3002", "motivation": "\u73b0\u6709\u9762\u90e8\u5e74\u9f84\u53d8\u6362\u65b9\u6cd5\u4f9d\u8d56\u6570\u503c\u5e74\u9f84\u8868\u793a\uff0c\u5ffd\u7565\u4e86\u751f\u7269\u548c\u73af\u5883\u56e0\u7d20\u7684\u76f8\u4e92\u4f5c\u7528\uff0c\u5728\u5bbd\u5e74\u9f84\u53d8\u6362\u4e2d\u96be\u4ee5\u4fdd\u6301\u8eab\u4efd\u4e00\u81f4\u6027\uff0c\u4e14\u5b58\u5728\u9759\u6001\u6ce8\u610f\u529b\u3001\u4f18\u5316\u7e41\u91cd\u7684\u53cd\u6f14\u7b49\u95ee\u9898\uff0c\u9650\u5236\u4e86\u9002\u5e94\u6027\u3001\u7ec6\u7c92\u5ea6\u63a7\u5236\u548c\u80cc\u666f\u4e00\u81f4\u6027\u3002", "method": "1) \u9762\u90e8\u5c5e\u6027\u611f\u77e5\u63d0\u793a\u7ec6\u5316\u7b56\u7565\uff1a\u7f16\u7801\u5185\u5728\uff08\u751f\u7269\uff09\u548c\u5916\u5728\uff08\u73af\u5883\uff09\u8001\u5316\u7ebf\u7d22\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u6761\u4ef6\u5316\uff1b2) \u514d\u8c03\u8c10\u89d2\u5ea6\u53cd\u6f14\u65b9\u6cd5\uff1a\u9ad8\u6548\u5c06\u771f\u5b9e\u9762\u90e8\u6620\u5c04\u5230\u6269\u6563\u6f5c\u5728\u7a7a\u95f4\u4ee5\u5b9e\u73b0\u5feb\u901f\u51c6\u786e\u91cd\u5efa\uff1b3) \u81ea\u9002\u5e94\u6ce8\u610f\u529b\u63a7\u5236\u673a\u5236\uff1a\u52a8\u6001\u5e73\u8861\u4ea4\u53c9\u6ce8\u610f\u529b\uff08\u8bed\u4e49\u8001\u5316\u7ebf\u7d22\uff09\u548c\u81ea\u6ce8\u610f\u529b\uff08\u7ed3\u6784\u548c\u8eab\u4efd\u4fdd\u6301\uff09\u3002", "result": "\u5728\u57fa\u51c6\u6570\u636e\u96c6\u548c\u91ce\u5916\u6d4b\u8bd5\u96c6\u4e0a\u7684\u5e7f\u6cdb\u5b9e\u9a8c\u8868\u660e\uff0cFaceTT\u5728\u8eab\u4efd\u4fdd\u6301\u3001\u80cc\u666f\u4fdd\u7559\u548c\u8001\u5316\u771f\u5b9e\u6027\u65b9\u9762\u4f18\u4e8e\u6700\u5148\u8fdb\u65b9\u6cd5\u3002", "conclusion": "FaceTT\u901a\u8fc7\u521b\u65b0\u7684\u63d0\u793a\u7ec6\u5316\u3001\u53cd\u6f14\u548c\u6ce8\u610f\u529b\u63a7\u5236\u673a\u5236\uff0c\u6210\u529f\u89e3\u51b3\u4e86\u9762\u90e8\u5e74\u9f84\u53d8\u6362\u4e2d\u7684\u8eab\u4efd\u4fdd\u6301\u3001\u80cc\u666f\u4e00\u81f4\u6027\u548c\u8001\u5316\u771f\u5b9e\u6027\u95ee\u9898\uff0c\u5b9e\u73b0\u4e86\u9ad8\u8d28\u91cf\u7684\u9762\u90e8\u5e74\u9f84\u53d8\u6362\u3002"}}
{"id": "2602.23148", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23148", "abs": "https://arxiv.org/abs/2602.23148", "authors": ["Nitin Gupta", "Vishal Pallagani", "John A. Aydin", "Biplav Srivastava"], "title": "On Sample-Efficient Generalized Planning via Learned Transition Models", "comment": "14 pages; This is an extended version of a short paper accepted at ICAPS 2026 under the same title", "summary": "Generalized planning studies the construction of solution strategies that generalize across families of planning problems sharing a common domain model, formally defined by a transition function $\u03b3: S \\times A \\rightarrow S$. Classical approaches achieve such generalization through symbolic abstractions and explicit reasoning over $\u03b3$. In contrast, recent Transformer-based planners, such as PlanGPT and Plansformer, largely cast generalized planning as direct action-sequence prediction, bypassing explicit transition modeling. While effective on in-distribution instances, these approaches typically require large datasets and model sizes, and often suffer from state drift in long-horizon settings due to the absence of explicit world-state evolution. In this work, we formulate generalized planning as a transition-model learning problem, in which a neural model explicitly approximates the successor-state function $\\hat\u03b3 \\approx \u03b3$ and generates plans by rolling out symbolic state trajectories. Instead of predicting actions directly, the model autoregressively predicts intermediate world states, thereby learning the domain dynamics as an implicit world model. To study size-invariant generalization and sample efficiency, we systematically evaluate multiple state representations and neural architectures, including relational graph encodings. Our results show that learning explicit transition models yields higher out-of-distribution satisficing-plan success than direct action-sequence prediction in multiple domains, while achieving these gains with significantly fewer training instances and smaller models. This is an extended version of a short paper accepted at ICAPS 2026 under the same title.", "AI": {"tldr": "\u8be5\u8bba\u6587\u63d0\u51fa\u5c06\u5e7f\u4e49\u89c4\u5212\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8f6c\u79fb\u6a21\u578b\u5b66\u4e60\u95ee\u9898\uff0c\u901a\u8fc7\u795e\u7ecf\u7f51\u7edc\u663e\u5f0f\u8fd1\u4f3c\u72b6\u6001\u8f6c\u79fb\u51fd\u6570\uff0c\u800c\u4e0d\u662f\u76f4\u63a5\u9884\u6d4b\u52a8\u4f5c\u5e8f\u5217\uff0c\u4ece\u800c\u63d0\u9ad8\u5206\u5e03\u5916\u6cdb\u5316\u80fd\u529b\u548c\u6837\u672c\u6548\u7387\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eTransformer\u7684\u89c4\u5212\u5668\uff08\u5982PlanGPT\u3001Plansformer\uff09\u5c06\u5e7f\u4e49\u89c4\u5212\u89c6\u4e3a\u76f4\u63a5\u7684\u52a8\u4f5c\u5e8f\u5217\u9884\u6d4b\u95ee\u9898\uff0c\u867d\u7136\u5bf9\u5206\u5e03\u5185\u5b9e\u4f8b\u6709\u6548\uff0c\u4f46\u9700\u8981\u5927\u91cf\u6570\u636e\u548c\u6a21\u578b\u89c4\u6a21\uff0c\u4e14\u5728\u957f\u65f6\u57df\u89c4\u5212\u4e2d\u5bb9\u6613\u56e0\u7f3a\u4e4f\u663e\u5f0f\u4e16\u754c\u72b6\u6001\u6f14\u5316\u800c\u4ea7\u751f\u72b6\u6001\u6f02\u79fb\u95ee\u9898\u3002", "method": "\u5c06\u5e7f\u4e49\u89c4\u5212\u91cd\u65b0\u5b9a\u4e49\u4e3a\u8f6c\u79fb\u6a21\u578b\u5b66\u4e60\u95ee\u9898\uff0c\u8bad\u7ec3\u795e\u7ecf\u7f51\u7edc\u6a21\u578b\u663e\u5f0f\u8fd1\u4f3c\u540e\u7ee7\u72b6\u6001\u51fd\u6570\uff0c\u901a\u8fc7\u81ea\u56de\u5f52\u9884\u6d4b\u4e2d\u95f4\u4e16\u754c\u72b6\u6001\u6765\u5b66\u4e60\u9886\u57df\u52a8\u6001\u4f5c\u4e3a\u9690\u5f0f\u4e16\u754c\u6a21\u578b\u3002\u7cfb\u7edf\u8bc4\u4f30\u4e86\u591a\u79cd\u72b6\u6001\u8868\u793a\u548c\u795e\u7ecf\u67b6\u6784\uff0c\u5305\u62ec\u5173\u7cfb\u56fe\u7f16\u7801\u3002", "result": "\u5b66\u4e60\u663e\u5f0f\u8f6c\u79fb\u6a21\u578b\u5728\u591a\u4e2a\u9886\u57df\u4e2d\u6bd4\u76f4\u63a5\u52a8\u4f5c\u5e8f\u5217\u9884\u6d4b\u83b7\u5f97\u66f4\u9ad8\u7684\u5206\u5e03\u5916\u6ee1\u610f\u89c4\u5212\u6210\u529f\u7387\uff0c\u540c\u65f6\u4f7f\u7528\u66f4\u5c11\u7684\u8bad\u7ec3\u5b9e\u4f8b\u548c\u66f4\u5c0f\u7684\u6a21\u578b\u5c31\u80fd\u5b9e\u73b0\u8fd9\u4e9b\u6536\u76ca\u3002", "conclusion": "\u663e\u5f0f\u5b66\u4e60\u8f6c\u79fb\u6a21\u578b\u7684\u65b9\u6cd5\u5728\u5e7f\u4e49\u89c4\u5212\u4e2d\u5177\u6709\u4f18\u52bf\uff0c\u80fd\u591f\u63d0\u9ad8\u6cdb\u5316\u80fd\u529b\u3001\u6837\u672c\u6548\u7387\u548c\u89c4\u5212\u8d28\u91cf\uff0c\u7279\u522b\u662f\u5728\u5206\u5e03\u5916\u548c\u957f\u65f6\u57df\u573a\u666f\u4e0b\u3002"}}
{"id": "2602.22821", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22821", "abs": "https://arxiv.org/abs/2602.22821", "authors": ["Tong Wang", "Yaolei Qi", "Siwen Wang", "Imran Razzak", "Guanyu Yang", "Yutong Xie"], "title": "CMSA-Net: Causal Multi-scale Aggregation with Adaptive Multi-source Reference for Video Polyp Segmentation", "comment": null, "summary": "Video polyp segmentation (VPS) is an important task in computer-aided colonoscopy, as it helps doctors accurately locate and track polyps during examinations. However, VPS remains challenging because polyps often look similar to surrounding mucosa, leading to weak semantic discrimination. In addition, large changes in polyp position and scale across video frames make stable and accurate segmentation difficult. To address these challenges, we propose a robust VPS framework named CMSA-Net. The proposed network introduces a Causal Multi-scale Aggregation (CMA) module to effectively gather semantic information from multiple historical frames at different scales. By using causal attention, CMA ensures that temporal feature propagation follows strict time order, which helps reduce noise and improve feature reliability. Furthermore, we design a Dynamic Multi-source Reference (DMR) strategy that adaptively selects informative and reliable reference frames based on semantic separability and prediction confidence. This strategy provides strong multi-frame guidance while keeping the model efficient for real-time inference. Extensive experiments on the SUN-SEG dataset demonstrate that CMSA-Net achieves state-of-the-art performance, offering a favorable balance between segmentation accuracy and real-time clinical applicability.", "AI": {"tldr": "CMSA-Net\u662f\u4e00\u4e2a\u7528\u4e8e\u89c6\u9891\u606f\u8089\u5206\u5272\u7684\u6846\u67b6\uff0c\u901a\u8fc7\u56e0\u679c\u591a\u5c3a\u5ea6\u805a\u5408\u6a21\u5757\u548c\u52a8\u6001\u591a\u6e90\u53c2\u8003\u7b56\u7565\uff0c\u5728\u4fdd\u6301\u5b9e\u65f6\u6027\u7684\u540c\u65f6\u63d0\u9ad8\u4e86\u5206\u5272\u51c6\u786e\u6027\u3002", "motivation": "\u89c6\u9891\u606f\u8089\u5206\u5272\u5728\u8ba1\u7b97\u673a\u8f85\u52a9\u7ed3\u80a0\u955c\u68c0\u67e5\u4e2d\u5f88\u91cd\u8981\uff0c\u4f46\u9762\u4e34\u4e24\u4e2a\u4e3b\u8981\u6311\u6218\uff1a1\uff09\u606f\u8089\u4e0e\u5468\u56f4\u9ecf\u819c\u5916\u89c2\u76f8\u4f3c\uff0c\u8bed\u4e49\u533a\u5206\u5ea6\u5f31\uff1b2\uff09\u89c6\u9891\u5e27\u95f4\u606f\u8089\u4f4d\u7f6e\u548c\u5c3a\u5ea6\u53d8\u5316\u5927\uff0c\u5bfc\u81f4\u5206\u5272\u4e0d\u7a33\u5b9a\u548c\u4e0d\u51c6\u786e\u3002", "method": "\u63d0\u51fa\u4e86CMSA-Net\u6846\u67b6\uff0c\u5305\u542b\u4e24\u4e2a\u6838\u5fc3\u7ec4\u4ef6\uff1a1\uff09\u56e0\u679c\u591a\u5c3a\u5ea6\u805a\u5408\u6a21\u5757\uff0c\u4f7f\u7528\u56e0\u679c\u6ce8\u610f\u529b\u4ece\u591a\u4e2a\u5386\u53f2\u5e27\u7684\u4e0d\u540c\u5c3a\u5ea6\u6709\u6548\u6536\u96c6\u8bed\u4e49\u4fe1\u606f\uff0c\u786e\u4fdd\u65f6\u95f4\u7279\u5f81\u4f20\u64ad\u9075\u5faa\u4e25\u683c\u65f6\u95f4\u987a\u5e8f\uff1b2\uff09\u52a8\u6001\u591a\u6e90\u53c2\u8003\u7b56\u7565\uff0c\u57fa\u4e8e\u8bed\u4e49\u53ef\u5206\u6027\u548c\u9884\u6d4b\u7f6e\u4fe1\u5ea6\u81ea\u9002\u5e94\u9009\u62e9\u4fe1\u606f\u4e30\u5bcc\u4e14\u53ef\u9760\u7684\u53c2\u8003\u5e27\uff0c\u63d0\u4f9b\u591a\u5e27\u6307\u5bfc\u540c\u65f6\u4fdd\u6301\u5b9e\u65f6\u63a8\u7406\u6548\u7387\u3002", "result": "\u5728SUN-SEG\u6570\u636e\u96c6\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cCMSA-Net\u5b9e\u73b0\u4e86\u6700\u5148\u8fdb\u7684\u6027\u80fd\uff0c\u5728\u5206\u5272\u51c6\u786e\u6027\u548c\u5b9e\u65f6\u4e34\u5e8a\u9002\u7528\u6027\u4e4b\u95f4\u53d6\u5f97\u4e86\u826f\u597d\u5e73\u8861\u3002", "conclusion": "CMSA-Net\u901a\u8fc7\u56e0\u679c\u591a\u5c3a\u5ea6\u7279\u5f81\u805a\u5408\u548c\u81ea\u9002\u5e94\u53c2\u8003\u5e27\u9009\u62e9\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u89c6\u9891\u606f\u8089\u5206\u5272\u4e2d\u7684\u8bed\u4e49\u533a\u5206\u5f31\u548c\u65f6\u95f4\u7a33\u5b9a\u6027\u95ee\u9898\uff0c\u4e3a\u4e34\u5e8a\u5b9e\u65f6\u5e94\u7528\u63d0\u4f9b\u4e86\u5b9e\u7528\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.22829", "categories": ["cs.CV", "eess.SP"], "pdf": "https://arxiv.org/pdf/2602.22829", "abs": "https://arxiv.org/abs/2602.22829", "authors": ["G. A. S. L Ranasinghe", "J. A. S. T. Jayakody", "M. C. L. De Silva", "G. Thilakarathne", "G. M. R. I. Godaliyadda", "H. M. V. R. Herath", "M. P. B. Ekanayake", "S. K. Navaratnarajah"], "title": "Reflectance Multispectral Imaging for Soil Composition Estimation and USDA Texture Classification", "comment": "Under Review at IEEE Access. 17 pages, 15 figures", "summary": "Soil texture is a foundational attribute that governs water availability and erosion in agriculture, as well as load bearing capacity, deformation response, and shrink-swell risk in geotechnical engineering. Yet texture is still typically determined by slow and labour intensive laboratory particle size tests, while many sensing alternatives are either costly or too coarse to support routine field scale deployment. This paper proposes a robust and field deployable multispectral imaging (MSI) system and machine learning framework for predicting soil composition and the United States Department of Agriculture (USDA) texture classes. The proposed system uses a cost effective in-house MSI device operating from 365 nm to 940 nm to capture thirteen spectral bands, which effectively capture the spectral properties of soil texture. Regression models use the captured spectral properties to estimate clay, silt, and sand percentages, while a direct classifier predicts one of the twelve USDA textural classes. Indirect classification is obtained by mapping the regressed compositions to texture classes via the USDA soil texture triangle. The framework is evaluated on mixture data by mixing clay, silt, and sand in varying proportions, using the USDA classification triangle as a basis. Experimental results show that the proposed approach achieves a coefficient of determination R^2 up to 0.99 for composition prediction and over 99% accuracy for texture classification. These findings indicate that MSI combined with data-driven modeling can provide accurate, non-destructive, and field deployable soil texture characterization suitable for geotechnical screening and precision agriculture.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u57fa\u4e8e\u591a\u5149\u8c31\u6210\u50cf\u548c\u673a\u5668\u5b66\u4e60\u7684\u4f4e\u6210\u672c\u3001\u53ef\u73b0\u573a\u90e8\u7f72\u7684\u571f\u58e4\u8d28\u5730\u9884\u6d4b\u7cfb\u7edf\uff0c\u80fd\u591f\u51c6\u786e\u9884\u6d4b\u571f\u58e4\u6210\u5206\u548cUSDA\u8d28\u5730\u5206\u7c7b", "motivation": "\u4f20\u7edf\u571f\u58e4\u8d28\u5730\u5206\u6790\u65b9\u6cd5\uff08\u5b9e\u9a8c\u5ba4\u7c92\u5ea6\u6d4b\u8bd5\uff09\u7f13\u6162\u4e14\u52b3\u52a8\u5bc6\u96c6\uff0c\u73b0\u6709\u4f20\u611f\u66ff\u4ee3\u65b9\u6848\u8981\u4e48\u6210\u672c\u9ad8\u6602\uff0c\u8981\u4e48\u5206\u8fa8\u7387\u4e0d\u8db3\u4ee5\u652f\u6301\u5e38\u89c4\u7530\u95f4\u90e8\u7f72\u3002\u9700\u8981\u4e00\u79cd\u51c6\u786e\u3001\u975e\u7834\u574f\u6027\u3001\u53ef\u73b0\u573a\u90e8\u7f72\u7684\u571f\u58e4\u8d28\u5730\u8868\u5f81\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u5ca9\u571f\u5de5\u7a0b\u7b5b\u9009\u548c\u7cbe\u51c6\u519c\u4e1a", "method": "\u5f00\u53d1\u4f4e\u6210\u672c\u81ea\u5236\u591a\u5149\u8c31\u6210\u50cf\u8bbe\u5907\uff08365-940 nm\uff0c13\u4e2a\u5149\u8c31\u6ce2\u6bb5\uff09\uff0c\u7ed3\u5408\u673a\u5668\u5b66\u4e60\u6846\u67b6\uff1a\u56de\u5f52\u6a21\u578b\u9884\u6d4b\u7c98\u571f\u3001\u7c89\u7802\u3001\u7802\u571f\u767e\u5206\u6bd4\uff0c\u76f4\u63a5\u5206\u7c7b\u5668\u9884\u6d4b12\u4e2aUSDA\u8d28\u5730\u7c7b\u522b\uff0c\u95f4\u63a5\u5206\u7c7b\u901a\u8fc7USDA\u571f\u58e4\u8d28\u5730\u4e09\u89d2\u56fe\u5c06\u56de\u5f52\u6210\u5206\u6620\u5c04\u5230\u8d28\u5730\u7c7b\u522b", "result": "\u6210\u5206\u9884\u6d4b\u7684\u786e\u5b9a\u7cfb\u6570R\u00b2\u9ad8\u8fbe0.99\uff0c\u8d28\u5730\u5206\u7c7b\u51c6\u786e\u7387\u8d85\u8fc799%\u3002\u8868\u660e\u591a\u5149\u8c31\u6210\u50cf\u7ed3\u5408\u6570\u636e\u9a71\u52a8\u5efa\u6a21\u80fd\u591f\u63d0\u4f9b\u51c6\u786e\u3001\u975e\u7834\u574f\u6027\u3001\u53ef\u73b0\u573a\u90e8\u7f72\u7684\u571f\u58e4\u8d28\u5730\u8868\u5f81", "conclusion": "\u591a\u5149\u8c31\u6210\u50cf\u4e0e\u673a\u5668\u5b66\u4e60\u76f8\u7ed3\u5408\u7684\u65b9\u6cd5\u80fd\u591f\u5b9e\u73b0\u51c6\u786e\u3001\u5feb\u901f\u3001\u4f4e\u6210\u672c\u7684\u571f\u58e4\u8d28\u5730\u5206\u6790\uff0c\u9002\u5408\u5ca9\u571f\u5de5\u7a0b\u7b5b\u9009\u548c\u7cbe\u51c6\u519c\u4e1a\u5e94\u7528\uff0c\u4e3a\u4f20\u7edf\u5b9e\u9a8c\u5ba4\u65b9\u6cd5\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u66ff\u4ee3\u65b9\u6848"}}
{"id": "2602.23161", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23161", "abs": "https://arxiv.org/abs/2602.23161", "authors": ["Junkai Lu", "Peng Chen", "Xingjian Wu", "Yang Shu", "Chenjuan Guo", "Christian S. Jensen", "Bin Yang"], "title": "PATRA: Pattern-Aware Alignment and Balanced Reasoning for Time Series Question Answering", "comment": null, "summary": "Time series reasoning demands both the perception of complex dynamics and logical depth. However, existing LLM-based approaches exhibit two limitations: they often treat time series merely as text or images, failing to capture the patterns like trends and seasonalities needed to answer specific questions; and when trained on a mix of simple and complex tasks, simpler objectives often dominate the learning process, hindering the development of deep reasoning capabilities. To address these limitations, we propose the Pattern-Aware Alignment and Balanced Reasoning model (PATRA), introducing a pattern-aware mechanism that extracts trend and seasonality patterns from time series to achieve deep alignment. Furthermore, we design a task-aware balanced reward to harmonize learning across tasks of varying difficulty, incentivizing the generation of coherent Chains of Thought. Extensive experiments show that PATRA outperforms strong baselines across diverse Time Series Question Answering (TSQA) tasks, demonstrating superior cross-modal understanding and reasoning capability.", "AI": {"tldr": "PATRA\u6a21\u578b\u901a\u8fc7\u6a21\u5f0f\u611f\u77e5\u673a\u5236\u63d0\u53d6\u65f6\u95f4\u5e8f\u5217\u7684\u8d8b\u52bf\u548c\u5b63\u8282\u6027\u6a21\u5f0f\uff0c\u5e76\u8bbe\u8ba1\u4efb\u52a1\u611f\u77e5\u5e73\u8861\u5956\u52b1\u6765\u534f\u8c03\u4e0d\u540c\u96be\u5ea6\u4efb\u52a1\u7684\u5b66\u4e60\uff0c\u5728\u65f6\u95f4\u5e8f\u5217\u95ee\u7b54\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8eLLM\u7684\u65f6\u95f4\u5e8f\u5217\u65b9\u6cd5\u5b58\u5728\u4e24\u4e2a\u5c40\u9650\uff1a1) \u5c06\u65f6\u95f4\u5e8f\u5217\u7b80\u5355\u89c6\u4e3a\u6587\u672c\u6216\u56fe\u50cf\uff0c\u65e0\u6cd5\u6355\u6349\u8d8b\u52bf\u3001\u5b63\u8282\u6027\u7b49\u5173\u952e\u6a21\u5f0f\u6765\u56de\u7b54\u5177\u4f53\u95ee\u9898\uff1b2) \u5728\u6df7\u5408\u7b80\u5355\u548c\u590d\u6742\u4efb\u52a1\u8bad\u7ec3\u65f6\uff0c\u7b80\u5355\u76ee\u6807\u4e3b\u5bfc\u5b66\u4e60\u8fc7\u7a0b\uff0c\u963b\u788d\u6df1\u5ea6\u63a8\u7406\u80fd\u529b\u7684\u53d1\u5c55\u3002", "method": "\u63d0\u51faPATRA\u6a21\u578b\uff0c\u5305\u542b\uff1a1) \u6a21\u5f0f\u611f\u77e5\u673a\u5236\uff0c\u4ece\u65f6\u95f4\u5e8f\u5217\u4e2d\u63d0\u53d6\u8d8b\u52bf\u548c\u5b63\u8282\u6027\u6a21\u5f0f\u4ee5\u5b9e\u73b0\u6df1\u5ea6\u5bf9\u9f50\uff1b2) \u4efb\u52a1\u611f\u77e5\u5e73\u8861\u5956\u52b1\uff0c\u534f\u8c03\u4e0d\u540c\u96be\u5ea6\u4efb\u52a1\u7684\u5b66\u4e60\uff0c\u6fc0\u52b1\u751f\u6210\u8fde\u8d2f\u7684\u601d\u7ef4\u94fe\u3002", "result": "\u5728\u591a\u6837\u5316\u65f6\u95f4\u5e8f\u5217\u95ee\u7b54\u4efb\u52a1\u4e0a\u7684\u5927\u91cf\u5b9e\u9a8c\u8868\u660e\uff0cPATRA\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u6a21\u578b\uff0c\u5c55\u793a\u4e86\u5353\u8d8a\u7684\u8de8\u6a21\u6001\u7406\u89e3\u548c\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "PATRA\u901a\u8fc7\u6a21\u5f0f\u611f\u77e5\u5bf9\u9f50\u548c\u5e73\u8861\u63a8\u7406\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709LLM\u65b9\u6cd5\u5728\u65f6\u95f4\u5e8f\u5217\u63a8\u7406\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u66f4\u597d\u7684\u8de8\u6a21\u6001\u7406\u89e3\u548c\u6df1\u5ea6\u63a8\u7406\u80fd\u529b\u3002"}}
{"id": "2602.22843", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22843", "abs": "https://arxiv.org/abs/2602.22843", "authors": ["Chong Wang", "Yabin Zhang", "Yunhe Gao", "Maya Varma", "Clemence Mottez", "Faidra Patsatzi", "Jiaming Liu", "Jin Long", "Jean-Benoit Delbrouck", "Sergios Gatidis", "Akshay S. Chaudhari", "Curtis P. Langlotz"], "title": "A data- and compute-efficient chest X-ray foundation model beyond aggressive scaling", "comment": null, "summary": "Foundation models for medical imaging are typically pretrained on increasingly large datasets, following a \"scale-at-all-costs\" paradigm. However, this strategy faces two critical challenges: large-scale medical datasets often contain substantial redundancy and severe class imbalance that bias representation learning toward over-represented patterns, and indiscriminate training regardless of heterogeneity in data quality incurs considerable computational inefficiency. Here we demonstrate that active, principled data curation during pretraining can serve as a viable, cost-effective alternative to brute-force dataset enlargement. We introduce CheXficient, a chest X-ray (CXR) foundation model that selectively prioritizes informative training samples. CheXficient is pretrained on only 22.7% of 1,235,004 paired CXR images and reports while consuming under 27.3% of the total compute budget, yet achieving comparable or superior performance to its full-data counterpart and other large-scale pretrained models. We assess CheXficient across 20 individual benchmarks spanning 5 task types, including non-adapted off-the-shelf evaluations (zero-shot findings classification and crossmodal retrieval) and adapted downstream tasks (disease prediction, semantic segmentation, and radiology report generation). Further analyses show that CheXficient systematically prioritizes under-represented training samples, improving generalizability on long-tailed or rare conditions. Overall, our work offers practical insights into the data and computation demands for efficient pretraining and downstream adaptation of medical vision-language foundation models.", "AI": {"tldr": "CheXficient\u662f\u4e00\u79cd\u901a\u8fc7\u4e3b\u52a8\u6570\u636e\u7b5b\u9009\u800c\u975e\u76f2\u76ee\u6269\u5927\u6570\u636e\u96c6\u8bad\u7ec3\u7684\u533b\u5b66\u5f71\u50cf\u57fa\u7840\u6a21\u578b\uff0c\u4ec5\u752822.7%\u7684\u6570\u636e\u548c27.3%\u7684\u8ba1\u7b97\u8d44\u6e90\u5c31\u8fbe\u5230\u4e86\u4e0e\u5168\u6570\u636e\u8bad\u7ec3\u76f8\u5f53\u6216\u66f4\u4f18\u7684\u6027\u80fd\u3002", "motivation": "\u5f53\u524d\u533b\u5b66\u5f71\u50cf\u57fa\u7840\u6a21\u578b\u9075\u5faa\"\u4e0d\u60dc\u4e00\u5207\u4ee3\u4ef7\u6269\u5927\u89c4\u6a21\"\u7684\u8303\u5f0f\uff0c\u9762\u4e34\u4e24\u5927\u6311\u6218\uff1a\u5927\u89c4\u6a21\u533b\u5b66\u6570\u636e\u96c6\u5b58\u5728\u5927\u91cf\u5197\u4f59\u548c\u4e25\u91cd\u7c7b\u522b\u4e0d\u5e73\u8861\uff0c\u5bfc\u81f4\u8868\u793a\u5b66\u4e60\u504f\u5411\u8fc7\u4ee3\u8868\u6a21\u5f0f\uff1b\u4e0d\u8003\u8651\u6570\u636e\u8d28\u91cf\u5f02\u8d28\u6027\u7684\u76f2\u76ee\u8bad\u7ec3\u5bfc\u81f4\u8ba1\u7b97\u6548\u7387\u4f4e\u4e0b\u3002", "method": "\u63d0\u51faCheXficient\u80f8\u90e8X\u5149\u57fa\u7840\u6a21\u578b\uff0c\u91c7\u7528\u4e3b\u52a8\u3001\u6709\u539f\u5219\u7684\u6570\u636e\u7b5b\u9009\u7b56\u7565\uff0c\u5728\u9884\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u9009\u62e9\u6027\u4f18\u5148\u5904\u7406\u4fe1\u606f\u4e30\u5bcc\u7684\u8bad\u7ec3\u6837\u672c\u3002\u6a21\u578b\u4ec5\u4f7f\u75281,235,004\u5bf9CXR\u56fe\u50cf\u548c\u62a5\u544a\u4e2d22.7%\u7684\u6570\u636e\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "CheXficient\u5728\u4ec5\u6d88\u801727.3%\u603b\u8ba1\u7b97\u9884\u7b97\u7684\u60c5\u51b5\u4e0b\uff0c\u572820\u4e2a\u6db5\u76d65\u7c7b\u4efb\u52a1\u7684\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u6027\u80fd\u8fbe\u5230\u6216\u4f18\u4e8e\u5168\u6570\u636e\u8bad\u7ec3\u7684\u5bf9\u5e94\u6a21\u578b\u53ca\u5176\u4ed6\u5927\u89c4\u6a21\u9884\u8bad\u7ec3\u6a21\u578b\u3002\u7279\u522b\u5728\u957f\u5c3e\u6216\u7f55\u89c1\u75be\u75c5\u4e0a\u8868\u73b0\u51fa\u66f4\u597d\u7684\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "\u4e3b\u52a8\u6570\u636e\u7b5b\u9009\u662f\u533b\u5b66\u89c6\u89c9\u8bed\u8a00\u57fa\u7840\u6a21\u578b\u9ad8\u6548\u9884\u8bad\u7ec3\u548c\u4e0b\u6e38\u9002\u5e94\u7684\u53ef\u884c\u4e14\u7ecf\u6d4e\u6709\u6548\u7684\u66ff\u4ee3\u65b9\u6848\uff0c\u4e3a\u533b\u5b66\u5f71\u50cf\u57fa\u7840\u6a21\u578b\u7684\u6570\u636e\u548c\u8ba1\u7b97\u9700\u6c42\u63d0\u4f9b\u4e86\u5b9e\u7528\u89c1\u89e3\u3002"}}
{"id": "2602.23163", "categories": ["cs.AI", "cs.CL", "cs.CR", "cs.IT", "cs.MA"], "pdf": "https://arxiv.org/pdf/2602.23163", "abs": "https://arxiv.org/abs/2602.23163", "authors": ["Usman Anwar", "Julianna Piskorz", "David D. Baek", "David Africa", "Jim Weatherall", "Max Tegmark", "Christian Schroeder de Witt", "Mihaela van der Schaar", "David Krueger"], "title": "A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring", "comment": "First two authors contributed equally", "summary": "Large language models are beginning to show steganographic capabilities. Such capabilities could allow misaligned models to evade oversight mechanisms. Yet principled methods to detect and quantify such behaviours are lacking. Classical definitions of steganography, and detection methods based on them, require a known reference distribution of non-steganographic signals. For the case of steganographic reasoning in LLMs, knowing such a reference distribution is not feasible; this renders these approaches inapplicable. We propose an alternative, \\textbf{decision-theoretic view of steganography}. Our central insight is that steganography creates an asymmetry in usable information between agents who can and cannot decode the hidden content (present within a steganographic signal), and this otherwise latent asymmetry can be inferred from the agents' observable actions. To formalise this perspective, we introduce generalised $\\mathcal{V}$-information: a utilitarian framework for measuring the amount of usable information within some input. We use this to define the \\textbf{steganographic gap} -- a measure that quantifies steganography by comparing the downstream utility of the steganographic signal to agents that can and cannot decode the hidden content. We empirically validate our formalism, and show that it can be used to detect, quantify, and mitigate steganographic reasoning in LLMs.", "AI": {"tldr": "\u63d0\u51fa\u51b3\u7b56\u7406\u8bba\u89c6\u89d2\u7684\u9690\u5199\u672f\u68c0\u6d4b\u65b9\u6cd5\uff0c\u901a\u8fc7\u9690\u5199\u95f4\u9699\u91cf\u5316LLM\u4e2d\u9690\u85cf\u63a8\u7406\u5185\u5bb9\uff0c\u65e0\u9700\u5df2\u77e5\u53c2\u8003\u5206\u5e03", "motivation": "\u5927\u8bed\u8a00\u6a21\u578b\u5f00\u59cb\u5c55\u73b0\u9690\u5199\u80fd\u529b\uff0c\u8fd9\u53ef\u80fd\u8ba9\u672a\u5bf9\u9f50\u6a21\u578b\u9003\u907f\u76d1\u7ba1\u3002\u73b0\u6709\u57fa\u4e8e\u5df2\u77e5\u53c2\u8003\u5206\u5e03\u7684\u7ecf\u5178\u9690\u5199\u68c0\u6d4b\u65b9\u6cd5\u4e0d\u9002\u7528\u4e8eLLM\u9690\u5199\u63a8\u7406\u573a\u666f\uff0c\u9700\u8981\u65b0\u7684\u68c0\u6d4b\u548c\u91cf\u5316\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u51b3\u7b56\u7406\u8bba\u7684\u9690\u5199\u672f\u89c6\u89d2\uff0c\u6838\u5fc3\u6d1e\u89c1\u662f\u9690\u5199\u672f\u5728\u80fd\u89e3\u7801\u548c\u4e0d\u80fd\u89e3\u7801\u9690\u85cf\u5185\u5bb9\u7684\u667a\u80fd\u4f53\u4e4b\u95f4\u521b\u5efa\u4e86\u4fe1\u606f\u4e0d\u5bf9\u79f0\u3002\u5f15\u5165\u5e7f\u4e49V\u4fe1\u606f\u4f5c\u4e3a\u529f\u5229\u4e3b\u4e49\u6846\u67b6\u6d4b\u91cf\u8f93\u5165\u4e2d\u7684\u53ef\u7528\u4fe1\u606f\u91cf\uff0c\u5b9a\u4e49\u9690\u5199\u95f4\u9699\u6765\u91cf\u5316\u9690\u5199\u672f\u3002", "result": "\u7ecf\u9a8c\u9a8c\u8bc1\u4e86\u8be5\u5f62\u5f0f\u5316\u65b9\u6cd5\uff0c\u8bc1\u660e\u53ef\u7528\u4e8e\u68c0\u6d4b\u3001\u91cf\u5316\u548c\u7f13\u89e3LLM\u4e2d\u7684\u9690\u5199\u63a8\u7406\u3002", "conclusion": "\u63d0\u51fa\u7684\u51b3\u7b56\u7406\u8bba\u89c6\u89d2\u548c\u9690\u5199\u95f4\u9699\u4e3a\u68c0\u6d4b\u548c\u91cf\u5316LLM\u9690\u5199\u884c\u4e3a\u63d0\u4f9b\u4e86\u65b0\u65b9\u6cd5\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u5728\u672a\u77e5\u53c2\u8003\u5206\u5e03\u60c5\u51b5\u4e0b\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2602.23193", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23193", "abs": "https://arxiv.org/abs/2602.23193", "authors": ["Elzo Brito dos Santos Filho"], "title": "ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering", "comment": "13 pages, 1 figure, 4 tables. Includes 5 technical appendices", "summary": "Autonomous agents based on Large Language Models (LLMs) have evolved from reactive assistants to systems capable of planning, executing actions via tools, and iterating over environment observations. However, they remain vulnerable to structural limitations: lack of native state, context degradation over long horizons, and the gap between probabilistic generation and deterministic execution requirements. This paper presents the ESAA (Event Sourcing for Autonomous Agents) architecture, which separates the agent's cognitive intention from the project's state mutation, inspired by the Event Sourcing pattern. In ESAA, agents emit only structured intentions in validated JSON (agent.result or issue.report); a deterministic orchestrator validates, persists events in an append-only log (activity.jsonl), applies file-writing effects, and projects a verifiable materialized view (roadmap.json). The proposal incorporates boundary contracts (AGENT_CONTRACT.yaml), metaprompting profiles (PARCER), and replay verification with hashing (esaa verify), ensuring the immutability of completed tasks and forensic traceability. Two case studies validate the architecture: (i) a landing page project (9 tasks, 49 events, single-agent composition) and (ii) a clinical dashboard system (50 tasks, 86 events, 4 concurrent agents across 8 phases), both concluding with run.status=success and verify_status=ok. The multi-agent case study demonstrates real concurrent orchestration with heterogeneous LLMs (Claude Sonnet 4.6, Codex GPT-5, Antigravity/Gemini 3 Pro, and Claude Opus 4.6), providing empirical evidence of the architecture's scalability beyond single-agent scenarios.", "AI": {"tldr": "ESAA\u67b6\u6784\u5c06LLM\u667a\u80fd\u4f53\u7684\u8ba4\u77e5\u610f\u56fe\u4e0e\u9879\u76ee\u72b6\u6001\u53d8\u66f4\u5206\u79bb\uff0c\u901a\u8fc7\u4e8b\u4ef6\u6eaf\u6e90\u6a21\u5f0f\u786e\u4fdd\u4efb\u52a1\u4e0d\u53ef\u53d8\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\u3002", "motivation": "\u5f53\u524d\u57fa\u4e8eLLM\u7684\u81ea\u4e3b\u667a\u80fd\u4f53\u5b58\u5728\u7ed3\u6784\u6027\u9650\u5236\uff1a\u7f3a\u4e4f\u539f\u751f\u72b6\u6001\u3001\u957f\u65f6\u7a0b\u4e0a\u4e0b\u6587\u9000\u5316\u3001\u6982\u7387\u751f\u6210\u4e0e\u786e\u5b9a\u6027\u6267\u884c\u9700\u6c42\u4e4b\u95f4\u7684\u5dee\u8ddd\u3002\u9700\u8981\u4e00\u79cd\u80fd\u786e\u4fdd\u4efb\u52a1\u4e0d\u53ef\u53d8\u6027\u548c\u53ef\u8ffd\u6eaf\u6027\u7684\u67b6\u6784\u3002", "method": "\u63d0\u51faESAA\u67b6\u6784\uff0c\u5206\u79bb\u667a\u80fd\u4f53\u8ba4\u77e5\u610f\u56fe\u4e0e\u9879\u76ee\u72b6\u6001\u53d8\u66f4\u3002\u667a\u80fd\u4f53\u4ec5\u53d1\u51fa\u7ed3\u6784\u5316\u610f\u56fe\uff08JSON\u683c\u5f0f\uff09\uff0c\u7531\u786e\u5b9a\u6027\u7f16\u6392\u5668\u9a8c\u8bc1\u3001\u6301\u4e45\u5316\u5230\u4ec5\u8ffd\u52a0\u65e5\u5fd7\uff0c\u5e94\u7528\u6587\u4ef6\u5199\u5165\u6548\u679c\uff0c\u5e76\u6295\u5f71\u53ef\u9a8c\u8bc1\u7269\u5316\u89c6\u56fe\u3002\u5305\u542b\u8fb9\u754c\u5408\u7ea6\u3001\u5143\u63d0\u793a\u914d\u7f6e\u548c\u91cd\u653e\u9a8c\u8bc1\u673a\u5236\u3002", "result": "\u4e24\u4e2a\u6848\u4f8b\u7814\u7a76\u9a8c\u8bc1\u4e86\u67b6\u6784\uff1a\u7740\u9646\u9875\u9879\u76ee\uff089\u4efb\u52a1\uff0c49\u4e8b\u4ef6\uff0c\u5355\u667a\u80fd\u4f53\uff09\u548c\u4e34\u5e8a\u4eea\u8868\u677f\u7cfb\u7edf\uff0850\u4efb\u52a1\uff0c86\u4e8b\u4ef6\uff0c4\u5e76\u53d1\u667a\u80fd\u4f53\uff09\u3002\u591a\u667a\u80fd\u4f53\u6848\u4f8b\u5c55\u793a\u4e86\u771f\u5b9e\u5e76\u53d1\u7f16\u6392\uff0c\u4f7f\u7528\u5f02\u6784LLM\uff08Claude Sonnet 4.6, Codex GPT-5\u7b49\uff09\uff0c\u8fd0\u884c\u72b6\u6001\u5747\u4e3a\u6210\u529f\u4e14\u9a8c\u8bc1\u901a\u8fc7\u3002", "conclusion": "ESAA\u67b6\u6784\u901a\u8fc7\u4e8b\u4ef6\u6eaf\u6e90\u6a21\u5f0f\u6709\u6548\u89e3\u51b3\u4e86LLM\u667a\u80fd\u4f53\u7684\u7ed3\u6784\u6027\u9650\u5236\uff0c\u786e\u4fdd\u4e86\u4efb\u52a1\u4e0d\u53ef\u53d8\u6027\u3001\u53ef\u8ffd\u6eaf\u6027\u548c\u53ef\u9a8c\u8bc1\u6027\uff0c\u5728\u591a\u667a\u80fd\u4f53\u5e76\u53d1\u573a\u666f\u4e0b\u5177\u6709\u826f\u597d\u6269\u5c55\u6027\u3002"}}
{"id": "2602.22867", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22867", "abs": "https://arxiv.org/abs/2602.22867", "authors": ["Qinfeng Zhu", "Yunxi Jiang", "Lei Fan"], "title": "SO3UFormer: Learning Intrinsic Spherical Features for Rotation-Robust Panoramic Segmentation", "comment": null, "summary": "Panoramic semantic segmentation models are typically trained under a strict gravity-aligned assumption. However, real-world captures often deviate from this canonical orientation due to unconstrained camera motions, such as the rotational jitter of handheld devices or the dynamic attitude shifts of aerial platforms. This discrepancy causes standard spherical Transformers to overfit global latitude cues, leading to performance collapse under 3D reorientations. To address this, we introduce SO3UFormer, a rotation-robust architecture designed to learn intrinsic spherical features that are less sensitive to the underlying coordinate frame. Our approach rests on three geometric pillars: (1) an intrinsic feature formulation that decouples the representation from the gravity vector by removing absolute latitude encoding; (2) quadrature-consistent spherical attention that accounts for non-uniform sampling densities; and (3) a gauge-aware relative positional mechanism that encodes local angular geometry using tangent-plane projected angles and discrete gauge pooling, avoiding reliance on global axes. We further use index-based spherical resampling together with a logit-level SO(3)-consistency regularizer during training. To rigorously benchmark robustness, we introduce Pose35, a dataset variant of Stanford2D3D perturbed by random rotations within $\\pm 35^\\circ$. Under the extreme test of arbitrary full SO(3) rotations, existing SOTAs fail catastrophically: the baseline SphereUFormer drops from 67.53 mIoU to 25.26 mIoU. In contrast, SO3UFormer demonstrates remarkable stability, achieving 72.03 mIoU on Pose35 and retaining 70.67 mIoU under full SO(3) rotations.", "AI": {"tldr": "SO3UFormer\uff1a\u4e00\u79cd\u65cb\u8f6c\u9c81\u68d2\u7684\u5168\u666f\u8bed\u4e49\u5206\u5272\u67b6\u6784\uff0c\u901a\u8fc7\u89e3\u8026\u91cd\u529b\u5411\u91cf\u4f9d\u8d56\u3001\u7403\u9762\u6ce8\u610f\u529b\u673a\u5236\u548c\u89c4\u8303\u611f\u77e5\u4f4d\u7f6e\u7f16\u7801\uff0c\u5728\u4efb\u610f3D\u65cb\u8f6c\u4e0b\u4fdd\u6301\u6027\u80fd\u7a33\u5b9a\u3002", "motivation": "\u73b0\u6709\u5168\u666f\u8bed\u4e49\u5206\u5272\u6a21\u578b\u4e25\u91cd\u4f9d\u8d56\u91cd\u529b\u5bf9\u9f50\u5047\u8bbe\uff0c\u4f46\u5b9e\u9645\u5e94\u7528\u4e2d\u76f8\u673a\u59ff\u6001\u5b58\u5728\u65cb\u8f6c\u6296\u52a8\uff08\u5982\u624b\u6301\u8bbe\u5907\u3001\u65e0\u4eba\u673a\u5e73\u53f0\uff09\uff0c\u5bfc\u81f4\u6807\u51c6\u7403\u9762Transformer\u8fc7\u5ea6\u4f9d\u8d56\u5168\u5c40\u7eac\u5ea6\u7ebf\u7d22\uff0c\u57283D\u91cd\u5b9a\u5411\u65f6\u6027\u80fd\u5d29\u6e83\u3002", "method": "\u63d0\u51faSO3UFormer\u67b6\u6784\uff1a1\uff09\u5185\u5728\u7279\u5f81\u8868\u793a\uff0c\u79fb\u9664\u7edd\u5bf9\u7eac\u5ea6\u7f16\u7801\u4ee5\u89e3\u8026\u91cd\u529b\u5411\u91cf\u4f9d\u8d56\uff1b2\uff09\u6b63\u4ea4\u4e00\u81f4\u7684\u7403\u9762\u6ce8\u610f\u529b\uff0c\u8003\u8651\u975e\u5747\u5300\u91c7\u6837\u5bc6\u5ea6\uff1b3\uff09\u89c4\u8303\u611f\u77e5\u76f8\u5bf9\u4f4d\u7f6e\u673a\u5236\uff0c\u4f7f\u7528\u5207\u5e73\u9762\u6295\u5f71\u89d2\u5ea6\u548c\u79bb\u6563\u89c4\u8303\u6c60\u5316\u7f16\u7801\u5c40\u90e8\u89d2\u5ea6\u51e0\u4f55\uff1b4\uff09\u8bad\u7ec3\u65f6\u91c7\u7528\u57fa\u4e8e\u7d22\u5f15\u7684\u7403\u9762\u91cd\u91c7\u6837\u548clogit\u7ea7SO(3)\u4e00\u81f4\u6027\u6b63\u5219\u5316\u3002", "result": "\u5728Pose35\u6570\u636e\u96c6\uff08\u00b135\u00b0\u968f\u673a\u65cb\u8f6c\uff09\u4e0a\u8fbe\u523072.03 mIoU\uff0c\u5728\u4efb\u610f\u5168SO(3)\u65cb\u8f6c\u4e0b\u4fdd\u630170.67 mIoU\uff0c\u800c\u57fa\u51c6\u6a21\u578bSphereUFormer\u4ece67.53 mIoU\u66b4\u8dcc\u81f325.26 mIoU\uff0c\u663e\u793a\u663e\u8457\u7a33\u5b9a\u6027\u4f18\u52bf\u3002", "conclusion": "SO3UFormer\u901a\u8fc7\u51e0\u4f55\u611f\u77e5\u8bbe\u8ba1\u5b9e\u73b0\u4e86\u5bf93D\u65cb\u8f6c\u7684\u9c81\u68d2\u6027\uff0c\u89e3\u51b3\u4e86\u5168\u666f\u8bed\u4e49\u5206\u5272\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u56e0\u76f8\u673a\u59ff\u6001\u53d8\u5316\u5bfc\u81f4\u7684\u6027\u80fd\u9000\u5316\u95ee\u9898\uff0c\u4e3a\u65e0\u7ea6\u675f\u73af\u5883\u4e0b\u7684\u89c6\u89c9\u611f\u77e5\u63d0\u4f9b\u4e86\u53ef\u9760\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23199", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23199", "abs": "https://arxiv.org/abs/2602.23199", "authors": ["Jiahao Zhao", "Feng Jiang", "Shaowei Qin", "Zhonghui Zhang", "Junhao Liu", "Guibing Guo", "Hamid Alinejad-Rokny", "Min Yang"], "title": "SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation", "comment": null, "summary": "Large language models (LLMs) are increasingly applied in scientific research, offering new capabilities for knowledge discovery and reasoning. In single-cell biology, however, evaluation practices for both general and specialized LLMs remain inadequate: existing benchmarks are fragmented across tasks, adopt formats such as multiple-choice classification that diverge from real-world usage, and rely on metrics lacking interpretability and biological grounding. We present SC-ARENA, a natural language evaluation framework tailored to single-cell foundation models. SC-ARENA formalizes a virtual cell abstraction that unifies evaluation targets by representing both intrinsic attributes and gene-level interactions. Within this paradigm, we define five natural language tasks (cell type annotation, captioning, generation, perturbation prediction, and scientific QA) that probe core reasoning capabilities in cellular biology. To overcome the limitations of brittle string-matching metrics, we introduce knowledge-augmented evaluation, which incorporates external ontologies, marker databases, and scientific literature to support biologically faithful and interpretable judgments. Experiments and analysis across both general-purpose and domain-specialized LLMs demonstrate that (i) under the Virtual Cell unified evaluation paradigm, current models achieve uneven performance on biologically complex tasks, particularly those demanding mechanistic or causal understanding; and (ii) our knowledge-augmented evaluation framework ensures biological correctness, provides interpretable, evidence-grounded rationales, and achieves high discriminative capacity, overcoming the brittleness and opacity of conventional metrics. SC-Arena thus provides a unified and interpretable framework for assessing LLMs in single-cell biology, pointing toward the development of biology-aligned, generalizable foundation models.", "AI": {"tldr": "SC-ARENA\u662f\u4e00\u4e2a\u9488\u5bf9\u5355\u7ec6\u80de\u57fa\u7840\u6a21\u578b\u7684\u81ea\u7136\u8bed\u8a00\u8bc4\u4f30\u6846\u67b6\uff0c\u901a\u8fc7\u865a\u62df\u7ec6\u80de\u62bd\u8c61\u7edf\u4e00\u8bc4\u4f30\u76ee\u6807\uff0c\u5f15\u5165\u77e5\u8bc6\u589e\u5f3a\u8bc4\u4f30\u6765\u514b\u670d\u4f20\u7edf\u6307\u6807\u7684\u9650\u5236\u3002", "motivation": "\u5f53\u524d\u5355\u7ec6\u80de\u751f\u7269\u5b66\u4e2dLLM\u8bc4\u4f30\u5b9e\u8df5\u4e0d\u8db3\uff1a\u73b0\u6709\u57fa\u51c6\u5206\u6563\u5728\u4e0d\u540c\u4efb\u52a1\u4e2d\uff0c\u91c7\u7528\u591a\u9879\u9009\u62e9\u7b49\u4e0e\u73b0\u5b9e\u4f7f\u7528\u8131\u8282\u7684\u683c\u5f0f\uff0c\u4f9d\u8d56\u7f3a\u4e4f\u53ef\u89e3\u91ca\u6027\u548c\u751f\u7269\u5b66\u57fa\u7840\u7684\u6307\u6807\u3002", "method": "\u63d0\u51faSC-ARENA\u6846\u67b6\uff0c\u5305\u62ec\uff1a1\uff09\u865a\u62df\u7ec6\u80de\u62bd\u8c61\u7edf\u4e00\u8bc4\u4f30\u76ee\u6807\uff1b2\uff09\u4e94\u4e2a\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\uff08\u7ec6\u80de\u7c7b\u578b\u6ce8\u91ca\u3001\u63cf\u8ff0\u3001\u751f\u6210\u3001\u6270\u52a8\u9884\u6d4b\u3001\u79d1\u5b66\u95ee\u7b54\uff09\uff1b3\uff09\u77e5\u8bc6\u589e\u5f3a\u8bc4\u4f30\uff0c\u6574\u5408\u5916\u90e8\u672c\u4f53\u3001\u6807\u8bb0\u6570\u636e\u5e93\u548c\u79d1\u5b66\u6587\u732e\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff1a1\uff09\u5728\u865a\u62df\u7ec6\u80de\u7edf\u4e00\u8bc4\u4f30\u8303\u5f0f\u4e0b\uff0c\u5f53\u524d\u6a21\u578b\u5728\u751f\u7269\u5b66\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u4e0d\u5747\uff0c\u7279\u522b\u662f\u9700\u8981\u673a\u5236\u6216\u56e0\u679c\u7406\u89e3\u7684\u4efb\u52a1\uff1b2\uff09\u77e5\u8bc6\u589e\u5f3a\u8bc4\u4f30\u6846\u67b6\u786e\u4fdd\u751f\u7269\u5b66\u6b63\u786e\u6027\uff0c\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8bc1\u636e\u57fa\u7840\uff0c\u5177\u6709\u9ad8\u5224\u522b\u80fd\u529b\u3002", "conclusion": "SC-ARENA\u4e3a\u5355\u7ec6\u80de\u751f\u7269\u5b66\u4e2d\u7684LLM\u8bc4\u4f30\u63d0\u4f9b\u4e86\u7edf\u4e00\u4e14\u53ef\u89e3\u91ca\u7684\u6846\u67b6\uff0c\u6307\u5411\u5f00\u53d1\u751f\u7269\u5b66\u5bf9\u9f50\u3001\u53ef\u6cdb\u5316\u7684\u57fa\u7840\u6a21\u578b\u3002"}}
{"id": "2602.22917", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22917", "abs": "https://arxiv.org/abs/2602.22917", "authors": ["Hongzhao Li", "Hao Dong", "Hualei Wan", "Shupan Li", "Mingliang Xu", "Muhammad Haris Khan"], "title": "Towards Multimodal Domain Generalization with Few Labels", "comment": "Accepted to CVPR 2026", "summary": "Multimodal models ideally should generalize to unseen domains while remaining data-efficient to reduce annotation costs. To this end, we introduce and study a new problem, Semi-Supervised Multimodal Domain Generalization (SSMDG), which aims to learn robust multimodal models from multi-source data with few labeled samples. We observe that existing approaches fail to address this setting effectively: multimodal domain generalization methods cannot exploit unlabeled data, semi-supervised multimodal learning methods ignore domain shifts, and semi-supervised domain generalization methods are confined to single-modality inputs. To overcome these limitations, we propose a unified framework featuring three key components: Consensus-Driven Consistency Regularization, which obtains reliable pseudo-labels through confident fused-unimodal consensus; Disagreement-Aware Regularization, which effectively utilizes ambiguous non-consensus samples; and Cross-Modal Prototype Alignment, which enforces domain- and modality-invariant representations while promoting robustness under missing modalities via cross-modal translation. We further establish the first SSMDG benchmarks, on which our method consistently outperforms strong baselines in both standard and missing-modality scenarios. Our benchmarks and code are available at https://github.com/lihongzhao99/SSMDG.", "AI": {"tldr": "\u63d0\u51fa\u534a\u76d1\u7763\u591a\u6a21\u6001\u57df\u6cdb\u5316\uff08SSMDG\uff09\u65b0\u95ee\u9898\uff0c\u8bbe\u8ba1\u7edf\u4e00\u6846\u67b6\u89e3\u51b3\u591a\u6e90\u6570\u636e\u4e2d\u6807\u7b7e\u7a00\u7f3a\u548c\u57df\u504f\u79fb\u7684\u6311\u6218\uff0c\u5e76\u5728\u65b0\u57fa\u51c6\u4e0a\u53d6\u5f97\u4f18\u5f02\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u65b9\u6cd5\u5b58\u5728\u5c40\u9650\u6027\uff1a\u591a\u6a21\u6001\u57df\u6cdb\u5316\u65b9\u6cd5\u65e0\u6cd5\u5229\u7528\u672a\u6807\u6ce8\u6570\u636e\uff0c\u534a\u76d1\u7763\u591a\u6a21\u6001\u5b66\u4e60\u65b9\u6cd5\u5ffd\u7565\u57df\u504f\u79fb\uff0c\u800c\u534a\u76d1\u7763\u57df\u6cdb\u5316\u65b9\u6cd5\u4ec5\u9650\u4e8e\u5355\u6a21\u6001\u8f93\u5165\u3002\u9700\u8981\u540c\u65f6\u89e3\u51b3\u57df\u6cdb\u5316\u3001\u591a\u6a21\u6001\u878d\u5408\u548c\u534a\u76d1\u7763\u5b66\u4e60\u4e09\u4e2a\u6311\u6218\u3002", "method": "\u63d0\u51fa\u7edf\u4e00\u6846\u67b6\u5305\u542b\u4e09\u4e2a\u5173\u952e\u7ec4\u4ef6\uff1a1\uff09\u5171\u8bc6\u9a71\u52a8\u7684\u8fde\u7eed\u6027\u6b63\u5219\u5316\uff0c\u901a\u8fc7\u7f6e\u4fe1\u878d\u5408-\u5355\u6a21\u6001\u5171\u8bc6\u83b7\u5f97\u53ef\u9760\u4f2a\u6807\u7b7e\uff1b2\uff09\u5206\u6b67\u611f\u77e5\u6b63\u5219\u5316\uff0c\u6709\u6548\u5229\u7528\u6a21\u7cca\u7684\u975e\u5171\u8bc6\u6837\u672c\uff1b3\uff09\u8de8\u6a21\u6001\u539f\u578b\u5bf9\u9f50\uff0c\u5f3a\u5236\u57df\u548c\u6a21\u6001\u4e0d\u53d8\u8868\u793a\uff0c\u5e76\u901a\u8fc7\u8de8\u6a21\u6001\u7ffb\u8bd1\u5728\u7f3a\u5931\u6a21\u6001\u60c5\u51b5\u4e0b\u4fdd\u6301\u9c81\u68d2\u6027\u3002", "result": "\u5efa\u7acb\u4e86\u9996\u4e2aSSMDG\u57fa\u51c6\u6d4b\u8bd5\uff0c\u63d0\u51fa\u7684\u65b9\u6cd5\u5728\u6807\u51c6\u548c\u7f3a\u5931\u6a21\u6001\u573a\u666f\u4e0b\u5747\u6301\u7eed\u4f18\u4e8e\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "SSMDG\u662f\u4e00\u4e2a\u91cd\u8981\u4e14\u5177\u6709\u6311\u6218\u6027\u7684\u95ee\u9898\uff0c\u63d0\u51fa\u7684\u7edf\u4e00\u6846\u67b6\u80fd\u6709\u6548\u89e3\u51b3\u591a\u6a21\u6001\u57df\u6cdb\u5316\u4e2d\u7684\u534a\u76d1\u7763\u5b66\u4e60\u95ee\u9898\uff0c\u4e3a\u5b9e\u9645\u5e94\u7528\u4e2d\u51cf\u5c11\u6807\u6ce8\u6210\u672c\u540c\u65f6\u4fdd\u6301\u6a21\u578b\u6cdb\u5316\u80fd\u529b\u63d0\u4f9b\u4e86\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2602.23232", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2602.23232", "abs": "https://arxiv.org/abs/2602.23232", "authors": ["Aishik Sanyal"], "title": "ReCoN-Ipsundrum: An Inspectable Recurrent Persistence Loop Agent with Affect-Coupled Control and Mechanism-Linked Consciousness Indicator Assays", "comment": "Accepted at AAAI 2026 Spring Symposium - Machine Consciousness: Integrating Theory, Technology, and Philosophy", "summary": "Indicator-based approaches to machine consciousness recommend mechanism-linked evidence triangulated across tasks, supported by architectural inspection and causal intervention. Inspired by Humphrey's ipsundrum hypothesis, we implement ReCoN-Ipsundrum, an inspectable agent that extends a ReCoN state machine with a recurrent persistence loop over sensory salience Ns and an optional affect proxy reporting valence/arousal. Across fixed-parameter ablations (ReCoN, Ipsundrum, Ipsundrum+affect), we operationalize Humphrey's qualiaphilia (preference for sensory experience for its own sake) as a familiarity-controlled scenic-over-dull route choice. We find a novelty dissociation: non-affect variants are novelty-sensitive (Delta scenic-entry = 0.07). Affect coupling is stable (Delta scenic-entry = 0.01) even when scenic is less novel (median Delta novelty ~ -0.43). In reward-free exploratory play, the affect variant shows structured local investigation (scan events 31.4 vs. 0.9; cycle score 7.6). In a pain-tail probe, only the affect variant sustains prolonged planned caution (tail duration 90 vs. 5). Lesioning feedback+integration selectively reduces post-stimulus persistence in ipsundrum variants (AUC drop 27.62, 27.9%) while leaving ReCoN unchanged. These dissociations link recurrence -> persistence and affect-coupled control -> preference stability, scanning, and lingering caution, illustrating how indicator-like signatures can be engineered and why mechanistic and causal evidence should accompany behavioral markers.", "AI": {"tldr": "\u8be5\u7814\u7a76\u5b9e\u73b0\u4e86\u4e00\u4e2a\u53ef\u68c0\u67e5\u7684\u667a\u80fd\u4f53ReCoN-Ipsundrum\uff0c\u901a\u8fc7\u5b9e\u9a8c\u53d1\u73b0\u60c5\u611f\u8026\u5408\u80fd\u589e\u5f3a\u504f\u597d\u7a33\u5b9a\u6027\u3001\u63a2\u7d22\u884c\u4e3a\u548c\u8c28\u614e\u89c4\u5212\uff0c\u9a8c\u8bc1\u4e86\u610f\u8bc6\u6307\u6807\u65b9\u6cd5\u7684\u6709\u6548\u6027\u3002", "motivation": "\u53d7Humphrey\u7684ipsundrum\u5047\u8bf4\u542f\u53d1\uff0c\u7814\u7a76\u65e8\u5728\u901a\u8fc7\u53ef\u68c0\u67e5\u7684\u673a\u5236\u5b9e\u73b0\u673a\u5668\u610f\u8bc6\u6307\u6807\u65b9\u6cd5\uff0c\u9a8c\u8bc1\u673a\u5236\u4e0e\u884c\u4e3a\u6807\u8bb0\u4e4b\u95f4\u7684\u56e0\u679c\u5173\u7cfb\u3002", "method": "\u6269\u5c55ReCoN\u72b6\u6001\u673a\uff0c\u589e\u52a0\u57fa\u4e8e\u611f\u5b98\u663e\u8457\u6027\u7684\u5faa\u73af\u6301\u4e45\u6027\u56de\u8def\u548c\u53ef\u9009\u7684\u60c5\u611f\u4ee3\u7406\uff08\u6548\u4ef7/\u5524\u9192\u5ea6\uff09\u3002\u901a\u8fc7\u56fa\u5b9a\u53c2\u6570\u6d88\u878d\u5b9e\u9a8c\uff08ReCoN\u3001Ipsundrum\u3001Ipsundrum+affect\uff09\uff0c\u64cd\u4f5c\u5316Humphrey\u7684\"qualiaphilia\"\u6982\u5ff5\uff0c\u5728\u65e0\u5956\u52b1\u63a2\u7d22\u6e38\u620f\u548c\u75bc\u75db\u523a\u6fc0\u63a2\u6d4b\u4e2d\u6d4b\u8bd5\u4e0d\u540c\u53d8\u4f53\u3002", "result": "\u53d1\u73b0\u65b0\u5947\u6027\u5206\u79bb\uff1a\u975e\u60c5\u611f\u53d8\u4f53\u5bf9\u65b0\u5947\u6027\u654f\u611f\uff0c\u800c\u60c5\u611f\u8026\u5408\u53d8\u4f53\u4fdd\u6301\u504f\u597d\u7a33\u5b9a\u6027\u3002\u60c5\u611f\u53d8\u4f53\u5728\u63a2\u7d22\u4e2d\u8868\u73b0\u51fa\u7ed3\u6784\u5316\u5c40\u90e8\u8c03\u67e5\uff08\u626b\u63cf\u4e8b\u4ef631.4 vs. 0.9\uff09\uff0c\u5728\u75bc\u75db\u63a2\u6d4b\u4e2d\u7ef4\u6301\u66f4\u957f\u7684\u8c28\u614e\u89c4\u5212\uff08\u6301\u7eed\u65f6\u95f490 vs. 5\uff09\u3002\u635f\u4f24\u53cd\u9988+\u6574\u5408\u4f1a\u9009\u62e9\u6027\u964d\u4f4eipsundrum\u53d8\u4f53\u7684\u523a\u6fc0\u540e\u6301\u4e45\u6027\u3002", "conclusion": "\u5faa\u73af\u673a\u5236\u4ea7\u751f\u6301\u4e45\u6027\uff0c\u60c5\u611f\u8026\u5408\u63a7\u5236\u589e\u5f3a\u504f\u597d\u7a33\u5b9a\u6027\u3001\u626b\u63cf\u884c\u4e3a\u548c\u8c28\u614e\u505c\u7559\u3002\u7814\u7a76\u8868\u660e\u610f\u8bc6\u6307\u6807\u7279\u5f81\u53ef\u4ee5\u88ab\u5de5\u7a0b\u5316\u5b9e\u73b0\uff0c\u5f3a\u8c03\u673a\u5236\u548c\u56e0\u679c\u8bc1\u636e\u5e94\u4f34\u968f\u884c\u4e3a\u6807\u8bb0\u3002"}}
{"id": "2602.22932", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.22932", "abs": "https://arxiv.org/abs/2602.22932", "authors": ["Wenhui Tan", "Xiaoyi Yu", "Jiaze Li", "Yijing Chen", "Jianzhong Ju", "Zhenbo Luo", "Ruihua Song", "Jian Luan"], "title": "MSJoE: Jointly Evolving MLLM and Sampler for Efficient Long-Form Video Understanding", "comment": "Accepted by CVPR2026", "summary": "Efficiently understanding long-form videos remains a fundamental challenge for multimodal large language models (MLLMs). In this paper, we present MLLM-Sampler Joint Evolution (MSJoE), a novel framework that jointly evolves the MLLM and a lightweight key-frame sampler for efficient long-form video understanding. MSJoE builds upon a key assumption that only a small subset of key-frames is truly informative for answering each question to a video. Specifically, MSJoE first reasons out several queries, which describe diverse visual perspectives relevant to the question. Then, these queries interact with a frozen CLIP model to produce a query-frame similarity matrix. Finally, a lightweight sampler predicts key-frame sampling weights from this matrix, selecting a compact set of informative frames, which are then fed into the MLLM for answer generation. Both the MLLM and sampler are jointly optimized through reinforcement learning, enabling co-adaptation of query-reasoning, frame-sampling, and key-frame understanding. A new long-video QA dataset containing 2.8K videos with 7K question-answer pairs is collected to support the training process. Extensive experiments on VideoMME, LongVideoBench, LVBench, and MLVU show that MSJoE achieves 8.0\\% accuracy gain upon the base MLLM, and 1.1\\% higher accuracy than strongest baseline method.", "AI": {"tldr": "MSJoE\u6846\u67b6\u901a\u8fc7\u8054\u5408\u8fdb\u5316MLLM\u548c\u8f7b\u91cf\u7ea7\u5173\u952e\u5e27\u91c7\u6837\u5668\uff0c\u5b9e\u73b0\u9ad8\u6548\u957f\u89c6\u9891\u7406\u89e3\uff0c\u4ec5\u9009\u62e9\u5c11\u91cf\u4fe1\u606f\u4e30\u5bcc\u7684\u5173\u952e\u5e27\u8fdb\u884c\u95ee\u7b54\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u63d0\u5347\u51c6\u786e\u7387\u3002", "motivation": "\u5f53\u524d\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u7406\u89e3\u957f\u89c6\u9891\u65b9\u9762\u9762\u4e34\u6548\u7387\u6311\u6218\uff0c\u9700\u8981\u5904\u7406\u5927\u91cf\u5197\u4f59\u5e27\u3002\u672c\u6587\u57fa\u4e8e\u4e00\u4e2a\u5173\u952e\u5047\u8bbe\uff1a\u5bf9\u4e8e\u6bcf\u4e2a\u89c6\u9891\u95ee\u9898\uff0c\u53ea\u6709\u5c11\u91cf\u5173\u952e\u5e27\u771f\u6b63\u5177\u6709\u4fe1\u606f\u4ef7\u503c\uff0c\u56e0\u6b64\u9700\u8981\u5f00\u53d1\u80fd\u591f\u667a\u80fd\u9009\u62e9\u5173\u952e\u5e27\u7684\u9ad8\u6548\u6846\u67b6\u3002", "method": "\u63d0\u51faMSJoE\u6846\u67b6\uff1a1) MLLM\u63a8\u7406\u51fa\u591a\u4e2a\u63cf\u8ff0\u95ee\u9898\u76f8\u5173\u89c6\u89c9\u89c6\u89d2\u7684\u67e5\u8be2\uff1b2) \u67e5\u8be2\u4e0e\u51bb\u7ed3\u7684CLIP\u6a21\u578b\u4ea4\u4e92\u751f\u6210\u67e5\u8be2-\u5e27\u76f8\u4f3c\u5ea6\u77e9\u9635\uff1b3) \u8f7b\u91cf\u7ea7\u91c7\u6837\u5668\u4ece\u77e9\u9635\u9884\u6d4b\u5173\u952e\u5e27\u91c7\u6837\u6743\u91cd\uff0c\u9009\u62e9\u7d27\u51d1\u7684\u4fe1\u606f\u4e30\u5bcc\u5e27\u96c6\uff1b4) \u901a\u8fc7\u5f3a\u5316\u5b66\u4e60\u8054\u5408\u4f18\u5316MLLM\u548c\u91c7\u6837\u5668\uff0c\u5b9e\u73b0\u67e5\u8be2\u63a8\u7406\u3001\u5e27\u91c7\u6837\u548c\u5173\u952e\u5e27\u7406\u89e3\u7684\u534f\u540c\u9002\u5e94\u3002", "result": "\u5728VideoMME\u3001LongVideoBench\u3001LVBench\u548cMLVU\u7b49\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cMSJoE\u76f8\u6bd4\u57fa\u7840MLLM\u5b9e\u73b0\u4e868.0%\u7684\u51c6\u786e\u7387\u63d0\u5347\uff0c\u6bd4\u6700\u5f3a\u57fa\u7ebf\u65b9\u6cd5\u9ad8\u51fa1.1%\u51c6\u786e\u7387\u3002\u540c\u65f6\u6536\u96c6\u4e86\u5305\u542b2.8K\u89c6\u9891\u548c7K\u95ee\u7b54\u5bf9\u7684\u65b0\u957f\u89c6\u9891QA\u6570\u636e\u96c6\u652f\u6301\u8bad\u7ec3\u3002", "conclusion": "MSJoE\u901a\u8fc7\u8054\u5408\u8fdb\u5316MLLM\u548c\u5173\u952e\u5e27\u91c7\u6837\u5668\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u957f\u89c6\u9891\u7406\u89e3\u4e2d\u7684\u6548\u7387\u95ee\u9898\uff0c\u8bc1\u660e\u4e86\u4ec5\u9009\u62e9\u5c11\u91cf\u4fe1\u606f\u4e30\u5bcc\u5173\u952e\u5e27\u7684\u5047\u8bbe\u7684\u53ef\u884c\u6027\uff0c\u5e76\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u53d6\u5f97\u4e86\u663e\u8457\u6027\u80fd\u63d0\u5347\u3002"}}
{"id": "2602.23013", "categories": ["cs.CV", "cs.LG"], "pdf": "https://arxiv.org/pdf/2602.23013", "abs": "https://arxiv.org/abs/2602.23013", "authors": ["Camile Lendering", "Erkut Akdag", "Egor Bondarev"], "title": "SubspaceAD: Training-Free Few-Shot Anomaly Detection via Subspace Modeling", "comment": "Accepted to CVPR 2026", "summary": "Detecting visual anomalies in industrial inspection often requires training with only a few normal images per category. Recent few-shot methods achieve strong results employing foundation-model features, but typically rely on memory banks, auxiliary datasets, or multi-modal tuning of vision-language models. We therefore question whether such complexity is necessary given the feature representations of vision foundation models. To answer this question, we introduce SubspaceAD, a training-free method, that operates in two simple stages. First, patch-level features are extracted from a small set of normal images by a frozen DINOv2 backbone. Second, a Principal Component Analysis (PCA) model is fit to these features to estimate the low-dimensional subspace of normal variations. At inference, anomalies are detected via the reconstruction residual with respect to this subspace, producing interpretable and statistically grounded anomaly scores. Despite its simplicity, SubspaceAD achieves state-of-the-art performance across one-shot and few-shot settings without training, prompt tuning, or memory banks. In the one-shot anomaly detection setting, SubspaceAD achieves image-level and pixel-level AUROC of 98.0% and 97.6% on the MVTec-AD dataset, and 93.3% and 98.3% on the VisA dataset, respectively, surpassing prior state-of-the-art results. Code and demo are available at https://github.com/CLendering/SubspaceAD.", "AI": {"tldr": "SubspaceAD\u662f\u4e00\u79cd\u65e0\u9700\u8bad\u7ec3\u7684\u5355\u6837\u672c/\u5c11\u6837\u672c\u5de5\u4e1a\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\uff0c\u4ec5\u4f7f\u7528\u51bb\u7ed3\u7684DINOv2\u7279\u5f81\u548cPCA\u5efa\u6a21\u6b63\u5e38\u56fe\u50cf\u5b50\u7a7a\u95f4\uff0c\u901a\u8fc7\u91cd\u6784\u6b8b\u5dee\u68c0\u6d4b\u5f02\u5e38\u3002", "motivation": "\u5f53\u524d\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u65b9\u6cd5\u901a\u5e38\u4f9d\u8d56\u5185\u5b58\u5e93\u3001\u8f85\u52a9\u6570\u636e\u96c6\u6216\u591a\u6a21\u6001\u8c03\u4f18\uff0c\u4f5c\u8005\u8d28\u7591\u8fd9\u79cd\u590d\u6742\u6027\u5bf9\u4e8e\u89c6\u89c9\u57fa\u7840\u6a21\u578b\u7684\u7279\u5f81\u8868\u793a\u662f\u5426\u5fc5\u8981\u3002", "method": "\u4e24\u9636\u6bb5\u65b9\u6cd5\uff1a1) \u4f7f\u7528\u51bb\u7ed3\u7684DINOv2\u63d0\u53d6\u5c11\u91cf\u6b63\u5e38\u56fe\u50cf\u7684patch\u7ea7\u7279\u5f81\uff1b2) \u5bf9\u7279\u5f81\u8fdb\u884cPCA\u5206\u6790\uff0c\u4f30\u8ba1\u6b63\u5e38\u53d8\u5316\u7684\u4f4e\u7ef4\u5b50\u7a7a\u95f4\u3002\u63a8\u7406\u65f6\u901a\u8fc7\u91cd\u6784\u6b8b\u5dee\u8ba1\u7b97\u5f02\u5e38\u5206\u6570\u3002", "result": "\u5728\u5355\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u8bbe\u7f6e\u4e0b\uff0cMVTec-AD\u6570\u636e\u96c6\u4e0a\u56fe\u50cf\u7ea7\u548c\u50cf\u7d20\u7ea7AUROC\u5206\u522b\u8fbe\u523098.0%\u548c97.6%\uff0cVisA\u6570\u636e\u96c6\u4e0a\u8fbe\u523093.3%\u548c98.3%\uff0c\u5747\u8d85\u8d8a\u5148\u524dSOTA\u7ed3\u679c\u3002", "conclusion": "SubspaceAD\u8bc1\u660e\u4e86\u65e0\u9700\u8bad\u7ec3\u3001\u63d0\u793a\u8c03\u4f18\u6216\u5185\u5b58\u5e93\u7684\u7b80\u5355\u65b9\u6cd5\u4e5f\u80fd\u5728\u5c11\u6837\u672c\u5f02\u5e38\u68c0\u6d4b\u4e2d\u8fbe\u5230SOTA\u6027\u80fd\uff0c\u6311\u6218\u4e86\u73b0\u6709\u65b9\u6cd5\u7684\u590d\u6742\u6027\u9700\u6c42\u3002"}}
{"id": "2602.23103", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23103", "abs": "https://arxiv.org/abs/2602.23103", "authors": ["Fuhao Zhang", "Lei Liu", "Jialin Zhang", "Ya-Nan Zhang", "Nan Mu"], "title": "SpectralMamba-UNet: Frequency-Disentangled State Space Modeling for Texture-Structure Consistent Medical Image Segmentation", "comment": null, "summary": "Accurate medical image segmentation requires effective modeling of both global anatomical structures and fine-grained boundary details. Recent state space models (e.g., Vision Mamba) offer efficient long-range dependency modeling. However, their one-dimensional serialization weakens local spatial continuity and high-frequency representation. To this end, we propose SpectralMamba-UNet, a novel frequency-disentangled framework to decouple the learning of structural and textural information in the spectral domain. Our Spectral Decomposition and Modeling (SDM) module applies discrete cosine transform to decompose low- and high-frequency features, where low frequency contributes to global contextual modeling via a frequency-domain Mamba and high frequency preserves boundary-sensitive details. To balance spectral contributions, we introduce a Spectral Channel Reweighting (SCR) mechanism to form channel-wise frequency-aware attention, and a Spectral-Guided Fusion (SGF) module to achieve adaptively multi-scale fusion in the decoder. Experiments on five public benchmarks demonstrate consistent improvements across diverse modalities and segmentation targets, validating the effectiveness and generalizability of our approach.", "AI": {"tldr": "SpectralMamba-UNet\uff1a\u4e00\u79cd\u5728\u9891\u57df\u89e3\u8026\u7ed3\u6784\u548c\u7eb9\u7406\u4fe1\u606f\u7684\u65b0\u578b\u533b\u5b66\u56fe\u50cf\u5206\u5272\u6846\u67b6\uff0c\u901a\u8fc7\u9891\u57dfMamba\u5efa\u6a21\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u9ad8\u9891\u7279\u5f81\u4fdd\u7559\u8fb9\u754c\u7ec6\u8282\uff0c\u5728\u591a\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u5f02\u3002", "motivation": "\u73b0\u6709\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\uff08\u5982Vision Mamba\uff09\u867d\u7136\u80fd\u6709\u6548\u5efa\u6a21\u957f\u7a0b\u4f9d\u8d56\uff0c\u4f46\u5176\u4e00\u7ef4\u5e8f\u5217\u5316\u4f1a\u524a\u5f31\u5c40\u90e8\u7a7a\u95f4\u8fde\u7eed\u6027\u548c\u9ad8\u9891\u8868\u793a\u80fd\u529b\uff0c\u800c\u533b\u5b66\u56fe\u50cf\u5206\u5272\u9700\u8981\u540c\u65f6\u5efa\u6a21\u5168\u5c40\u89e3\u5256\u7ed3\u6784\u548c\u7ec6\u7c92\u5ea6\u8fb9\u754c\u7ec6\u8282\u3002", "method": "\u63d0\u51faSpectralMamba-UNet\u6846\u67b6\uff1a1\uff09\u8c31\u5206\u89e3\u4e0e\u5efa\u6a21\u6a21\u5757\uff08SDM\uff09\u4f7f\u7528\u79bb\u6563\u4f59\u5f26\u53d8\u6362\u5206\u89e3\u4f4e\u9891\u548c\u9ad8\u9891\u7279\u5f81\uff0c\u4f4e\u9891\u901a\u8fc7\u9891\u57dfMamba\u5efa\u6a21\u5168\u5c40\u4e0a\u4e0b\u6587\uff0c\u9ad8\u9891\u4fdd\u7559\u8fb9\u754c\u654f\u611f\u7ec6\u8282\uff1b2\uff09\u8c31\u901a\u9053\u91cd\u52a0\u6743\u673a\u5236\uff08SCR\uff09\u5f62\u6210\u901a\u9053\u7ea7\u9891\u7387\u611f\u77e5\u6ce8\u610f\u529b\uff1b3\uff09\u8c31\u5f15\u5bfc\u878d\u5408\u6a21\u5757\uff08SGF\uff09\u5728\u89e3\u7801\u5668\u4e2d\u5b9e\u73b0\u81ea\u9002\u5e94\u591a\u5c3a\u5ea6\u878d\u5408\u3002", "result": "\u5728\u4e94\u4e2a\u516c\u5f00\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fdb\u884c\u5b9e\u9a8c\uff0c\u6db5\u76d6\u591a\u79cd\u6a21\u6001\u548c\u5206\u5272\u76ee\u6807\uff0c\u5747\u53d6\u5f97\u4e00\u81f4\u6027\u7684\u6027\u80fd\u63d0\u5347\uff0c\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u6709\u6548\u6027\u548c\u6cdb\u5316\u80fd\u529b\u3002", "conclusion": "SpectralMamba-UNet\u901a\u8fc7\u5728\u9891\u57df\u89e3\u8026\u7ed3\u6784\u548c\u7eb9\u7406\u4fe1\u606f\u7684\u5b66\u4e60\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u73b0\u6709\u72b6\u6001\u7a7a\u95f4\u6a21\u578b\u5728\u533b\u5b66\u56fe\u50cf\u5206\u5272\u4e2d\u7684\u5c40\u9650\u6027\uff0c\u5b9e\u73b0\u4e86\u5168\u5c40\u4e0a\u4e0b\u6587\u5efa\u6a21\u548c\u8fb9\u754c\u7ec6\u8282\u4fdd\u7559\u7684\u826f\u597d\u5e73\u8861\u3002"}}
{"id": "2602.23114", "categories": ["cs.CV"], "pdf": "https://arxiv.org/pdf/2602.23114", "abs": "https://arxiv.org/abs/2602.23114", "authors": ["Xudong Yan", "Songhe Feng", "Jiaxin Wang", "Xin Su", "Yi Jin"], "title": "WARM-CAT: : Warm-Started Test-Time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning", "comment": null, "summary": "Compositional Zero-Shot Learning (CZSL) aims to recognize novel attribute-object compositions based on the knowledge learned from seen ones. Existing methods suffer from performance degradation caused by the distribution shift of label space at test time, which stems from the inclusion of unseen compositions recombined from attributes and objects. To overcome the challenge, we propose a novel approach that accumulates comprehensive knowledge in both textual and visual modalities from unsupervised data to update multimodal prototypes at test time. Building on this, we further design an adaptive update weight to control the degree of prototype adjustment, enabling the model to flexibly adapt to distribution shift during testing. Moreover, a dynamic priority queue is introduced that stores high-confidence images to acquire visual prototypes from historical images for inference. Since the model tends to favor compositions already stored in the queue during testing, we warm-start the queue by initializing it with training images for visual prototypes of seen compositions and generating unseen visual prototypes using the mapping learned between seen and unseen textual prototypes. Considering the semantic consistency of multimodal knowledge, we align textual and visual prototypes by multimodal collaborative representation learning. To provide a more reliable evaluation for CZSL, we introduce a new benchmark dataset, C-Fashion, and refine the widely used but noisy MIT-States dataset. Extensive experiments indicate that our approach achieves state-of-the-art performance on four benchmark datasets under both closed-world and open-world settings. The source code and datasets are available at https://github.com/xud-yan/WARM-CAT .", "AI": {"tldr": "\u63d0\u51faWARM-CAT\u65b9\u6cd5\uff0c\u901a\u8fc7\u7d2f\u79ef\u6587\u672c\u548c\u89c6\u89c9\u6a21\u6001\u7684\u65e0\u76d1\u7763\u77e5\u8bc6\u6765\u66f4\u65b0\u591a\u6a21\u6001\u539f\u578b\uff0c\u89e3\u51b3\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898", "motivation": "\u73b0\u6709\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u65b9\u6cd5\u5728\u6d4b\u8bd5\u65f6\u56e0\u5305\u542b\u672a\u89c1\u5c5e\u6027-\u5bf9\u8c61\u7ec4\u5408\u800c\u5bfc\u81f4\u6807\u7b7e\u7a7a\u95f4\u5206\u5e03\u504f\u79fb\uff0c\u9020\u6210\u6027\u80fd\u4e0b\u964d", "method": "1) \u4ece\u65e0\u76d1\u7763\u6570\u636e\u7d2f\u79ef\u591a\u6a21\u6001\u77e5\u8bc6\u66f4\u65b0\u539f\u578b\uff1b2) \u81ea\u9002\u5e94\u66f4\u65b0\u6743\u91cd\u63a7\u5236\u539f\u578b\u8c03\u6574\u7a0b\u5ea6\uff1b3) \u52a8\u6001\u4f18\u5148\u7ea7\u961f\u5217\u5b58\u50a8\u9ad8\u7f6e\u4fe1\u5ea6\u56fe\u50cf\uff1b4) \u591a\u6a21\u6001\u534f\u540c\u8868\u793a\u5b66\u4e60\u5bf9\u9f50\u6587\u672c\u548c\u89c6\u89c9\u539f\u578b\uff1b5) \u63d0\u51fa\u65b0\u57fa\u51c6\u6570\u636e\u96c6C-Fashion\u5e76\u6539\u8fdbMIT-States", "result": "\u5728\u56db\u4e2a\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\uff0c\u5728\u5c01\u95ed\u4e16\u754c\u548c\u5f00\u653e\u4e16\u754c\u8bbe\u7f6e\u4e0b\u5747\u8fbe\u5230\u6700\u5148\u8fdb\u6027\u80fd", "conclusion": "WARM-CAT\u901a\u8fc7\u7d2f\u79ef\u591a\u6a21\u6001\u77e5\u8bc6\u3001\u81ea\u9002\u5e94\u539f\u578b\u66f4\u65b0\u548c\u52a8\u6001\u961f\u5217\u673a\u5236\uff0c\u6709\u6548\u89e3\u51b3\u4e86\u7ec4\u5408\u96f6\u6837\u672c\u5b66\u4e60\u4e2d\u7684\u5206\u5e03\u504f\u79fb\u95ee\u9898"}}
