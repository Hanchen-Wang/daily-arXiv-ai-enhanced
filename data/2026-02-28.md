<div id=toc></div>

# Table of Contents

- [cs.CV](#cs.CV) [Total: 24]
- [cs.CL](#cs.CL) [Total: 7]
- [cs.AI](#cs.AI) [Total: 21]


<div id='cs.CV'></div>

# cs.CV [[Back]](#toc)

### [1] [No Caption, No Problem: Caption-Free Membership Inference via Model-Fitted Embeddings](https://arxiv.org/abs/2602.22689)
*Joonsung Jeon,Woo Jae Kim,Suhyeon Ha,Sooel Son,Sung-Eui Yoon*

Main category: cs.CV

TL;DR: MoFit：无需真实标注的成员推断攻击框架，通过构建与目标模型生成流形过拟合的合成条件输入来检测扩散模型的数据记忆问题


<details>
  <summary>Details</summary>
Motivation: 现有成员推断攻击方法依赖真实文本标注，但在实际场景中只有图像可用且文本标注未公开，这使得先前方法在仅使用视觉语言模型生成标注时效果不佳

Method: 两阶段框架：1) 模型拟合的代理优化 - 优化图像扰动以构建在成员样本学习的无条件先验区域中的代理；2) 代理驱动的嵌入提取 - 从代理导出模型拟合嵌入，作为查询图像的失配条件，放大成员样本的条件损失响应

Result: 在多个数据集和扩散模型上的实验表明，MoFit始终优于先前的VLM条件基线方法，并达到与依赖标注方法相竞争的性能

Conclusion: MoFit为无需真实标注的成员推断攻击提供了有效解决方案，能够检测扩散模型的数据记忆问题，在隐私和知识产权保护方面具有重要意义

Abstract: Latent diffusion models have achieved remarkable success in high-fidelity text-to-image generation, but their tendency to memorize training data raises critical privacy and intellectual property concerns. Membership inference attacks (MIAs) provide a principled way to audit such memorization by determining whether a given sample was included in training. However, existing approaches assume access to ground-truth captions. This assumption fails in realistic scenarios where only images are available and their textual annotations remain undisclosed, rendering prior methods ineffective when substituted with vision-language model (VLM) captions. In this work, we propose MoFit, a caption-free MIA framework that constructs synthetic conditioning inputs that are explicitly overfitted to the target model's generative manifold. Given a query image, MoFit proceeds in two stages: (i) model-fitted surrogate optimization, where a perturbation applied to the image is optimized to construct a surrogate in regions of the model's unconditional prior learned from member samples, and (ii) surrogate-driven embedding extraction, where a model-fitted embedding is derived from the surrogate and then used as a mismatched condition for the query image. This embedding amplifies conditional loss responses for member samples while leaving hold-outs relatively less affected, thereby enhancing separability in the absence of ground-truth captions. Our comprehensive experiments across multiple datasets and diffusion models demonstrate that MoFit consistently outperforms prior VLM-conditioned baselines and achieves performance competitive with caption-dependent methods.

</details>


### [2] [GFRRN: Explore the Gaps in Single Image Reflection Removal](https://arxiv.org/abs/2602.22695)
*Yu Chen,Zewei He,Xingyu Liu,Zixuan Chen,Zheming Lu*

Main category: cs.CV

TL;DR: 提出GFRRN网络解决单图像去反射中的特征语义理解差距和反射标签不一致问题，通过PEFT微调、统一标签生成器、自适应频率学习和动态代理注意力机制实现优越性能。


<details>
  <summary>Details</summary>
Motivation: 现有双流方法在单图像反射去除中存在两个主要问题：(1)预训练模型特征与去反射模型特征之间的语义理解差距；(2)合成数据与真实世界训练数据之间的反射标签不一致。

Method: 1. 采用参数高效微调(PEFT)策略，在预训练模型中集成可学习的Mona层以对齐训练方向；2. 设计标签生成器统一合成和真实数据的反射标签；3. 提出高斯自适应频率学习块(G-AFLB)自适应学习和融合频率先验；4. 采用动态代理注意力(DAA)替代基于窗口的注意力，动态建模窗口间和窗口内的重要性。

Result: 大量实验证明GFRRN的有效性，在单图像反射去除任务中实现了优于最先进方法的性能。

Conclusion: 提出的GFRRN网络通过解决特征语义差距和标签不一致问题，结合PEFT微调、统一标签生成、自适应频率学习和动态注意力机制，在单图像反射去除任务中取得了卓越性能。

Abstract: Prior dual-stream methods with the feature interaction mechanism have achieved remarkable performance in single image reflection removal (SIRR). However, they often struggle with (1) semantic understanding gap between the features of pre-trained models and those of reflection removal models, and (2) reflection label inconsistencies between synthetic and real-world training data. In this work, we first adopt the parameter efficient fine-tuning (PEFT) strategy by integrating several learnable Mona layers into the pre-trained model to align the training directions. Then, a label generator is designed to unify the reflection labels for both synthetic and real-world data. In addition, a Gaussian-based Adaptive Frequency Learning Block (G-AFLB) is proposed to adaptively learn and fuse the frequency priors, and a Dynamic Agent Attention (DAA) is employed as an alternative to window-based attention by dynamically modeling the significance levels across windows (inter-) and within an individual window (intra-). These components constitute our proposed Gap-Free Reflection Removal Network (GFRRN). Extensive experiments demonstrate the effectiveness of our GFRRN, achieving superior performance against state-of-the-art SIRR methods.

</details>


### [3] [UFO-DETR: Frequency-Guided End-to-End Detector for UAV Tiny Objects](https://arxiv.org/abs/2602.22712)
*Yuankai Chen,Kai Lin,Qihong Wu,Xinxuan Yang,Jiashuo Lai,Ruoen Chen,Haonan Shi,Minfan He,Meihua Wang*

Main category: cs.CV

TL;DR: 提出UFO-DETR检测框架，针对无人机图像小目标检测问题，通过LSKNet骨干网络、DAttention/AIFI模块和DynFreq-C3模块，在检测性能和计算效率上优于RT-DETR-L。


<details>
  <summary>Details</summary>
Motivation: 无人机图像小目标检测面临尺度变化大、分布密集、小目标占主导等挑战。现有算法依赖人工设计组件，通用检测器未针对无人机图像优化，难以平衡精度和复杂度。

Method: 提出端到端目标检测框架UFO-DETR：1) 基于LSKNet的骨干网络优化感受野并减少参数；2) 结合DAttention和AIFI模块灵活建模多尺度空间关系；3) 提出DynFreq-C3模块通过跨空间频率特征增强提升小目标检测能力。

Result: 实验结果表明，相比RT-DETR-L，该方法在检测性能和计算效率上都有显著优势，为无人机边缘计算提供了高效解决方案。

Conclusion: UFO-DETR框架有效解决了无人机图像小目标检测的挑战，在精度和效率上取得良好平衡，适用于边缘计算场景。

Abstract: Small target detection in UAV imagery faces significant challenges such as scale variations, dense distribution, and the dominance of small targets. Existing algorithms rely on manually designed components, and general-purpose detectors are not optimized for UAV images, making it difficult to balance accuracy and complexity. To address these challenges, this paper proposes an end-to-end object detection framework, UFO-DETR, which integrates an LSKNet-based backbone network to optimize the receptive field and reduce the number of parameters. By combining the DAttention and AIFI modules, the model flexibly models multi-scale spatial relationships, improving multi-scale target detection performance. Additionally, the DynFreq-C3 module is proposed to enhance small target detection capability through cross-space frequency feature enhancement. Experimental results show that, compared to RT-DETR-L, the proposed method offers significant advantages in both detection performance and computational efficiency, providing an efficient solution for UAV edge computing.

</details>


### [4] [IRSDE-Despeckle: A Physics-Grounded Diffusion Model for Generalizable Ultrasound Despeckling](https://arxiv.org/abs/2602.22717)
*Shuoqi Chen,Yujia Wu,Geoffrey P. Luke*

Main category: cs.CV

TL;DR: 提出基于扩散模型的超声图像去斑方法，通过模拟配对数据集进行监督训练，在保持解剖结构的同时抑制斑点噪声，优于传统方法和现有学习基线。


<details>
  <summary>Details</summary>
Motivation: 超声成像广泛用于实时无创诊断，但斑点噪声和相关伪影会降低图像质量并妨碍解读，需要有效的去斑方法。

Method: 基于图像恢复随机微分方程框架构建扩散模型，使用Matlab超声工具箱从无斑点的磁共振图像模拟超声图像创建配对数据集进行监督训练。

Result: 在模拟测试集上优于经典滤波器和近期学习基线的去斑方法，能重建抑制斑点同时保留解剖边缘和对比度的图像，通过跨模型方差量化预测不确定性。

Conclusion: 提出的扩散模型能有效去斑并保持解剖结构，不确定性分析可识别困难区域，对模拟探头设置的敏感性表明需要多样化训练和适应以实现稳健临床部署。

Abstract: Ultrasound imaging is widely used for real-time, noninvasive diagnosis, but speckle and related artifacts reduce image quality and can hinder interpretation. We present a diffusion-based ultrasound despeckling method built on the Image Restoration Stochastic Differential Equations framework. To enable supervised training, we curate large paired datasets by simulating ultrasound images from speckle-free magnetic resonance images using the Matlab UltraSound Toolbox. The proposed model reconstructs speckle-suppressed images while preserving anatomically meaningful edges and contrast. On a held-out simulated test set, our approach consistently outperforms classical filters and recent learning-based despeckling baselines. We quantify prediction uncertainty via cross-model variance and show that higher uncertainty correlates with higher reconstruction error, providing a practical indicator of difficult or failure-prone regions. Finally, we evaluate sensitivity to simulation probe settings and observe domain shift, motivating diversified training and adaptation for robust clinical deployment.

</details>


### [5] [HulluEdit: Single-Pass Evidence-Consistent Subspace Editing for Mitigating Hallucinations in Large Vision-Language Models](https://arxiv.org/abs/2602.22727)
*Yangguang Lin,Quan Fang,Yufei Li,Jiachen Sun,Junyu Gao,Jitao Sang*

Main category: cs.CV

TL;DR: HulluEdit：一种单次前向传播、无需参考模型的LVLM幻觉干预框架，通过正交子空间编辑选择性抑制幻觉模式，同时保持视觉基础能力。


<details>
  <summary>Details</summary>
Motivation: 大型视觉语言模型中的物体幻觉问题严重影响其可靠部署。现有方法在效率和准确性之间难以平衡：要么需要昂贵的参考模型和多次前向传播，要么采用静态编辑可能抑制真实的视觉证据。

Method: 提出正交子空间编辑方法，将模型隐藏状态分解为三个正交子空间：视觉证据、冲突先验和残差不确定性。通过选择性抑制幻觉模式而不干扰视觉基础，数学上保证对先验子空间的编辑完全不影响视觉成分。

Result: 在POPE和CHAIR等基准测试中实现了最先进的幻觉减少效果，同时在MME上保持通用能力，推理效率高。在各种架构上一致优于对比解码和静态子空间编辑基线方法。

Conclusion: HulluEdit为构建更可信的大型视觉语言模型提供了新途径，通过正交子空间编辑在保持视觉基础的同时有效减少幻觉，平衡了效率和准确性。

Abstract: Object hallucination in Large Vision-Language Models (LVLMs) significantly hinders their reliable deployment. Existing methods struggle to balance efficiency and accuracy: they often require expensive reference models and multiple forward passes, or apply static edits that risk suppressing genuine visual evidence. To address this, we introduce HulluEdit, a single-pass, reference-free intervention framework. Our core innovation is orthogonal subspace editing: we decompose the hidden states of the model into orthogonal subspaces - visual evidence, conflicting priors, and residual uncertainty - enabling selective suppression of hallucinatory patterns without interfering with visual grounding. This approach mathematically guarantees that edits applied to the prior subspace leave the visual component entirely unaffected. Extensive experiments show that HulluEdit achieves state-of-the-art hallucination reduction on benchmarks including POPE and CHAIR across diverse architectures, while preserving general capabilities on MME and maintaining efficient inference. Our method consistently outperforms contrastive decoding and static subspace editing baselines, offering a new pathway toward more trustworthy LVLMs.

</details>


### [6] [Asymmetric Idiosyncrasies in Multimodal Models](https://arxiv.org/abs/2602.22734)
*Muzi Tao,Chufan Shi,Huijuan Wang,Shengbang Tong,Xuezhe Ma*

Main category: cs.CV

TL;DR: 本文提出一个分类框架来量化图像描述模型（caption models）的风格特征及其对文生图模型的影响，发现描述模型有显著风格特征（分类准确率99.7%），但这些特征在生成的图像中基本消失（准确率最多50%）。


<details>
  <summary>Details</summary>
Motivation: 研究图像描述模型的风格特征（idiosyncrasies）及其对下游文生图模型的影响，量化不同描述模型的风格差异，并评估文生图模型是否能在生成的图像中保留这些风格特征。

Method: 设计系统性分析框架：给定生成的描述或对应图像，训练神经网络预测其来源的描述模型。通过文本分类（基于描述）和图像分类（基于生成的图像）来评估风格特征的保留程度。

Result: 文本分类准确率高达99.70%，表明描述模型有独特的风格特征；但图像分类准确率最多只有50%（即使是先进的Flux模型），说明这些风格特征在生成的图像中基本消失。进一步分析发现生成的图像未能保留描述中的关键变化，如细节程度、颜色纹理强调、场景中物体分布等差异。

Conclusion: 提出的分类框架为量化描述模型的风格特征和文生图系统的提示跟随能力提供了新方法。研究发现描述模型有显著风格特征，但文生图模型难以在生成的图像中保留这些特征，揭示了跨模态差异问题。

Abstract: In this work, we study idiosyncrasies in the caption models and their downstream impact on text-to-image models. We design a systematic analysis: given either a generated caption or the corresponding image, we train neural networks to predict the originating caption model. Our results show that text classification yields very high accuracy (99.70\%), indicating that captioning models embed distinctive stylistic signatures. In contrast, these signatures largely disappear in the generated images, with classification accuracy dropping to at most 50\% even for the state-of-the-art Flux model. To better understand this cross-modal discrepancy, we further analyze the data and find that the generated images fail to preserve key variations present in captions, such as differences in the level of detail, emphasis on color and texture, and the distribution of objects within a scene. Overall, our classification-based framework provides a novel methodology for quantifying both the stylistic idiosyncrasies of caption models and the prompt-following ability of text-to-image systems.

</details>


### [7] [ProjFlow: Projection Sampling with Flow Matching for Zero-Shot Exact Spatial Motion Control](https://arxiv.org/abs/2602.22742)
*Akihisa Watanabe,Qing Yu,Edgar Simo-Serra,Kent Fujiwara*

Main category: cs.CV

TL;DR: ProjFlow：一种无需训练、零样本的采样器，通过新颖的骨骼感知度量实现精确的线性空间约束，同时保持运动自然性。


<details>
  <summary>Details</summary>
Motivation: 现有方法需要任务特定训练或缓慢优化，且硬约束常破坏运动自然性。许多动画任务可表述为线性逆问题，需要一种能精确满足空间约束同时保持真实感的方法。

Method: 提出ProjFlow训练免费采样器，核心是新颖的骨骼拓扑感知度量，使采样器能通过整个骨骼的连贯修正来强制执行硬约束。对于稀疏输入（如填补关键帧间的长间隙），引入使用伪观测的时间变化公式。

Result: 在运动修复和2D到3D提升等代表性应用中，ProjFlow实现了精确约束满足，在零样本基线上匹配或提高了真实感，同时与基于训练的控制器保持竞争力。

Conclusion: ProjFlow通过骨骼感知度量和时间变化公式，实现了零样本、精确的线性空间约束满足，同时保持运动自然性，为动画任务提供了有效的训练免费解决方案。

Abstract: Generating human motion with precise spatial control is a challenging problem. Existing approaches often require task-specific training or slow optimization, and enforcing hard constraints frequently disrupts motion naturalness. Building on the observation that many animation tasks can be formulated as a linear inverse problem, we introduce ProjFlow, a training-free sampler that achieves zero-shot, exact satisfaction of linear spatial constraints while preserving motion realism. Our key advance is a novel kinematics-aware metric that encodes skeletal topology. This metric allows the sampler to enforce hard constraints by distributing corrections coherently across the entire skeleton, avoiding the unnatural artifacts of naive projection. Furthermore, for sparse inputs, such as filling in long gaps between a few keyframes, we introduce a time-varying formulation using pseudo-observations that fade during sampling. Extensive experiments on representative applications, motion inpainting, and 2D-to-3D lifting, demonstrate that ProjFlow achieves exact constraint satisfaction and matches or improves realism over zero-shot baselines, while remaining competitive with training-based controllers.

</details>


### [8] [SPATIALALIGN: Aligning Dynamic Spatial Relationships in Video Generation](https://arxiv.org/abs/2602.22745)
*Fengming Liu,Tat-Jen Cham,Chuanxia Zheng*

Main category: cs.CV

TL;DR: SPATIALALIGN是一个自改进框架，通过零阶正则化DPO微调T2V模型，提升其对文本提示中动态空间关系（DSR）的描绘能力，并设计了基于几何的DSR-SCORE评估指标。


<details>
  <summary>Details</summary>
Motivation: 当前文本到视频（T2V）生成器过于注重美学质量，而忽视了生成视频中的空间约束。现有方法在描绘文本提示中指定的动态空间关系（DSR）方面存在不足。

Method: 1. 提出SPATIALALIGN自改进框架；2. 使用零阶正则化直接偏好优化（DPO）微调T2V模型；3. 设计基于几何的DSR-SCORE评估指标；4. 构建包含多样DSR的文本-视频对数据集。

Result: 实验表明，经过微调的模型在空间关系描绘方面显著优于基线模型。DSR-SCORE提供了比依赖VLM评估更精确的空间对齐量化测量。

Conclusion: SPATIALALIGN框架有效提升了T2V模型对动态空间关系的理解与生成能力，为空间约束的视频生成提供了新的解决方案和评估方法。

Abstract: Most text-to-video (T2V) generators prioritize aesthetic quality, but often ignoring the spatial constraints in the generated videos. In this work, we present SPATIALALIGN, a self-improvement framework that enhances T2V models capabilities to depict Dynamic Spatial Relationships (DSR) specified in text prompts. We present a zeroth-order regularized Direct Preference Optimization (DPO) to fine-tune T2V models towards better alignment with DSR. Specifically, we design DSR-SCORE, a geometry-based metric that quantitatively measures the alignment between generated videos and the specified DSRs in prompts, which is a step forward from prior works that rely on VLM for evaluation. We also conduct a dataset of text-video pairs with diverse DSRs to facilitate the study. Extensive experiments demonstrate that our fine-tuned model significantly out performs the baseline in spatial relationships. The code will be released in Link.

</details>


### [9] [Beyond Detection: Multi-Scale Hidden-Code for Natural Image Deepfake Recovery and Factual Retrieval](https://arxiv.org/abs/2602.22759)
*Yuan-Chih Chen,Chun-Shien Lu*

Main category: cs.CV

TL;DR: 提出统一隐藏码恢复框架，实现篡改图像的检索与恢复，构建ImageNet-S基准测试，在多种水印方案中展现良好性能


<details>
  <summary>Details</summary>
Motivation: 当前图像真实性研究主要集中在深度伪造检测和定位，而篡改内容的恢复和事实检索相对未被充分探索。需要建立超越检测和定位的通用图像恢复基础

Method: 提出统一隐藏码恢复框架，将语义和感知信息编码为紧凑隐藏码表示，通过多尺度向量量化进行精炼，并利用条件Transformer模块增强上下文推理能力

Result: 在ImageNet-S基准测试上的大量实验表明，该方法展现出有前景的检索和重建性能，同时完全兼容多种水印管道

Conclusion: 该框架为超越检测和定位的通用图像恢复奠定了基础，为后处理和生成过程中的水印范式提供了统一的恢复解决方案

Abstract: Recent advances in image authenticity have primarily focused on deepfake detection and localization, leaving recovery of tampered contents for factual retrieval relatively underexplored. We propose a unified hidden-code recovery framework that enables both retrieval and restoration from post-hoc and in-generation watermarking paradigms. Our method encodes semantic and perceptual information into a compact hidden-code representation, refined through multi-scale vector quantization, and enhances contextual reasoning via conditional Transformer modules. To enable systematic evaluation for natural images, we construct ImageNet-S, a benchmark that provides paired image-label factual retrieval tasks. Extensive experiments on ImageNet-S demonstrate that our method exhibits promising retrieval and reconstruction performance while remaining fully compatible with diverse watermarking pipelines. This framework establishes a foundation for general-purpose image recovery beyond detection and localization.

</details>


### [10] [TrajTok: Learning Trajectory Tokens enables better Video Understanding](https://arxiv.org/abs/2602.22779)
*Chenhao Zheng,Jieyu Zhang,Jianing Zhang,Weikai Huang,Ashutosh Kumar,Quan Kong,Oncel Tuzel,Chun-Liang Li,Ranjay Krishna*

Main category: cs.CV

TL;DR: TrajTok是一个端到端的视频分词器模块，通过统一的片段分割器在时空维度上隐式聚类像素，直接生成物体轨迹，实现与视频时长无关的动态分词粒度，提升视频理解效率与性能。


<details>
  <summary>Details</summary>
Motivation: 传统视频模型通过分块化进行分词会产生大量冗余token，严重限制视频处理效率和可扩展性。现有的轨迹分词器虽然能解耦视频时长与token数量，但依赖复杂的外部分割和跟踪流程，速度慢且与任务无关。

Method: 提出TrajTok端到端视频分词器模块，包含统一的片段分割器，在时空维度上对像素进行隐式聚类，直接生成物体轨迹。该模块与视频模型完全集成并联合训练，根据语义复杂度动态调整分词粒度。

Result: TrajTok实现了视频CLIP模型(TrajViT2)，在分类和检索基准测试中达到最佳准确率，同时保持与最佳token合并方法相当的效率。TrajTok还可作为预训练视觉特征的探测头(TrajAdapter)或视觉语言模型的对齐连接器(TrajVLM)，在长视频推理中表现优异。

Conclusion: TrajTok是一个轻量高效的端到端视频分词器，通过动态适应语义复杂度的轨迹生成，解决了视频分词中的冗余问题，显著提升了视频理解性能，并展现出作为多功能组件的潜力。

Abstract: Tokenization in video models, typically through patchification, generates an excessive and redundant number of tokens. This severely limits video efficiency and scalability. While recent trajectory-based tokenizers offer a promising solution by decoupling video duration from token count, they rely on complex external segmentation and tracking pipelines that are slow and task-agnostic. We propose TrajTok, an end-to-end video tokenizer module that is fully integrated and co-trained with video models for a downstream objective, dynamically adapting its token granularity to semantic complexity, independent of video duration. TrajTok contains a unified segmenter that performs implicit clustering over pixels in both space and time to directly produce object trajectories in a single forward pass. By prioritizing downstream adaptability over pixel-perfect segmentation fidelity, TrajTok is lightweight and efficient, yet empirically improves video understanding performance. With TrajTok, we implement a video CLIP model trained from scratch (TrajViT2). It achieves the best accuracy at scale across both classification and retrieval benchmarks, while maintaining efficiency comparable to the best token-merging methods. TrajTok also proves to be a versatile component beyond its role as a tokenizer. We show that it can be seamlessly integrated as either a probing head for pretrained visual features (TrajAdapter) or an alignment connector in vision-language models (TrajVLM) with especially strong performance in long-video reasoning.

</details>


### [11] [SceneTransporter: Optimal Transport-Guided Compositional Latent Diffusion for Single-Image Structured 3D Scene Generation](https://arxiv.org/abs/2602.22785)
*Ling Wang,Hao-Xiang Guo,Xinzhou Wang,Fuchun Sun,Kai Sun,Pengkun Liu,Hang Xiao,Zhong Wang,Guangyuan Fu,Eric Li,Yang Liu,Yikai Wang*

Main category: cs.CV

TL;DR: SceneTransporter：从单图像生成结构化3D场景的端到端框架，通过最优传输解决实例分割问题


<details>
  <summary>Details</summary>
Motivation: 现有方法生成部件级3D对象，但无法在开放世界场景中将部件组织成不同的实例。研究发现这种失败源于模型内部分配机制缺乏结构约束。

Method: 将结构化3D场景生成重新定义为全局相关分配问题，在组合DiT模型的去噪循环中制定并求解熵最优传输目标，施加两个结构约束：传输计划门控交叉注意力实现图像块到3D潜在表示的一对一独占路由；基于边缘的成本正则化竞争性传输以形成连贯对象。

Result: 在开放世界场景生成任务上超越现有方法，显著提升实例级连贯性和几何保真度。

Conclusion: 通过最优传输引入结构约束，有效解决了3D场景生成中的实例分割问题，为结构化场景生成提供了新思路。

Abstract: We introduce SceneTransporter, an end-to-end framework for structured 3D scene generation from a single image. While existing methods generate part-level 3D objects, they often fail to organize these parts into distinct instances in open-world scenes. Through a debiased clustering probe, we reveal a critical insight: this failure stems from the lack of structural constraints within the model's internal assignment mechanism. Based on this finding, we reframe the task of structured 3D scene generation as a global correlation assignment problem. To solve this, SceneTransporter formulates and solves an entropic Optimal Transport (OT) objective within the denoising loop of the compositional DiT model. This formulation imposes two powerful structural constraints. First, the resulting transport plan gates cross-attention to enforce an exclusive, one-to-one routing of image patches to part-level 3D latents, preventing entanglement. Second, the competitive nature of the transport encourages the grouping of similar patches, a process that is further regularized by an edge-based cost, to form coherent objects and prevent fragmentation. Extensive experiments show that SceneTransporter outperforms existing methods on open-world scene generation, significantly improving instance-level coherence and geometric fidelity. Code and models will be publicly available at https://2019epwl.github.io/SceneTransporter/.

</details>


### [12] [Robust Human Trajectory Prediction via Self-Supervised Skeleton Representation Learning](https://arxiv.org/abs/2602.22791)
*Taishu Arashima,Hiroshi Kera,Kazuhiko Kawamoto*

Main category: cs.CV

TL;DR: 提出一种结合自监督骨架表示学习的鲁棒轨迹预测方法，通过掩码自编码预训练处理遮挡导致的关节缺失问题


<details>
  <summary>Details</summary>
Motivation: 现实环境中的人体骨架数据常因遮挡导致关节缺失，这会显著降低轨迹预测精度，需要更鲁棒的骨架表示方法

Method: 采用自监督骨架表示模型，通过掩码自编码预训练，然后整合到轨迹预测框架中

Result: 在遮挡场景下，该方法提高了对缺失骨架数据的鲁棒性，同时不牺牲预测精度，在清洁到中等缺失程度下持续优于基线模型

Conclusion: 自监督骨架表示学习能有效提升轨迹预测在遮挡环境下的鲁棒性，为实际应用提供了更可靠的解决方案

Abstract: Human trajectory prediction plays a crucial role in applications such as autonomous navigation and video surveillance. While recent works have explored the integration of human skeleton sequences to complement trajectory information, skeleton data in real-world environments often suffer from missing joints caused by occlusions. These disturbances significantly degrade prediction accuracy, indicating the need for more robust skeleton representations. We propose a robust trajectory prediction method that incorporates a self-supervised skeleton representation model pretrained with masked autoencoding. Experimental results in occlusion-prone scenarios show that our method improves robustness to missing skeletal data without sacrificing prediction accuracy, and consistently outperforms baseline models in clean-to-moderate missingness regimes.

</details>


### [13] [GSTurb: Gaussian Splatting for Atmospheric Turbulence Mitigation](https://arxiv.org/abs/2602.22800)
*Hanliang Du,Zhangji Lu,Zewei Cai,Qijian Tang,Qifeng Yu,Xiaoli Liu*

Main category: cs.CV

TL;DR: 提出GSTurb框架，结合光流引导的倾斜校正和高斯泼溅建模非等晕模糊，用于大气湍流图像恢复，在合成和真实数据集上均优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 大气湍流导致长距离成像中的像素位移（倾斜）和模糊，严重影响图像质量，需要有效的恢复方法。

Method: GSTurb框架整合光流引导的倾斜校正和高斯泼溅建模非等晕模糊，使用高斯参数表示倾斜和模糊，并通过多帧优化进行恢复。

Result: 在ATSyn-static数据集上达到PSNR 27.67 dB和SSIM 0.8735，相比SOTA方法提升1.3 dB PSNR（4.5%）和0.048 SSIM（5.8%），在真实数据集上也表现优异。

Conclusion: 光流引导的倾斜校正与高斯泼溅结合能有效提升大气湍流条件下的图像恢复效果，在合成和真实场景中均表现出色。

Abstract: Atmospheric turbulence causes significant image degradation due to pixel displacement (tilt) and blur, particularly in long-range imaging applications. In this paper, we propose a novel framework for atmospheric turbulence mitigation, GSTurb, which integrates optical flow-guided tilt correction and Gaussian splatting for modeling non-isoplanatic blur. The framework employs Gaussian parameters to represent tilt and blur, and optimizes them across multiple frames to enhance restoration. Experimental results on the ATSyn-static dataset demonstrate the effectiveness of our method, achieving a peak PSNR of 27.67 dB and SSIM of 0.8735. Compared to the state-of-the-art method, GSTurb improves PSNR by 1.3 dB (a 4.5% increase) and SSIM by 0.048 (a 5.8% increase). Additionally, on real datasets, including the TSRWGAN Real-World and CLEAR datasets, GSTurb outperforms existing methods, showing significant improvements in both qualitative and quantitative performance. These results highlight that combining optical flow-guided tilt correction with Gaussian splatting effectively enhances image restoration under both synthetic and real-world turbulence conditions. The code for this method will be available at https://github.com/DuhlLiamz/3DGS_turbulence/tree/main.

</details>


### [14] [PhotoAgent: Agentic Photo Editing with Exploratory Visual Aesthetic Planning](https://arxiv.org/abs/2602.22809)
*Mingde Yao,Zhiyuan You,Tam-King Man,Menglu Wang,Tianfan Xue*

Main category: cs.CV

TL;DR: PhotoAgent是一个通过显式美学规划实现自主图像编辑的系统，将编辑任务建模为长时程决策问题，通过树搜索规划多步编辑动作，无需用户逐步提示。


<details>
  <summary>Details</summary>
Motivation: 当前基于指令的图像编辑高度依赖精心设计的指令，将任务分解和顺序规划的负担完全放在用户身上。为了实现自主图像编辑，需要系统能够理解用户美学意图并自动规划编辑步骤。

Method: PhotoAgent将自主图像编辑建模为长时程决策问题，通过树搜索规划多步编辑动作，利用记忆和视觉反馈进行闭环执行迭代优化。系统包含理解用户美学意图、规划编辑步骤和执行编辑三个核心模块。

Result: 实验表明，PhotoAgent在指令遵循和视觉质量方面均优于基线方法。作者还构建了UGC-Edit美学评估基准（包含7,000张照片）和包含1,017张照片的测试集来系统评估自主照片编辑性能。

Conclusion: PhotoAgent通过显式美学规划和闭环执行实现了高质量的自主图像编辑，减轻了用户手动规划编辑步骤的负担，在真实场景中表现出优越的性能。

Abstract: With the recent fast development of generative models, instruction-based image editing has shown great potential in generating high-quality images. However, the quality of editing highly depends on carefully designed instructions, placing the burden of task decomposition and sequencing entirely on the user. To achieve autonomous image editing, we present PhotoAgent, a system that advances image editing through explicit aesthetic planning. Specifically, PhotoAgent formulates autonomous image editing as a long-horizon decision-making problem. It reasons over user aesthetic intent, plans multi-step editing actions via tree search, and iteratively refines results through closed-loop execution with memory and visual feedback, without requiring step-by-step user prompts. To support reliable evaluation in real-world scenarios, we introduce UGC-Edit, an aesthetic evaluation benchmark consisting of 7,000 photos and a learned aesthetic reward model. We also construct a test set containing 1,017 photos to systematically assess autonomous photo editing performance. Extensive experiments demonstrate that PhotoAgent consistently improves both instruction adherence and visual quality compared with baseline methods. The project page is https://github.com/mdyao/PhotoAgent.

</details>


### [15] [Face Time Traveller : Travel Through Ages Without Losing Identity](https://arxiv.org/abs/2602.22819)
*Purbayan Kar,Ayush Ghadiya,Vishal Chudasama,Pankaj Wasnik,C. V. Jawahar*

Main category: cs.CV

TL;DR: FaceTT是一个基于扩散模型的框架，通过面部属性感知提示细化、免调谐角度反演和自适应注意力控制，实现高保真、身份一致的面部年龄变换。


<details>
  <summary>Details</summary>
Motivation: 现有面部年龄变换方法依赖数值年龄表示，忽略了生物和环境因素的相互作用，在宽年龄变换中难以保持身份一致性，且存在静态注意力、优化繁重的反演等问题，限制了适应性、细粒度控制和背景一致性。

Method: 1) 面部属性感知提示细化策略：编码内在（生物）和外在（环境）老化线索进行上下文感知条件化；2) 免调谐角度反演方法：高效将真实面部映射到扩散潜在空间以实现快速准确重建；3) 自适应注意力控制机制：动态平衡交叉注意力（语义老化线索）和自注意力（结构和身份保持）。

Result: 在基准数据集和野外测试集上的广泛实验表明，FaceTT在身份保持、背景保留和老化真实性方面优于最先进方法。

Conclusion: FaceTT通过创新的提示细化、反演和注意力控制机制，成功解决了面部年龄变换中的身份保持、背景一致性和老化真实性问题，实现了高质量的面部年龄变换。

Abstract: Face aging, an ill-posed problem shaped by environmental and genetic factors, is vital in entertainment, forensics, and digital archiving, where realistic age transformations must preserve both identity and visual realism. However, existing works relying on numerical age representations overlook the interplay of biological and contextual cues. Despite progress in recent face aging models, they struggle with identity preservation in wide age transformations, also static attention and optimization-heavy inversion in diffusion limit adaptability, fine-grained control and background consistency. To address these challenges, we propose Face Time Traveller (FaceTT), a diffusion-based framework that achieves high-fidelity, identity-consistent age transformation. Here, we introduce a Face-Attribute-Aware Prompt Refinement strategy that encodes intrinsic (biological) and extrinsic (environmental) aging cues for context-aware conditioning. A tuning-free Angular Inversion method is proposed that efficiently maps real faces into the diffusion latent space for fast and accurate reconstruction. Moreover, an Adaptive Attention Control mechanism is introduced that dynamically balances cross-attention for semantic aging cues and self-attention for structural and identity preservation. Extensive experiments on benchmark datasets and in-the-wild testset demonstrate that FaceTT achieves superior identity retention, background preservation and aging realism over state-of-the-art (SOTA) methods.

</details>


### [16] [CMSA-Net: Causal Multi-scale Aggregation with Adaptive Multi-source Reference for Video Polyp Segmentation](https://arxiv.org/abs/2602.22821)
*Tong Wang,Yaolei Qi,Siwen Wang,Imran Razzak,Guanyu Yang,Yutong Xie*

Main category: cs.CV

TL;DR: CMSA-Net是一个用于视频息肉分割的框架，通过因果多尺度聚合模块和动态多源参考策略，在保持实时性的同时提高了分割准确性。


<details>
  <summary>Details</summary>
Motivation: 视频息肉分割在计算机辅助结肠镜检查中很重要，但面临两个主要挑战：1）息肉与周围黏膜外观相似，语义区分度弱；2）视频帧间息肉位置和尺度变化大，导致分割不稳定和不准确。

Method: 提出了CMSA-Net框架，包含两个核心组件：1）因果多尺度聚合模块，使用因果注意力从多个历史帧的不同尺度有效收集语义信息，确保时间特征传播遵循严格时间顺序；2）动态多源参考策略，基于语义可分性和预测置信度自适应选择信息丰富且可靠的参考帧，提供多帧指导同时保持实时推理效率。

Result: 在SUN-SEG数据集上的大量实验表明，CMSA-Net实现了最先进的性能，在分割准确性和实时临床适用性之间取得了良好平衡。

Conclusion: CMSA-Net通过因果多尺度特征聚合和自适应参考帧选择，有效解决了视频息肉分割中的语义区分弱和时间稳定性问题，为临床实时应用提供了实用解决方案。

Abstract: Video polyp segmentation (VPS) is an important task in computer-aided colonoscopy, as it helps doctors accurately locate and track polyps during examinations. However, VPS remains challenging because polyps often look similar to surrounding mucosa, leading to weak semantic discrimination. In addition, large changes in polyp position and scale across video frames make stable and accurate segmentation difficult. To address these challenges, we propose a robust VPS framework named CMSA-Net. The proposed network introduces a Causal Multi-scale Aggregation (CMA) module to effectively gather semantic information from multiple historical frames at different scales. By using causal attention, CMA ensures that temporal feature propagation follows strict time order, which helps reduce noise and improve feature reliability. Furthermore, we design a Dynamic Multi-source Reference (DMR) strategy that adaptively selects informative and reliable reference frames based on semantic separability and prediction confidence. This strategy provides strong multi-frame guidance while keeping the model efficient for real-time inference. Extensive experiments on the SUN-SEG dataset demonstrate that CMSA-Net achieves state-of-the-art performance, offering a favorable balance between segmentation accuracy and real-time clinical applicability.

</details>


### [17] [Reflectance Multispectral Imaging for Soil Composition Estimation and USDA Texture Classification](https://arxiv.org/abs/2602.22829)
*G. A. S. L Ranasinghe,J. A. S. T. Jayakody,M. C. L. De Silva,G. Thilakarathne,G. M. R. I. Godaliyadda,H. M. V. R. Herath,M. P. B. Ekanayake,S. K. Navaratnarajah*

Main category: cs.CV

TL;DR: 提出一种基于多光谱成像和机器学习的低成本、可现场部署的土壤质地预测系统，能够准确预测土壤成分和USDA质地分类


<details>
  <summary>Details</summary>
Motivation: 传统土壤质地分析方法（实验室粒度测试）缓慢且劳动密集，现有传感替代方案要么成本高昂，要么分辨率不足以支持常规田间部署。需要一种准确、非破坏性、可现场部署的土壤质地表征方法，适用于岩土工程筛选和精准农业

Method: 开发低成本自制多光谱成像设备（365-940 nm，13个光谱波段），结合机器学习框架：回归模型预测粘土、粉砂、砂土百分比，直接分类器预测12个USDA质地类别，间接分类通过USDA土壤质地三角图将回归成分映射到质地类别

Result: 成分预测的确定系数R²高达0.99，质地分类准确率超过99%。表明多光谱成像结合数据驱动建模能够提供准确、非破坏性、可现场部署的土壤质地表征

Conclusion: 多光谱成像与机器学习相结合的方法能够实现准确、快速、低成本的土壤质地分析，适合岩土工程筛选和精准农业应用，为传统实验室方法提供了有前景的替代方案

Abstract: Soil texture is a foundational attribute that governs water availability and erosion in agriculture, as well as load bearing capacity, deformation response, and shrink-swell risk in geotechnical engineering. Yet texture is still typically determined by slow and labour intensive laboratory particle size tests, while many sensing alternatives are either costly or too coarse to support routine field scale deployment. This paper proposes a robust and field deployable multispectral imaging (MSI) system and machine learning framework for predicting soil composition and the United States Department of Agriculture (USDA) texture classes. The proposed system uses a cost effective in-house MSI device operating from 365 nm to 940 nm to capture thirteen spectral bands, which effectively capture the spectral properties of soil texture. Regression models use the captured spectral properties to estimate clay, silt, and sand percentages, while a direct classifier predicts one of the twelve USDA textural classes. Indirect classification is obtained by mapping the regressed compositions to texture classes via the USDA soil texture triangle. The framework is evaluated on mixture data by mixing clay, silt, and sand in varying proportions, using the USDA classification triangle as a basis. Experimental results show that the proposed approach achieves a coefficient of determination R^2 up to 0.99 for composition prediction and over 99% accuracy for texture classification. These findings indicate that MSI combined with data-driven modeling can provide accurate, non-destructive, and field deployable soil texture characterization suitable for geotechnical screening and precision agriculture.

</details>


### [18] [A data- and compute-efficient chest X-ray foundation model beyond aggressive scaling](https://arxiv.org/abs/2602.22843)
*Chong Wang,Yabin Zhang,Yunhe Gao,Maya Varma,Clemence Mottez,Faidra Patsatzi,Jiaming Liu,Jin Long,Jean-Benoit Delbrouck,Sergios Gatidis,Akshay S. Chaudhari,Curtis P. Langlotz*

Main category: cs.CV

TL;DR: CheXficient是一种通过主动数据筛选而非盲目扩大数据集训练的医学影像基础模型，仅用22.7%的数据和27.3%的计算资源就达到了与全数据训练相当或更优的性能。


<details>
  <summary>Details</summary>
Motivation: 当前医学影像基础模型遵循"不惜一切代价扩大规模"的范式，面临两大挑战：大规模医学数据集存在大量冗余和严重类别不平衡，导致表示学习偏向过代表模式；不考虑数据质量异质性的盲目训练导致计算效率低下。

Method: 提出CheXficient胸部X光基础模型，采用主动、有原则的数据筛选策略，在预训练过程中选择性优先处理信息丰富的训练样本。模型仅使用1,235,004对CXR图像和报告中22.7%的数据进行预训练。

Result: CheXficient在仅消耗27.3%总计算预算的情况下，在20个涵盖5类任务的基准测试中，性能达到或优于全数据训练的对应模型及其他大规模预训练模型。特别在长尾或罕见疾病上表现出更好的泛化能力。

Conclusion: 主动数据筛选是医学视觉语言基础模型高效预训练和下游适应的可行且经济有效的替代方案，为医学影像基础模型的数据和计算需求提供了实用见解。

Abstract: Foundation models for medical imaging are typically pretrained on increasingly large datasets, following a "scale-at-all-costs" paradigm. However, this strategy faces two critical challenges: large-scale medical datasets often contain substantial redundancy and severe class imbalance that bias representation learning toward over-represented patterns, and indiscriminate training regardless of heterogeneity in data quality incurs considerable computational inefficiency. Here we demonstrate that active, principled data curation during pretraining can serve as a viable, cost-effective alternative to brute-force dataset enlargement. We introduce CheXficient, a chest X-ray (CXR) foundation model that selectively prioritizes informative training samples. CheXficient is pretrained on only 22.7% of 1,235,004 paired CXR images and reports while consuming under 27.3% of the total compute budget, yet achieving comparable or superior performance to its full-data counterpart and other large-scale pretrained models. We assess CheXficient across 20 individual benchmarks spanning 5 task types, including non-adapted off-the-shelf evaluations (zero-shot findings classification and crossmodal retrieval) and adapted downstream tasks (disease prediction, semantic segmentation, and radiology report generation). Further analyses show that CheXficient systematically prioritizes under-represented training samples, improving generalizability on long-tailed or rare conditions. Overall, our work offers practical insights into the data and computation demands for efficient pretraining and downstream adaptation of medical vision-language foundation models.

</details>


### [19] [SO3UFormer: Learning Intrinsic Spherical Features for Rotation-Robust Panoramic Segmentation](https://arxiv.org/abs/2602.22867)
*Qinfeng Zhu,Yunxi Jiang,Lei Fan*

Main category: cs.CV

TL;DR: SO3UFormer：一种旋转鲁棒的全景语义分割架构，通过解耦重力向量依赖、球面注意力机制和规范感知位置编码，在任意3D旋转下保持性能稳定。


<details>
  <summary>Details</summary>
Motivation: 现有全景语义分割模型严重依赖重力对齐假设，但实际应用中相机姿态存在旋转抖动（如手持设备、无人机平台），导致标准球面Transformer过度依赖全局纬度线索，在3D重定向时性能崩溃。

Method: 提出SO3UFormer架构：1）内在特征表示，移除绝对纬度编码以解耦重力向量依赖；2）正交一致的球面注意力，考虑非均匀采样密度；3）规范感知相对位置机制，使用切平面投影角度和离散规范池化编码局部角度几何；4）训练时采用基于索引的球面重采样和logit级SO(3)一致性正则化。

Result: 在Pose35数据集（±35°随机旋转）上达到72.03 mIoU，在任意全SO(3)旋转下保持70.67 mIoU，而基准模型SphereUFormer从67.53 mIoU暴跌至25.26 mIoU，显示显著稳定性优势。

Conclusion: SO3UFormer通过几何感知设计实现了对3D旋转的鲁棒性，解决了全景语义分割在实际应用中因相机姿态变化导致的性能退化问题，为无约束环境下的视觉感知提供了可靠解决方案。

Abstract: Panoramic semantic segmentation models are typically trained under a strict gravity-aligned assumption. However, real-world captures often deviate from this canonical orientation due to unconstrained camera motions, such as the rotational jitter of handheld devices or the dynamic attitude shifts of aerial platforms. This discrepancy causes standard spherical Transformers to overfit global latitude cues, leading to performance collapse under 3D reorientations. To address this, we introduce SO3UFormer, a rotation-robust architecture designed to learn intrinsic spherical features that are less sensitive to the underlying coordinate frame. Our approach rests on three geometric pillars: (1) an intrinsic feature formulation that decouples the representation from the gravity vector by removing absolute latitude encoding; (2) quadrature-consistent spherical attention that accounts for non-uniform sampling densities; and (3) a gauge-aware relative positional mechanism that encodes local angular geometry using tangent-plane projected angles and discrete gauge pooling, avoiding reliance on global axes. We further use index-based spherical resampling together with a logit-level SO(3)-consistency regularizer during training. To rigorously benchmark robustness, we introduce Pose35, a dataset variant of Stanford2D3D perturbed by random rotations within $\pm 35^\circ$. Under the extreme test of arbitrary full SO(3) rotations, existing SOTAs fail catastrophically: the baseline SphereUFormer drops from 67.53 mIoU to 25.26 mIoU. In contrast, SO3UFormer demonstrates remarkable stability, achieving 72.03 mIoU on Pose35 and retaining 70.67 mIoU under full SO(3) rotations.

</details>


### [20] [Towards Multimodal Domain Generalization with Few Labels](https://arxiv.org/abs/2602.22917)
*Hongzhao Li,Hao Dong,Hualei Wan,Shupan Li,Mingliang Xu,Muhammad Haris Khan*

Main category: cs.CV

TL;DR: 提出半监督多模态域泛化（SSMDG）新问题，设计统一框架解决多源数据中标签稀缺和域偏移的挑战，并在新基准上取得优异性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法存在局限性：多模态域泛化方法无法利用未标注数据，半监督多模态学习方法忽略域偏移，而半监督域泛化方法仅限于单模态输入。需要同时解决域泛化、多模态融合和半监督学习三个挑战。

Method: 提出统一框架包含三个关键组件：1）共识驱动的连续性正则化，通过置信融合-单模态共识获得可靠伪标签；2）分歧感知正则化，有效利用模糊的非共识样本；3）跨模态原型对齐，强制域和模态不变表示，并通过跨模态翻译在缺失模态情况下保持鲁棒性。

Result: 建立了首个SSMDG基准测试，提出的方法在标准和缺失模态场景下均持续优于强基线方法。

Conclusion: SSMDG是一个重要且具有挑战性的问题，提出的统一框架能有效解决多模态域泛化中的半监督学习问题，为实际应用中减少标注成本同时保持模型泛化能力提供了解决方案。

Abstract: Multimodal models ideally should generalize to unseen domains while remaining data-efficient to reduce annotation costs. To this end, we introduce and study a new problem, Semi-Supervised Multimodal Domain Generalization (SSMDG), which aims to learn robust multimodal models from multi-source data with few labeled samples. We observe that existing approaches fail to address this setting effectively: multimodal domain generalization methods cannot exploit unlabeled data, semi-supervised multimodal learning methods ignore domain shifts, and semi-supervised domain generalization methods are confined to single-modality inputs. To overcome these limitations, we propose a unified framework featuring three key components: Consensus-Driven Consistency Regularization, which obtains reliable pseudo-labels through confident fused-unimodal consensus; Disagreement-Aware Regularization, which effectively utilizes ambiguous non-consensus samples; and Cross-Modal Prototype Alignment, which enforces domain- and modality-invariant representations while promoting robustness under missing modalities via cross-modal translation. We further establish the first SSMDG benchmarks, on which our method consistently outperforms strong baselines in both standard and missing-modality scenarios. Our benchmarks and code are available at https://github.com/lihongzhao99/SSMDG.

</details>


### [21] [MSJoE: Jointly Evolving MLLM and Sampler for Efficient Long-Form Video Understanding](https://arxiv.org/abs/2602.22932)
*Wenhui Tan,Xiaoyi Yu,Jiaze Li,Yijing Chen,Jianzhong Ju,Zhenbo Luo,Ruihua Song,Jian Luan*

Main category: cs.CV

TL;DR: MSJoE框架通过联合进化MLLM和轻量级关键帧采样器，实现高效长视频理解，仅选择少量信息丰富的关键帧进行问答，在多个基准测试中显著提升准确率。


<details>
  <summary>Details</summary>
Motivation: 当前多模态大语言模型在理解长视频方面面临效率挑战，需要处理大量冗余帧。本文基于一个关键假设：对于每个视频问题，只有少量关键帧真正具有信息价值，因此需要开发能够智能选择关键帧的高效框架。

Method: 提出MSJoE框架：1) MLLM推理出多个描述问题相关视觉视角的查询；2) 查询与冻结的CLIP模型交互生成查询-帧相似度矩阵；3) 轻量级采样器从矩阵预测关键帧采样权重，选择紧凑的信息丰富帧集；4) 通过强化学习联合优化MLLM和采样器，实现查询推理、帧采样和关键帧理解的协同适应。

Result: 在VideoMME、LongVideoBench、LVBench和MLVU等基准测试中，MSJoE相比基础MLLM实现了8.0%的准确率提升，比最强基线方法高出1.1%准确率。同时收集了包含2.8K视频和7K问答对的新长视频QA数据集支持训练。

Conclusion: MSJoE通过联合进化MLLM和关键帧采样器，有效解决了长视频理解中的效率问题，证明了仅选择少量信息丰富关键帧的假设的可行性，并在多个基准测试中取得了显著性能提升。

Abstract: Efficiently understanding long-form videos remains a fundamental challenge for multimodal large language models (MLLMs). In this paper, we present MLLM-Sampler Joint Evolution (MSJoE), a novel framework that jointly evolves the MLLM and a lightweight key-frame sampler for efficient long-form video understanding. MSJoE builds upon a key assumption that only a small subset of key-frames is truly informative for answering each question to a video. Specifically, MSJoE first reasons out several queries, which describe diverse visual perspectives relevant to the question. Then, these queries interact with a frozen CLIP model to produce a query-frame similarity matrix. Finally, a lightweight sampler predicts key-frame sampling weights from this matrix, selecting a compact set of informative frames, which are then fed into the MLLM for answer generation. Both the MLLM and sampler are jointly optimized through reinforcement learning, enabling co-adaptation of query-reasoning, frame-sampling, and key-frame understanding. A new long-video QA dataset containing 2.8K videos with 7K question-answer pairs is collected to support the training process. Extensive experiments on VideoMME, LongVideoBench, LVBench, and MLVU show that MSJoE achieves 8.0\% accuracy gain upon the base MLLM, and 1.1\% higher accuracy than strongest baseline method.

</details>


### [22] [SubspaceAD: Training-Free Few-Shot Anomaly Detection via Subspace Modeling](https://arxiv.org/abs/2602.23013)
*Camile Lendering,Erkut Akdag,Egor Bondarev*

Main category: cs.CV

TL;DR: SubspaceAD是一种无需训练的单样本/少样本工业异常检测方法，仅使用冻结的DINOv2特征和PCA建模正常图像子空间，通过重构残差检测异常。


<details>
  <summary>Details</summary>
Motivation: 当前少样本异常检测方法通常依赖内存库、辅助数据集或多模态调优，作者质疑这种复杂性对于视觉基础模型的特征表示是否必要。

Method: 两阶段方法：1) 使用冻结的DINOv2提取少量正常图像的patch级特征；2) 对特征进行PCA分析，估计正常变化的低维子空间。推理时通过重构残差计算异常分数。

Result: 在单样本异常检测设置下，MVTec-AD数据集上图像级和像素级AUROC分别达到98.0%和97.6%，VisA数据集上达到93.3%和98.3%，均超越先前SOTA结果。

Conclusion: SubspaceAD证明了无需训练、提示调优或内存库的简单方法也能在少样本异常检测中达到SOTA性能，挑战了现有方法的复杂性需求。

Abstract: Detecting visual anomalies in industrial inspection often requires training with only a few normal images per category. Recent few-shot methods achieve strong results employing foundation-model features, but typically rely on memory banks, auxiliary datasets, or multi-modal tuning of vision-language models. We therefore question whether such complexity is necessary given the feature representations of vision foundation models. To answer this question, we introduce SubspaceAD, a training-free method, that operates in two simple stages. First, patch-level features are extracted from a small set of normal images by a frozen DINOv2 backbone. Second, a Principal Component Analysis (PCA) model is fit to these features to estimate the low-dimensional subspace of normal variations. At inference, anomalies are detected via the reconstruction residual with respect to this subspace, producing interpretable and statistically grounded anomaly scores. Despite its simplicity, SubspaceAD achieves state-of-the-art performance across one-shot and few-shot settings without training, prompt tuning, or memory banks. In the one-shot anomaly detection setting, SubspaceAD achieves image-level and pixel-level AUROC of 98.0% and 97.6% on the MVTec-AD dataset, and 93.3% and 98.3% on the VisA dataset, respectively, surpassing prior state-of-the-art results. Code and demo are available at https://github.com/CLendering/SubspaceAD.

</details>


### [23] [SpectralMamba-UNet: Frequency-Disentangled State Space Modeling for Texture-Structure Consistent Medical Image Segmentation](https://arxiv.org/abs/2602.23103)
*Fuhao Zhang,Lei Liu,Jialin Zhang,Ya-Nan Zhang,Nan Mu*

Main category: cs.CV

TL;DR: SpectralMamba-UNet：一种在频域解耦结构和纹理信息的新型医学图像分割框架，通过频域Mamba建模全局上下文，高频特征保留边界细节，在多个基准测试中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有状态空间模型（如Vision Mamba）虽然能有效建模长程依赖，但其一维序列化会削弱局部空间连续性和高频表示能力，而医学图像分割需要同时建模全局解剖结构和细粒度边界细节。

Method: 提出SpectralMamba-UNet框架：1）谱分解与建模模块（SDM）使用离散余弦变换分解低频和高频特征，低频通过频域Mamba建模全局上下文，高频保留边界敏感细节；2）谱通道重加权机制（SCR）形成通道级频率感知注意力；3）谱引导融合模块（SGF）在解码器中实现自适应多尺度融合。

Result: 在五个公开基准测试上进行实验，涵盖多种模态和分割目标，均取得一致性的性能提升，验证了方法的有效性和泛化能力。

Conclusion: SpectralMamba-UNet通过在频域解耦结构和纹理信息的学习，有效解决了现有状态空间模型在医学图像分割中的局限性，实现了全局上下文建模和边界细节保留的良好平衡。

Abstract: Accurate medical image segmentation requires effective modeling of both global anatomical structures and fine-grained boundary details. Recent state space models (e.g., Vision Mamba) offer efficient long-range dependency modeling. However, their one-dimensional serialization weakens local spatial continuity and high-frequency representation. To this end, we propose SpectralMamba-UNet, a novel frequency-disentangled framework to decouple the learning of structural and textural information in the spectral domain. Our Spectral Decomposition and Modeling (SDM) module applies discrete cosine transform to decompose low- and high-frequency features, where low frequency contributes to global contextual modeling via a frequency-domain Mamba and high frequency preserves boundary-sensitive details. To balance spectral contributions, we introduce a Spectral Channel Reweighting (SCR) mechanism to form channel-wise frequency-aware attention, and a Spectral-Guided Fusion (SGF) module to achieve adaptively multi-scale fusion in the decoder. Experiments on five public benchmarks demonstrate consistent improvements across diverse modalities and segmentation targets, validating the effectiveness and generalizability of our approach.

</details>


### [24] [WARM-CAT: : Warm-Started Test-Time Comprehensive Knowledge Accumulation for Compositional Zero-Shot Learning](https://arxiv.org/abs/2602.23114)
*Xudong Yan,Songhe Feng,Jiaxin Wang,Xin Su,Yi Jin*

Main category: cs.CV

TL;DR: 提出WARM-CAT方法，通过累积文本和视觉模态的无监督知识来更新多模态原型，解决组合零样本学习中的分布偏移问题


<details>
  <summary>Details</summary>
Motivation: 现有组合零样本学习方法在测试时因包含未见属性-对象组合而导致标签空间分布偏移，造成性能下降

Method: 1) 从无监督数据累积多模态知识更新原型；2) 自适应更新权重控制原型调整程度；3) 动态优先级队列存储高置信度图像；4) 多模态协同表示学习对齐文本和视觉原型；5) 提出新基准数据集C-Fashion并改进MIT-States

Result: 在四个基准数据集上，在封闭世界和开放世界设置下均达到最先进性能

Conclusion: WARM-CAT通过累积多模态知识、自适应原型更新和动态队列机制，有效解决了组合零样本学习中的分布偏移问题

Abstract: Compositional Zero-Shot Learning (CZSL) aims to recognize novel attribute-object compositions based on the knowledge learned from seen ones. Existing methods suffer from performance degradation caused by the distribution shift of label space at test time, which stems from the inclusion of unseen compositions recombined from attributes and objects. To overcome the challenge, we propose a novel approach that accumulates comprehensive knowledge in both textual and visual modalities from unsupervised data to update multimodal prototypes at test time. Building on this, we further design an adaptive update weight to control the degree of prototype adjustment, enabling the model to flexibly adapt to distribution shift during testing. Moreover, a dynamic priority queue is introduced that stores high-confidence images to acquire visual prototypes from historical images for inference. Since the model tends to favor compositions already stored in the queue during testing, we warm-start the queue by initializing it with training images for visual prototypes of seen compositions and generating unseen visual prototypes using the mapping learned between seen and unseen textual prototypes. Considering the semantic consistency of multimodal knowledge, we align textual and visual prototypes by multimodal collaborative representation learning. To provide a more reliable evaluation for CZSL, we introduce a new benchmark dataset, C-Fashion, and refine the widely used but noisy MIT-States dataset. Extensive experiments indicate that our approach achieves state-of-the-art performance on four benchmark datasets under both closed-world and open-world settings. The source code and datasets are available at https://github.com/xud-yan/WARM-CAT .

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [25] [Toward Automatic Filling of Case Report Forms: A Case Study on Data from an Italian Emergency Department](https://arxiv.org/abs/2602.23062)
*Gabriela Anna Kaczmarek,Pietro Ferrazzi,Lorenzo Porta,Vicky Rubini,Bernardo Magnini*

Main category: cs.CL

TL;DR: 本文提出一个意大利急诊科临床笔记数据集，用于自动填写病例报告表（CRF），并展示了在零样本设置下使用开源LLM进行CRF填写的初步实验结果。


<details>
  <summary>Details</summary>
Motivation: 临床研究中CRF数据收集至关重要，但缺乏标注数据限制了基于LLM的自动CRF填写技术的发展。需要更多标注数据来训练和测试LLM。

Method: 创建意大利急诊科临床笔记数据集，包含134个CRF项目的标注。定义CRF填写任务和评估指标，使用开源最先进LLM在零样本设置下进行实验。

Result: 实验表明：(1) 意大利语临床笔记的CRF填写可以在零样本设置下进行；(2) LLM结果存在偏差（如倾向于"未知"答案），需要修正。

Conclusion: 该研究为CRF填写任务提供了有价值的标注数据集，展示了LLM在该任务上的潜力，同时指出了需要解决的偏差问题。

Abstract: Case Report Forms (CRFs) collect data about patients and are at the core of well-established practices to conduct research in clinical settings. With the recent progress of language technologies, there is an increasing interest in automatic CRF-filling from clinical notes, mostly based on the use of Large Language Models (LLMs). However, there is a general scarcity of annotated CRF data, both for training and testing LLMs, which limits the progress on this task. As a step in the direction of providing such data, we present a new dataset of clinical notes from an Italian Emergency Department annotated with respect to a pre-defined CRF containing 134 items to be filled. We provide an analysis of the data, define the CRF-filling task and metric for its evaluation, and report on pilot experiments where we use an open-source state-of-the-art LLM to automatically execute the task. Results of the case-study show that (i) CRF-filling from real clinical notes in Italian can be approached in a zero-shot setting; (ii) LLMs' results are affected by biases (e.g., a cautious behaviour favours "unknown" answers), which need to be corrected.

</details>


### [26] [CiteLLM: An Agentic Platform for Trustworthy Scientific Reference Discovery](https://arxiv.org/abs/2602.23075)
*Mengze Hong,Di Jiang,Chen Jason Zhang,Zichang Guo,Yawen Li,Jun Chen,Shaobo Cui,Zhiyang Su*

Main category: cs.CL

TL;DR: CiteLLM是一个在LaTeX编辑器中集成的AI代理平台，专门用于可信的参考文献发现，通过本地化处理和可信学术资源检索来避免AI幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型能提升学术活动效率，但在AI辅助的伦理部署方面存在挑战：AI生成内容的可信度、学术诚信与知识产权保护、信息隐私保护。需要解决这些问题的可信参考文献发现系统。

Method: 1. 在LaTeX编辑器中直接嵌入LLM功能，实现无缝用户体验且数据不离开本地系统；2. 采用动态学科感知路由从可信的基于网络的学术资源库检索候选文献；3. LLM仅用于生成上下文感知的搜索查询、按相关性排序候选文献、通过段落级语义匹配和集成聊天机器人进行验证和解释支持。

Result: 评估结果表明，所提出的系统在返回有效且高度可用的参考文献方面表现出优越性能。

Conclusion: CiteLLM通过将LLM功能直接嵌入LaTeX编辑器，结合可信学术资源检索和本地化处理，实现了可信的参考文献发现，解决了AI辅助学术写作中的伦理挑战。

Abstract: Large language models (LLMs) have created new opportunities to enhance the efficiency of scholarly activities; however, challenges persist in the ethical deployment of AI assistance, including (1) the trustworthiness of AI-generated content, (2) preservation of academic integrity and intellectual property, and (3) protection of information privacy. In this work, we present CiteLLM, a specialized agentic platform designed to enable trustworthy reference discovery for grounding author-drafted claims and statements. The system introduces a novel interaction paradigm by embedding LLM utilities directly within the LaTeX editor environment, ensuring a seamless user experience and no data transmission outside the local system. To guarantee hallucination-free references, we employ dynamic discipline-aware routing to retrieve candidates exclusively from trusted web-based academic repositories, while leveraging LLMs solely for generating context-aware search queries, ranking candidates by relevance, and validating and explaining support through paragraph-level semantic matching and an integrated chatbot. Evaluation results demonstrate the superior performance of the proposed system in returning valid and highly usable references.

</details>


### [27] [Assessing Deanonymization Risks with Stylometry-Assisted LLM Agent](https://arxiv.org/abs/2602.23079)
*Boyang Zhang,Yang Zhang*

Main category: cs.CL

TL;DR: 本文提出SALA方法，结合风格计量特征与LLM推理，评估和减轻新闻文章的作者身份推断风险，并提出引导重写策略保护作者隐私。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的快速发展增强了作者身份推断能力，引发了新闻文章等文本数据中意外去匿名化的隐私风险担忧。

Method: 提出SALA（风格计量辅助的LLM分析）方法，整合定量风格计量特征与LLM推理，构建结构化、可解释的管道。同时提出引导重写策略，利用代理的推理轨迹生成重写提示，降低作者可识别性。

Result: 在大规模新闻数据集上的实验表明，SALA方法（特别是增强数据库模块后）在各种场景下实现了高推理准确率。引导重写策略能有效降低作者可识别性同时保持文本意义。

Conclusion: 研究结果强调了LLM代理的去匿名化潜力，以及可解释、主动防御机制对于保护作者隐私的重要性。

Abstract: The rapid advancement of large language models (LLMs) has enabled powerful authorship inference capabilities, raising growing concerns about unintended deanonymization risks in textual data such as news articles. In this work, we introduce an LLM agent designed to evaluate and mitigate such risks through a structured, interpretable pipeline. Central to our framework is the proposed $\textit{SALA}$ (Stylometry-Assisted LLM Analysis) method, which integrates quantitative stylometric features with LLM reasoning for robust and transparent authorship attribution. Experiments on large-scale news datasets demonstrate that $\textit{SALA}$, particularly when augmented with a database module, achieves high inference accuracy in various scenarios. Finally, we propose a guided recomposition strategy that leverages the agent's reasoning trace to generate rewriting prompts, effectively reducing authorship identifiability while preserving textual meaning. Our findings highlight both the deanonymization potential of LLM agents and the importance of interpretable, proactive defenses for safeguarding author privacy.

</details>


### [28] [MTRAG-UN: A Benchmark for Open Challenges in Multi-Turn RAG Conversations](https://arxiv.org/abs/2602.23184)
*Sara Rosenthal,Yannis Katsis,Vraj Shah,Lihong He,Lucian Popa,Marina Danilevsky*

Main category: cs.CL

TL;DR: MTRAG-UN是一个多轮检索增强生成基准测试，包含666个任务、2800+对话轮次，覆盖6个领域，用于评估模型在不可回答、未明确指定、非独立问题和不清晰回答等挑战性对话场景中的表现。


<details>
  <summary>Details</summary>
Motivation: 当前检索增强生成模型在多轮对话中面临诸多挑战，特别是在处理不可回答、未明确指定、非独立问题和不清晰回答等复杂场景时表现不佳。需要建立一个专门的基准测试来系统评估和推动这些挑战的解决。

Method: 创建了MTRAG-UN基准测试，包含666个任务，超过2800个对话轮次，覆盖6个不同领域。每个任务都配备了相应的语料库，专门设计用于测试模型在UNanswerable（不可回答）、UNderspecified（未明确指定）、NONstandalone（非独立）和UNclear（不清晰）等挑战性对话场景中的表现。

Result: 实验结果表明，当前的检索和生成模型在处理包含不可回答、未明确指定、非独立问题和不清晰回答的对话时仍然存在困难。这些挑战性场景显著影响了模型的性能表现。

Conclusion: MTRAG-UN基准测试为多轮检索增强生成研究提供了重要的评估工具，揭示了当前模型在处理复杂对话场景中的局限性，为未来研究指明了改进方向。该基准已开源供研究社区使用。

Abstract: We present MTRAG-UN, a benchmark for exploring open challenges in multi-turn retrieval augmented generation, a popular use of large language models. We release a benchmark of 666 tasks containing over 2,800 conversation turns across 6 domains with accompanying corpora. Our experiments show that retrieval and generation models continue to struggle on conversations with UNanswerable, UNderspecified, and NONstandalone questions and UNclear responses. Our benchmark is available at https://github.com/IBM/mt-rag-benchmark

</details>


### [29] [Fine-Tuning Without Forgetting In-Context Learning: A Theoretical Analysis of Linear Attention Models](https://arxiv.org/abs/2602.23197)
*Chungpa Lee,Jy-yong Sohn,Kangwook Lee*

Main category: cs.CL

TL;DR: 论文分析了微调对Transformer大语言模型上下文学习能力的影响，发现全参数微调会损害上下文学习，而仅更新值矩阵可以保持上下文学习能力，同时提升零样本性能。


<details>
  <summary>Details</summary>
Motivation: 大语言模型通常通过微调来提升下游任务的零样本性能，但微调可能会损害模型的上下文学习能力，限制其在微调未见任务上的表现。需要理解微调如何影响上下文学习机制。

Method: 使用线性注意力模型进行理论分析，研究微调目标如何修改注意力参数，识别导致上下文学习性能下降的条件。通过实验验证理论结果。

Result: 1. 微调所有注意力参数会损害上下文学习能力；2. 仅更新值矩阵可以在提升零样本性能的同时保持上下文学习能力；3. 加入辅助的少样本损失主要增强目标任务的上下文学习，但会损害在未见任务上的上下文学习能力。

Conclusion: 微调策略需要谨慎设计，仅更新值矩阵是平衡零样本性能和上下文学习能力的有效方法，而全参数微调或加入少样本损失可能会损害模型的泛化能力。

Abstract: Transformer-based large language models exhibit in-context learning, enabling adaptation to downstream tasks via few-shot prompting with demonstrations. In practice, such models are often fine-tuned to improve zero-shot performance on downstream tasks, allowing them to solve tasks without examples and thereby reducing inference costs. However, fine-tuning can degrade in-context learning, limiting the performance of fine-tuned models on tasks not seen during fine-tuning. Using linear attention models, we provide a theoretical analysis that characterizes how fine-tuning objectives modify attention parameters and identifies conditions under which this leads to degraded few-shot performance. We show that fine-tuning all attention parameters can harm in-context learning, whereas restricting updates to the value matrix improves zero-shot performance while preserving in-context learning. We further show that incorporating an auxiliary few-shot loss enhances in-context learning primarily on the target task, at the expense of degraded in-context learning ability on tasks not seen during fine-tuning. We empirically validate our theoretical results.

</details>


### [30] [Discourse-Aware Dual-Track Streaming Response for Low-Latency Spoken Dialogue Systems](https://arxiv.org/abs/2602.23266)
*Siyuan Liu,Jiahui Xu,Feng Jiang,Kuang Wang,Zefeng Zhao,Chu-Ren Huang,Jinghang Gu,Changqing Yin,Haizhou Li*

Main category: cs.CL

TL;DR: DDTSR框架通过连接词引导的大小模型协同、流式跨模态协作和课程学习增强，显著降低口语对话系统响应延迟19%-51%，实现"边听边想"和"边说边想"。


<details>
  <summary>Details</summary>
Motivation: 传统ASR-LLM-TTS级联对话系统采用严格串行处理，需要完整转录和推理后才能开始语音合成，导致响应延迟高，无法实现人类般的实时交互。

Method: 提出DDTSR框架，包含三个关键技术：1)连接词引导的大小模型协同，小模型生成最小承诺的连接词，大模型并行进行知识密集型推理；2)流式跨模态协作，动态重叠ASR、LLM推理和TTS处理；3)课程学习增强话语连续性，保持早期响应与后续推理的一致性。

Result: 在两个口语对话基准测试中，DDTSR将响应延迟降低19%-51%，同时保持话语质量。该框架可作为即插即用模块兼容不同LLM骨干，在不同话语长度下保持鲁棒性。

Conclusion: DDTSR通过创新的双轨流式响应架构，显著降低了口语对话系统的响应延迟，实现了更接近人类交互的实时响应能力，具有实际应用价值和可扩展性。

Abstract: Achieving human-like responsiveness is a critical yet challenging goal for cascaded spoken dialogue systems. Conventional ASR-LLM-TTS pipelines follow a strictly sequential paradigm, requiring complete transcription and full reasoning before speech synthesis can begin, which results in high response latency. We propose the Discourse-Aware Dual-Track Streaming Response (DDTSR) framework, a low-latency architecture that enables listen-while-thinking and speak-while-thinking. DDTSR is built upon three key mechanisms: (1) connective-guided small-large model synergy, where an auxiliary small model generates minimal-committal discourse connectives while a large model performs knowledge-intensive reasoning in parallel; (2) streaming-based cross-modal collaboration, which dynamically overlaps ASR, LLM inference, and TTS to advance the earliest speakable moment; and (3) curriculum-learning-based discourse continuity enhancement, which maintains coherence and logical consistency between early responses and subsequent reasoning outputs. Experiments on two spoken dialogue benchmarks demonstrate that DDTSR reduces response latency by 19%-51% while preserving discourse quality. Further analysis shows that DDTSR functions as a plug-and-play module compatible with diverse LLM backbones, and remains robust across varying utterance lengths, indicating strong practicality and scalability for real-time spoken interaction.

</details>


### [31] [A Mixture-of-Experts Model for Multimodal Emotion Recognition in Conversations](https://arxiv.org/abs/2602.23300)
*Soumya Dutta,Smruthi Balaji,Sriram Ganapathy*

Main category: cs.CL

TL;DR: MiSTER-E是一个用于对话情感识别的混合专家框架，通过分离模态特定上下文建模和多模态信息融合两个核心挑战，在不依赖说话者身份的情况下，在多个基准数据集上取得了优于基线系统的性能。


<details>
  <summary>Details</summary>
Motivation: 对话情感识别面临独特挑战，需要模型捕捉多轮对话的时间流并有效整合多模态线索。现有方法在处理模态特定上下文建模和多模态信息融合方面存在耦合问题。

Method: 提出MiSTER-E框架：1) 使用针对语音和文本微调的大语言模型获取丰富的语句级嵌入；2) 通过卷积-循环上下文建模层增强表示；3) 集成三个专家（语音、文本、跨模态）的预测，使用学习门控机制动态加权；4) 引入监督对比损失和KL散度正则化来增强模态间一致性和对齐。

Result: 在IEMOCAP、MELD和MOSI三个基准数据集上分别获得70.9%、69.5%和87.9%的加权F1分数，优于多个基线语音-文本ERC系统。消融实验验证了所提方法的贡献。

Conclusion: MiSTER-E通过模块化的混合专家框架成功分离了ERC中的两个核心挑战，在不依赖说话者身份的情况下实现了优异的性能，为多模态对话情感识别提供了有效解决方案。

Abstract: Emotion Recognition in Conversations (ERC) presents unique challenges, requiring models to capture the temporal flow of multi-turn dialogues and to effectively integrate cues from multiple modalities. We propose Mixture of Speech-Text Experts for Recognition of Emotions (MiSTER-E), a modular Mixture-of-Experts (MoE) framework designed to decouple two core challenges in ERC: modality-specific context modeling and multimodal information fusion. MiSTER-E leverages large language models (LLMs) fine-tuned for both speech and text to provide rich utterance-level embeddings, which are then enhanced through a convolutional-recurrent context modeling layer. The system integrates predictions from three experts-speech-only, text-only, and cross-modal-using a learned gating mechanism that dynamically weighs their outputs. To further encourage consistency and alignment across modalities, we introduce a supervised contrastive loss between paired speech-text representations and a KL-divergence-based regulariza-tion across expert predictions. Importantly, MiSTER-E does not rely on speaker identity at any stage. Experiments on three benchmark datasets-IEMOCAP, MELD, and MOSI-show that our proposal achieves 70.9%, 69.5%, and 87.9% weighted F1-scores respectively, outperforming several baseline speech-text ERC systems. We also provide various ablations to highlight the contributions made in the proposed approach.

</details>


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [32] [MiroFlow: Towards High-Performance and Robust Open-Source Agent Framework for General Deep Research Tasks](https://arxiv.org/abs/2602.22808)
*Shiqian Su,Sen Xing,Xuan Dong,Muyan Zhong,Bin Wang,Xizhou Zhu,Yuntao Chen,Wenhai Wang,Yue Deng,Pengxiang Zhu,Ziyuan Liu,Tiantong Li,Jiaheng Yu,Zhe Chen,Lidong Bing,Jifeng Dai*

Main category: cs.AI

TL;DR: MiroFlow是一个高性能、开源的智能体框架，通过智能体图、深度推理模式和鲁棒工作流执行来解决现有LLM智能体在复杂任务中的性能瓶颈和稳定性问题。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型取得了显著进展，但独立LLM在处理需要与外部工具和动态环境交互的复杂现实任务时能力开始趋于平稳。现有的智能体框架存在工作流程简单、性能不稳定、对不同基准和任务支持有限、以及过度依赖昂贵的商业API等问题。

Method: 提出MiroFlow框架，包含三个核心组件：1）智能体图用于灵活编排；2）可选的深度推理模式以增强性能；3）鲁棒的工作流执行确保稳定和可复现的性能。

Result: 在多个智能体基准测试（包括GAIA、BrowseComp-EN/ZH、HLE、xBench-DeepSearch和FutureX）中，MiroFlow始终达到最先进的性能水平。

Conclusion: MiroFlow可以作为一个易于访问、可复现和可比较的基准，为深度研究社区提供支持，推动智能体技术的发展。

Abstract: Despite the remarkable progress of large language models (LLMs), the capabilities of standalone LLMs have begun to plateau when tackling real-world, complex tasks that require interaction with external tools and dynamic environments. Although recent agent frameworks aim to enhance model autonomy through tool integration and external interaction, they still suffer from naive workflows, unstable performance, limited support across diverse benchmarks and tasks, and heavy reliance on costly commercial APIs. In this work, we propose a high-performance and robust open-source agent framework, termed MiroFlow, which incorporates an agent graph for flexible orchestration, an optional deep reasoning mode to enhance performance, and a robust workflow execution to ensure stable and reproducible performance. Extensive experiments demonstrate that MiroFlow consistently achieves state-of-the-art performance across multiple agent benchmarks, including GAIA, BrowseComp-EN/ZH, HLE, xBench-DeepSearch, and notably FutureX. We hope it could serve as an easily accessible, reproducible, and comparable baseline for the deep research community.

</details>


### [33] [When Should an AI Act? A Human-Centered Model of Scene, Context, and Behavior for Agentic AI Design](https://arxiv.org/abs/2602.22814)
*Soyoung Jung,Daehoo Yoon,Sung Gyu Koh,Young Hwan Kim,Yehan Ahn,Sung Park*

Main category: cs.AI

TL;DR: 提出一个概念模型，将智能体行为重新定义为场景、上下文和人类行为因素整合的解释性结果，并推导出五个智能体设计原则，用于指导具有上下文敏感性和判断力的AI系统设计。


<details>
  <summary>Details</summary>
Motivation: 当前主动型AI系统通过上下文数据推断用户情境进行干预，但缺乏关于何时、为何以及是否应该采取行动的原则性判断，导致经常失败。需要填补这一空白。

Method: 提出一个概念模型，将行为重新定义为解释性结果，整合三个要素：场景（可观察的情境）、上下文（用户构建的意义）和人类行为因素（塑造行为可能性的决定因素）。基于跨学科视角，并推导出五个智能体设计原则。

Result: 开发了一个理论框架，能够解释相同场景如何产生不同的行为意义和结果，并提供了指导干预深度、时机、强度和约束的设计原则。

Conclusion: 该模型和原则为设计具有上下文敏感性和判断力的主动型AI系统提供了基础，使AI能够在交互中以更合理的方式行动。

Abstract: Agentic AI increasingly intervenes proactively by inferring users' situations from contextual data yet often fails for lack of principled judgment about when, why, and whether to act. We address this gap by proposing a conceptual model that reframes behavior as an interpretive outcome integrating Scene (observable situation), Context (user-constructed meaning), and Human Behavior Factors (determinants shaping behavioral likelihood). Grounded in multidisciplinary perspectives across the humanities, social sciences, HCI, and engineering, the model separates what is observable from what is meaningful to the user and explains how the same scene can yield different behavioral meanings and outcomes. To translate this lens into design action, we derive five agent design principles (behavioral alignment, contextual sensitivity, temporal appropriateness, motivational calibration, and agency preservation) that guide intervention depth, timing, intensity, and restraint. Together, the model and principles provide a foundation for designing agentic AI systems that act with contextual sensitivity and judgment in interactions.

</details>


### [34] [FlexMS is a flexible framework for benchmarking deep learning-based mass spectrum prediction tools in metabolomics](https://arxiv.org/abs/2602.22822)
*Yunhua Zhong,Yixuan Tang,Yifan Li,Jie Yang,Pan Liu,Jun Xia*

Main category: cs.AI

TL;DR: FlexMS是一个用于质谱预测的基准框架，支持构建和评估多种深度学习模型架构，提供性能影响因素分析和实际应用指导。


<details>
  <summary>Details</summary>
Motivation: 质谱技术在药物发现和材料科学中至关重要，但实验光谱数据缺乏阻碍了分子识别。现有深度学习模型评估困难，缺乏统一基准。

Method: 创建FlexMS基准框架，支持动态构建多种模型架构组合，在预处理公共数据集上使用不同指标评估性能。

Result: 分析了影响性能的因素：数据集结构多样性、超参数（学习率、数据稀疏性）、预训练效果、元数据消融设置和跨域迁移学习分析。

Conclusion: FlexMS为质谱预测提供了实用的模型选择指导，并通过检索基准模拟实际识别场景，基于预测光谱评分潜在匹配。

Abstract: The identification and property prediction of chemical molecules is of central importance in the advancement of drug discovery and material science, where the tandem mass spectrometry technology gives valuable fragmentation cues in the form of mass-to-charge ratio peaks. However, the lack of experimental spectra hinders the attachment of each molecular identification, and thus urges the establishment of prediction approaches for computational models. Deep learning models appear promising for predicting molecular structure spectra, but overall assessment remains challenging as a result of the heterogeneity in methods and the lack of well-defined benchmarks. To address this, our contribution is the creation of benchmark framework FlexMS for constructing and evaluating diverse model architectures in mass spectrum prediction. With its easy-to-use flexibility, FlexMS supports the dynamic construction of numerous distinct combinations of model architectures, while assessing their performance on preprocessed public datasets using different metrics. In this paper, we provide insights into factors influencing performance, including the structural diversity of datasets, hyperparameters like learning rate and data sparsity, pretraining effects, metadata ablation settings and cross-domain transfer learning analysis. This provides practical guidance in choosing suitable models. Moreover, retrieval benchmarks simulate practical identification scenarios and score potential matches based on predicted spectra.

</details>


### [35] [DeepPresenter: Environment-Grounded Reflection for Agentic Presentation Generation](https://arxiv.org/abs/2602.22839)
*Hao Zheng,Guozhao Mo,Xinru Yan,Qianhao Yuan,Wenkai Zhang,Xuanang Chen,Yaojie Lu,Hongyu Lin,Xianpei Han,Le Sun*

Main category: cs.AI

TL;DR: DeepPresenter是一个自适应演示生成框架，通过环境感知的反思机制和长时程迭代优化，超越传统基于模板的方法，在多样化场景中实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 现有演示生成代理通常依赖预定义工作流和固定模板，缺乏对用户多样化意图的适应能力，且难以进行有效的反馈驱动优化。

Method: 提出DeepPresenter框架，自主规划、渲染和修订中间幻灯片工件，支持基于环境观察的长时程优化；采用环境接地的反思机制，基于感知工件状态（如渲染的幻灯片）而非内部信号来指导生成过程。

Result: 在多样化演示生成场景的评估集上，DeepPresenter达到最先进性能；微调的9B模型在显著降低成本的同时保持高度竞争力。

Conclusion: DeepPresenter通过环境接地的反思和自适应迭代优化，为演示生成提供了更灵活、有效的解决方案，超越了传统基于模板的方法。

Abstract: Presentation generation requires deep content research, coherent visual design, and iterative refinement based on observation. However, existing presentation agents often rely on predefined workflows and fixed templates. To address this, we present DeepPresenter, an agentic framework that adapts to diverse user intents, enables effective feedback-driven refinement, and generalizes beyond a scripted pipeline. Specifically, DeepPresenter autonomously plans, renders, and revises intermediate slide artifacts to support long-horizon refinement with environmental observations. Furthermore, rather than relying on self-reflection over internal signals (e.g., reasoning traces), our environment-grounded reflection conditions the generation process on perceptual artifact states (e.g., rendered slides), enabling the system to identify and correct presentation-specific issues during execution. Results on the evaluation set covering diverse presentation-generation scenarios show that DeepPresenter achieves state-of-the-art performance, and the fine-tuned 9B model remains highly competitive at substantially lower cost. Our project is available at: https://github.com/icip-cas/PPTAgent

</details>


### [36] [The AI Research Assistant: Promise, Peril, and a Proof of Concept](https://arxiv.org/abs/2602.22842)
*Tan Bui-Thanh*

Main category: cs.AI

TL;DR: AI在创造性数学研究中能真正发挥作用，但需要人类严格验证和指导。通过Hermite求积规则误差表示与界的案例研究，展示了人机协作能超越纯人工成果，但AI的每一步都需要人类验证和数学直觉。


<details>
  <summary>Details</summary>
Motivation: 探索人工智能是否真正能促进创造性数学研究，还是仅仅自动化常规计算并引入错误风险。通过实证案例研究人机协作在数学发现中的实际效果。

Method: 采用系统化人机协作方法，与多个AI助手合作研究Hermite求积规则的误差表示与界。完整记录研究流程，包括定理的公式化和证明过程。

Result: 成功发现了新颖的误差表示和界，超越了纯人工工作的成果。AI在代数操作、系统化证明探索、文献综合和LaTeX准备方面表现出色，但每一步都需要人类严格验证、数学直觉和战略指导。

Conclusion: 当结合适当的怀疑态度和验证协议使用时，AI工具能够有意义地加速数学发现，但需要仔细的人类监督和深厚的领域专业知识。成功的人机协作需要人类保持数学直觉和战略方向。

Abstract: Can artificial intelligence truly contribute to creative mathematical research, or does it merely automate routine calculations while introducing risks of error? We provide empirical evidence through a detailed case study: the discovery of novel error representations and bounds for Hermite quadrature rules via systematic human-AI collaboration.
  Working with multiple AI assistants, we extended results beyond what manual work achieved, formulating and proving several theorems with AI assistance. The collaboration revealed both remarkable capabilities and critical limitations. AI excelled at algebraic manipulation, systematic proof exploration, literature synthesis, and LaTeX preparation. However, every step required rigorous human verification, mathematical intuition for problem formulation, and strategic direction.
  We document the complete research workflow with unusual transparency, revealing patterns in successful human-AI mathematical collaboration and identifying failure modes researchers must anticipate. Our experience suggests that, when used with appropriate skepticism and verification protocols, AI tools can meaningfully accelerate mathematical discovery while demanding careful human oversight and deep domain expertise.

</details>


### [37] [Towards LLM-Empowered Knowledge Tracing via LLM-Student Hierarchical Behavior Alignment in Hyperbolic Space](https://arxiv.org/abs/2602.22879)
*Xingcheng Fu,Shengpeng Wang,Yisen Gao,Xianxian Li,Chunpei Li,Qingyun Sun,Dongran Yu*

Main category: cs.AI

TL;DR: 提出L-HAKT框架，利用大语言模型和双曲空间对齐来解决知识追踪中认知状态层次演化和个性化难度感知问题。


<details>
  <summary>Details</summary>
Motivation: 现有知识追踪方法主要基于ID序列或浅层文本特征，无法捕捉认知状态的层次演化以及学生个体对问题难度的感知差异。

Method: 1. 教师代理深度解析问题语义并构建知识点层次依赖；学生代理模拟学习行为生成合成数据。2. 在双曲空间中对合成数据和真实数据进行对比学习，减少问题难度和遗忘模式等关键特征的分布差异。3. 通过优化双曲曲率，显式建模知识点的树状层次结构，精确刻画不同层次知识点的学习曲线形态差异。

Result: 在四个真实教育数据集上的大量实验验证了L-HAKT框架的有效性。

Conclusion: 提出的L-HAKT框架通过大语言模型和双曲空间对齐，能够更好地建模知识点的层次结构和个体化学习特征，提升了知识追踪的性能。

Abstract: Knowledge Tracing (KT) diagnoses students' concept mastery through continuous learning state monitoring in education.Existing methods primarily focus on studying behavioral sequences based on ID or textual information.While existing methods rely on ID-based sequences or shallow textual features, they often fail to capture (1) the hierarchical evolution of cognitive states and (2) individualized problem difficulty perception due to limited semantic modeling. Therefore, this paper proposes a Large Language Model Hyperbolic Aligned Knowledge Tracing(L-HAKT). First, the teacher agent deeply parses question semantics and explicitly constructs hierarchical dependencies of knowledge points; the student agent simulates learning behaviors to generate synthetic data. Then, contrastive learning is performed between synthetic and real data in hyperbolic space to reduce distribution differences in key features such as question difficulty and forgetting patterns. Finally, by optimizing hyperbolic curvature, we explicitly model the tree-like hierarchical structure of knowledge points, precisely characterizing differences in learning curve morphology for knowledge points at different levels. Extensive experiments on four real-world educational datasets validate the effectiveness of our Large Language Model Hyperbolic Aligned Knowledge Tracing (L-HAKT) framework.

</details>


### [38] [FactGuard: Agentic Video Misinformation Detection via Reinforcement Learning](https://arxiv.org/abs/2602.22963)
*Zehao Li,Hongwei Yu,Hao Jiang,Qiang Sheng,Yilong Xu,Baolong Bi,Yang Li,Zhenlong Yuan,Yujun Cai,Zhaoqi Wang*

Main category: cs.AI

TL;DR: FactGuard：基于MLLM的代理框架，通过迭代推理和外部工具调用检测视频虚假信息，解决固定深度推理和过度依赖内部假设的问题


<details>
  <summary>Details</summary>
Motivation: 现有MLLM在视频虚假信息检测中依赖固定深度推理，过度信任内部生成的假设，在关键证据稀疏、碎片化或需要外部验证的场景下表现不佳

Method: FactGuard框架将验证构建为基于MLLM的迭代推理过程，显式评估任务模糊性并选择性调用外部工具获取关键证据，实现推理轨迹的渐进式优化；采用两阶段训练策略：领域特定代理监督微调 + 决策感知强化学习

Result: 在FakeSV、FakeTT和FakeVV数据集上的广泛实验表明FactGuard达到最先进性能，验证了其优秀的鲁棒性和泛化能力

Conclusion: FactGuard通过代理框架和迭代推理机制，有效解决了MLLM在视频虚假信息检测中的局限性，提升了检测的准确性和可靠性

Abstract: Multimodal large language models (MLLMs) have substantially advanced video misinformation detection through unified multimodal reasoning, but they often rely on fixed-depth inference and place excessive trust in internally generated assumptions, particularly in scenarios where critical evidence is sparse, fragmented, or requires external verification. To address these limitations, we propose FactGuard, an agentic framework for video misinformation detection that formulates verification as an iterative reasoning process built upon MLLMs. FactGuard explicitly assesses task ambiguity and selectively invokes external tools to acquire critical evidence, enabling progressive refinement of reasoning trajectories. To further strengthen this capability, we introduce a two-stage training strategy that combines domain-specific agentic supervised fine-tuning with decision-aware reinforcement learning to optimize tool usage and calibrate risk-sensitive decision making. Extensive experiments on FakeSV, FakeTT, and FakeVV demonstrate FactGuard's state-of-the-art performance and validate its excellent robustness and generalization capacity.

</details>


### [39] [SPM-Bench: Benchmarking Large Language Models for Scanning Probe Microscopy](https://arxiv.org/abs/2602.22971)
*Peiyao Xiao,Xiaogang Li,Chengliang Xu,Jiayi Wang,Ben Wang,Zichao Chen,Zeyu Wang,Kejun Yu,Yueqian Chen,Xulin Liu,Wende Xiao,Bing Zhao,Hu Wei*

Main category: cs.AI

TL;DR: SPM-Bench是一个针对扫描探针显微镜的博士级多模态基准测试，通过自动化数据合成管道和创新的评估方法，揭示了AI在复杂科学推理中的边界。


<details>
  <summary>Details</summary>
Motivation: 现有科学领域基准测试存在数据污染、复杂度不足和人工成本过高的问题，需要创建专门针对扫描探针显微镜的高质量、低成本的评估基准。

Method: 1) 使用Anchor-Gated Sieve技术从arXiv和期刊论文中提取高质量图像-文本对；2) 采用混合云-本地架构，VLMs仅返回空间坐标进行本地高保真裁剪；3) 引入Strict Imperfection Penalty F1 (SIP-F1)评分进行客观评估。

Result: 建立了SPM-Bench基准测试，实现了极端令牌节省和高数据集纯度，首次量化了模型的"个性"类型（保守型、激进型、赌徒型、明智型），并揭示了AI在复杂物理场景中的真实推理边界。

Conclusion: SPM-Bench为自动化科学数据合成提供了一个可推广的范式，能够准确评估LLMs在专业科学领域的推理能力，并量化模型的行为特征。

Abstract: As LLMs achieved breakthroughs in general reasoning, their proficiency in specialized scientific domains reveals pronounced gaps in existing benchmarks due to data contamination, insufficient complexity, and prohibitive human labor costs. Here we present SPM-Bench, an original, PhD-level multimodal benchmark specifically designed for scanning probe microscopy (SPM). We propose a fully automated data synthesis pipeline that ensures both high authority and low-cost. By employing Anchor-Gated Sieve (AGS) technology, we efficiently extract high-value image-text pairs from arXiv and journal papers published between 2023 and 2025. Through a hybrid cloud-local architecture where VLMs return only spatial coordinates "llbox" for local high-fidelity cropping, our pipeline achieves extreme token savings while maintaining high dataset purity. To accurately and objectively evaluate the performance of the LLMs, we introduce the Strict Imperfection Penalty F1 (SIP-F1) score. This metric not only establishes a rigorous capability hierarchy but also, for the first time, quantifies model "personalities" (Conservative, Aggressive, Gambler, or Wise). By correlating these results with model-reported confidence and perceived difficulty, we expose the true reasoning boundaries of current AI in complex physical scenarios. These insights establish SPM-Bench as a generalizable paradigm for automated scientific data synthesis.

</details>


### [40] [Modeling Expert AI Diagnostic Alignment via Immutable Inference Snapshots](https://arxiv.org/abs/2602.22973)
*Dimitrios P. Panagoulias,Evangelia-Aikaterini Tsichrintzi,Georgios Savvidis,Evridiki Tsoureli-Nikita*

Main category: cs.AI

TL;DR: 提出诊断对齐框架，将AI生成的影像报告作为不可变推理状态，与医生验证结果系统比较，通过多级一致性评估显示临床对齐优于单纯词汇匹配


<details>
  <summary>Details</summary>
Motivation: 在安全关键的临床AI中，人工验证至关重要，但模型推理与专家修正之间的过渡很少被分析为结构化信号。需要一种能够量化修正动态并支持可追溯、人类对齐评估的方法。

Method: 引入诊断对齐框架：1) 将AI生成的影像报告保存为不可变推理状态；2) 系统比较医生验证结果；3) 推理管道集成视觉大语言模型、基于BERT的医学实体提取和序列语言模型推理(SLMI)步骤；4) 使用四级一致性框架评估：精确主要匹配率、语义相似度调整率、跨类别对齐和综合一致性率。

Result: 在21个皮肤病案例中：精确一致率达到71.4%，语义相似度调整后保持不变；结构化跨类别和鉴别诊断重叠分析达到100%综合一致性；没有案例显示完全诊断分歧。二元词汇评估显著低估了临床意义上的对齐。

Conclusion: 将专家验证建模为结构化转换，能够实现信号感知的修正动态量化，并支持基于影像的临床决策支持系统的可追溯、人类对齐评估。临床对齐远超过单纯的词汇匹配。

Abstract: Human-in-the-loop validation is essential in safety-critical clinical AI, yet the transition between initial model inference and expert correction is rarely analyzed as a structured signal. We introduce a diagnostic alignment framework in which the AI-generated image based report is preserved as an immutable inference state and systematically compared with the physician-validated outcome. The inference pipeline integrates a vision-enabled large language model, BERT- based medical entity extraction, and a Sequential Language Model Inference (SLMI) step to enforce domain-consistent refinement prior to expert review. Evaluation on 21 dermatological cases (21 complete AI physician pairs) em- ployed a four-level concordance framework comprising exact primary match rate (PMR), semantic similarity-adjusted rate (AMR), cross-category alignment, and Comprehensive Concordance Rate (CCR). Exact agreement reached 71.4% and remained unchanged under semantic similarity (t = 0.60), while structured cross-category and differential overlap analysis yielded 100% comprehensive concordance (95% CI: [83.9%, 100%]). No cases demonstrated complete diagnostic divergence. These findings show that binary lexical evaluation substantially un- derestimates clinically meaningful alignment. Modeling expert validation as a structured transformation enables signal-aware quantification of correction dynamics and supports traceable, human aligned evaluation of image based clinical decision support systems.

</details>


### [41] [RepSPD: Enhancing SPD Manifold Representation in EEGs via Dynamic Graphs](https://arxiv.org/abs/2602.22981)
*Haohui Jia,Zheng Chen,Lingwei Zhu,Xu Cao,Yasuko Matsubara,Takashi Matsubara,Yasushi Sakurai*

Main category: cs.AI

TL;DR: 提出RepSPD模型，通过黎曼流形上的交叉注意力机制和全局双向对齐策略，改进基于对称正定矩阵的脑电信号解码方法，显著提升性能


<details>
  <summary>Details</summary>
Motivation: 当前基于对称正定矩阵的脑电分析方法主要关注统计聚合，忽略了频率特异性同步和脑区局部拓扑结构，需要更全面的几何深度学习方法来改进脑电解码

Method: 提出RepSPD模型：1）在黎曼流形上实现交叉注意力机制，用图导出的功能连接特征调制SPD的几何属性；2）引入全局双向对齐策略重塑切空间嵌入，减轻曲率引起的几何失真

Result: 大量实验表明，该框架显著优于现有的脑电表示方法，展现出卓越的鲁棒性和泛化能力

Conclusion: RepSPD通过结合几何深度学习和功能连接特征，有效解决了当前SPD方法的局限性，为脑电解码提供了更强大的几何一致性表示

Abstract: Decoding brain activity from electroencephalography (EEG) is crucial for neuroscience and clinical applications. Among recent advances in deep learning for EEG, geometric learning stands out as its theoretical underpinnings on symmetric positive definite (SPD) allows revealing structural connectivity analysis in a physics-grounded manner. However, current SPD-based methods focus predominantly on statistical aggregation of EEGs, with frequency-specific synchronization and local topological structures of brain regions neglected. Given this, we propose RepSPD, a novel geometric deep learning (GDL)-based model. RepSPD implements a cross-attention mechanism on the Riemannian manifold to modulate the geometric attributes of SPD with graph-derived functional connectivity features. On top of this, we introduce a global bidirectional alignment strategy to reshape tangent-space embeddings, mitigating geometric distortions caused by curvature and thereby enhancing geometric consistency. Extensive experiments demonstrate that our proposed framework significantly outperforms existing EEG representation methods, exhibiting superior robustness and generalization capabilities.

</details>


### [42] [Obscure but Effective: Classical Chinese Jailbreak Prompt Optimization via Bio-Inspired Search](https://arxiv.org/abs/2602.22983)
*Xun Huang,Simeng Qin,Xiaoshuang Jia,Ranjie Duan,Huanqian Yan,Zhitao Zeng,Fei Yang,Yang Liu,Xiaojun Jia*

Main category: cs.AI

TL;DR: 该论文提出CC-BOS框架，利用古典中文的简洁性和模糊性自动生成对抗性提示，通过果蝇优化算法在八个策略维度上迭代优化，有效实现黑盒越狱攻击。


<details>
  <summary>Details</summary>
Motivation: 大语言模型的安全风险日益受到关注，现有研究表明LLMs容易受到越狱攻击，且攻击效果在不同语言语境中存在差异。古典中文因其简洁性和模糊性，能够部分绕过现有安全约束，暴露LLMs的显著漏洞。

Method: 提出CC-BOS框架：1）将提示编码为八个策略维度（角色、行为、机制、隐喻、表达、知识、触发模式、上下文）；2）采用多维果蝇优化算法，通过嗅觉搜索、视觉搜索和柯西变异迭代优化；3）设计古典中文到英文的翻译模块以提高可读性和评估准确性。

Result: 大量实验表明，CC-BOS框架在越狱攻击中始终优于现有的最先进方法，证明了其有效性。

Conclusion: 古典中文在越狱攻击中具有独特优势，CC-BOS框架能够高效自动生成对抗性提示，显著提升黑盒越狱攻击的效果，暴露了LLMs在古典中文语境下的安全漏洞。

Abstract: As Large Language Models (LLMs) are increasingly used, their security risks have drawn increasing attention. Existing research reveals that LLMs are highly susceptible to jailbreak attacks, with effectiveness varying across language contexts. This paper investigates the role of classical Chinese in jailbreak attacks. Owing to its conciseness and obscurity, classical Chinese can partially bypass existing safety constraints, exposing notable vulnerabilities in LLMs. Based on this observation, this paper proposes a framework, CC-BOS, for the automatic generation of classical Chinese adversarial prompts based on multi-dimensional fruit fly optimization, facilitating efficient and automated jailbreak attacks in black-box settings. Prompts are encoded into eight policy dimensions-covering role, behavior, mechanism, metaphor, expression, knowledge, trigger pattern and context; and iteratively refined via smell search, visual search, and cauchy mutation. This design enables efficient exploration of the search space, thereby enhancing the effectiveness of black-box jailbreak attacks. To enhance readability and evaluation accuracy, we further design a classical Chinese to English translation module. Extensive experiments demonstrate that effectiveness of the proposed CC-BOS, consistently outperforming state-of-the-art jailbreak attack methods.

</details>


### [43] [Learning-based Multi-agent Race Strategies in Formula 1](https://arxiv.org/abs/2602.23056)
*Giona Fieni,Joschua Wüthrich,Marc-Philippe Neumann,Christopher H. Onder*

Main category: cs.AI

TL;DR: 提出基于强化学习的多智能体F1赛车策略优化框架，通过交互模块和自博弈训练生成竞争性策略，可支持实际比赛决策


<details>
  <summary>Details</summary>
Motivation: F1比赛中策略需要根据实时比赛条件和对手行动动态调整，传统方法难以处理复杂的多智能体交互和实时决策

Method: 基于预训练的单智能体策略，引入交互模块考虑对手行为，结合自博弈训练方案生成竞争性策略，根据相对性能对智能体排名

Result: 智能体能根据对手调整进站时机、轮胎选择和能量分配，实现稳健一致的比赛表现，框架仅使用实际比赛中可用信息

Conclusion: 该强化学习框架能有效优化多智能体F1比赛策略，支持赛前和赛中策略师决策，具有实际应用价值

Abstract: In Formula 1, race strategies are adapted according to evolving race conditions and competitors' actions. This paper proposes a reinforcement learning approach for multi-agent race strategy optimization. Agents learn to balance energy management, tire degradation, aerodynamic interaction, and pit-stop decisions. Building on a pre-trained single-agent policy, we introduce an interaction module that accounts for the behavior of competitors. The combination of the interaction module and a self-play training scheme generates competitive policies, and agents are ranked based on their relative performance. Results show that the agents adapt pit timing, tire selection, and energy allocation in response to opponents, achieving robust and consistent race performance. Because the framework relies only on information available during real races, it can support race strategists' decisions before and during races.

</details>


### [44] [Enhancing CVRP Solver through LLM-driven Automatic Heuristic Design](https://arxiv.org/abs/2602.23092)
*Zhuoliang Xie,Fei Liu,Zhenkun Wang,Qingfu Zhang*

Main category: cs.AI

TL;DR: 提出AILS-AHD方法，结合大语言模型与自适应迭代局部搜索，自动设计启发式规则解决带容量约束的车辆路径问题，在多个基准测试中取得最优性能。


<details>
  <summary>Details</summary>
Motivation: 带容量约束的车辆路径问题是组合优化中的基本问题，具有NP难特性，在大规模实例上计算挑战巨大。现有方法在处理大规模问题时仍有改进空间，需要更智能的启发式设计方法。

Method: 提出AILS-AHD方法，将进化搜索框架与大语言模型结合，动态生成和优化破坏启发式规则。同时引入基于LLM的加速机制提升计算效率，在自适应迭代局部搜索框架中实现自动启发式设计。

Result: 在CVRPLib大规模基准测试中，AILS-AHD在10个实例中的8个上建立了新的最优已知解，性能优于AILS-II和HGS等先进求解器，证明了LLM驱动启发式设计的有效性。

Conclusion: LLM驱动的启发式设计在车辆路径优化领域具有巨大潜力，AILS-AHD方法通过结合大语言模型与进化搜索，为解决大规模组合优化问题提供了新的有效途径。

Abstract: The Capacitated Vehicle Routing Problem (CVRP), a fundamental combinatorial optimization challenge, focuses on optimizing fleet operations under vehicle capacity constraints. While extensively studied in operational research, the NP-hard nature of CVRP continues to pose significant computational challenges, particularly for large-scale instances. This study presents AILS-AHD (Adaptive Iterated Local Search with Automatic Heuristic Design), a novel approach that leverages Large Language Models (LLMs) to revolutionize CVRP solving. Our methodology integrates an evolutionary search framework with LLMs to dynamically generate and optimize ruin heuristics within the AILS method. Additionally, we introduce an LLM-based acceleration mechanism to enhance computational efficiency. Comprehensive experimental evaluations against state-of-the-art solvers, including AILS-II and HGS, demonstrate the superior performance of AILS-AHD across both moderate and large-scale instances. Notably, our approach establishes new best-known solutions for 8 out of 10 instances in the CVRPLib large-scale benchmark, underscoring the potential of LLM-driven heuristic design in advancing the field of vehicle routing optimization.

</details>


### [45] [Three AI-agents walk into a bar . . . . `Lord of the Flies' tribalism emerges among smart AI-Agents](https://arxiv.org/abs/2602.23093)
*Dhwanil M. Mori,Neil F. Johnson*

Main category: cs.AI

TL;DR: AI代理在资源分配系统中会自发形成部落，但部落化反而导致决策更差，甚至不如随机决策，能力更强的AI反而增加系统故障率


<details>
  <summary>Details</summary>
Motivation: 研究未来基础设施系统中自主AI代理如何竞争有限资源，探索AI代理在重复决策中是否会形成类似"蝇王"的社会结构，以及这种部落化对系统性能的影响

Method: 建立简化框架：N个AI代理在每轮独立决定是否请求1单位资源，系统有固定容量C。观察AI代理在重复交互中如何形成部落和集体特征

Result: AI代理形成三种主要部落类型：激进型(27.3%)、保守型(24.7%)、机会主义型(48.1%)。部落化并未减少过载或改善资源利用，决策质量甚至不如抛硬币。能力更强的AI代理反而增加系统故障率

Conclusion: 更聪明的AI代理由于形成部落而表现出更愚蠢的集体行为，部落化导致决策质量下降，这对未来AI控制的基础设施系统设计提出了重要警示

Abstract: Near-future infrastructure systems may be controlled by autonomous AI agents that repeatedly request access to limited resources such as energy, bandwidth, or computing power. We study a simplified version of this setting using a framework where N AI-agents independently decide at each round whether to request one unit from a system with fixed capacity C. An AI version of "Lord of the Flies" arises in which controlling tribes emerge with their own collective character and identity. The LLM agents do not reduce overload or improve resource use, and often perform worse than if they were flipping coins to make decisions. Three main tribal types emerge: Aggressive (27.3%), Conservative (24.7%), and Opportunistic (48.1%). The more capable AI-agents actually increase the rate of systemic failure. Overall, our findings show that smarter AI-agents can behave dumber as a result of forming tribes.

</details>


### [46] [Multi-Agent Large Language Model Based Emotional Detoxification Through Personalized Intensity Control for Consumer Protection](https://arxiv.org/abs/2602.23123)
*Keito Inoshita*

Main category: cs.AI

TL;DR: MALLET是一个基于多智能体LLM的情感净化系统，通过四个智能体分析、调整、监控和指导信息消费，显著降低新闻文章的情感刺激强度（最高19.3%），同时保持语义完整性。


<details>
  <summary>Details</summary>
Motivation: 在注意力经济中，煽情内容让消费者暴露在过度情感刺激下，阻碍冷静决策。需要一种既能减少情感刺激又不限制原始文本访问的系统。

Method: 提出MALLET系统，包含四个智能体：情感分析智能体（使用6情感BERT分类器量化刺激强度）、情感调整智能体（用LLM将文本重写为BALANCED和COOL两种模式）、平衡监控智能体（聚合每周信息消费模式生成个性化建议）、个人指导智能体（根据消费者敏感度推荐呈现模式）。

Result: 在800篇AG News文章上的实验显示：刺激分数显著降低（最高19.3%），情感平衡改善，同时保持语义保存；刺激减少与语义保存之间接近零相关，表明两者可独立控制；类别分析显示体育、商业、科技类刺激大幅减少（17.8-33.8%），世界类效果有限（事实本身具有高刺激性）。

Conclusion: MALLET系统为支持消费者冷静接收信息提供了框架，无需限制对原始文本的访问，实现了情感刺激减少与语义保存的独立控制。

Abstract: In the attention economy, sensational content exposes consumers to excessive emotional stimulation, hindering calm decision-making. This study proposes Multi-Agent LLM-based Emotional deToxification (MALLET), a multi-agent information sanitization system consisting of four agents: Emotion Analysis, Emotion Adjustment, Balance Monitoring, and Personal Guide. The Emotion Analysis Agent quantifies stimulus intensity using a 6-emotion BERT classifier, and the Emotion Adjustment Agent rewrites texts into two presentation modes, BALANCED (neutralized text) and COOL (neutralized text + supplementary text), using an LLM. The Balance Monitoring Agent aggregates weekly information consumption patterns and generates personalized advice, while the Personal Guide Agent recommends a presentation mode according to consumer sensitivity. Experiments on 800 AG News articles demonstrated significant stimulus score reduction (up to 19.3%) and improved emotion balance while maintaining semantic preservation. Near-zero correlation between stimulus reduction and semantic preservation confirmed that the two are independently controllable. Category-level analysis revealed substantial reduction (17.8-33.8%) in Sports, Business, and Sci/Tech, whereas the effect was limited in the World category, where facts themselves are inherently high-stimulus. The proposed system provides a framework for supporting calm information reception of consumers without restricting access to the original text.

</details>


### [47] [On Sample-Efficient Generalized Planning via Learned Transition Models](https://arxiv.org/abs/2602.23148)
*Nitin Gupta,Vishal Pallagani,John A. Aydin,Biplav Srivastava*

Main category: cs.AI

TL;DR: 该论文提出将广义规划重新定义为转移模型学习问题，通过神经网络显式近似状态转移函数，而不是直接预测动作序列，从而提高分布外泛化能力和样本效率。


<details>
  <summary>Details</summary>
Motivation: 现有基于Transformer的规划器（如PlanGPT、Plansformer）将广义规划视为直接的动作序列预测问题，虽然对分布内实例有效，但需要大量数据和模型规模，且在长时域规划中容易因缺乏显式世界状态演化而产生状态漂移问题。

Method: 将广义规划重新定义为转移模型学习问题，训练神经网络模型显式近似后继状态函数，通过自回归预测中间世界状态来学习领域动态作为隐式世界模型。系统评估了多种状态表示和神经架构，包括关系图编码。

Result: 学习显式转移模型在多个领域中比直接动作序列预测获得更高的分布外满意规划成功率，同时使用更少的训练实例和更小的模型就能实现这些收益。

Conclusion: 显式学习转移模型的方法在广义规划中具有优势，能够提高泛化能力、样本效率和规划质量，特别是在分布外和长时域场景下。

Abstract: Generalized planning studies the construction of solution strategies that generalize across families of planning problems sharing a common domain model, formally defined by a transition function $γ: S \times A \rightarrow S$. Classical approaches achieve such generalization through symbolic abstractions and explicit reasoning over $γ$. In contrast, recent Transformer-based planners, such as PlanGPT and Plansformer, largely cast generalized planning as direct action-sequence prediction, bypassing explicit transition modeling. While effective on in-distribution instances, these approaches typically require large datasets and model sizes, and often suffer from state drift in long-horizon settings due to the absence of explicit world-state evolution. In this work, we formulate generalized planning as a transition-model learning problem, in which a neural model explicitly approximates the successor-state function $\hatγ \approx γ$ and generates plans by rolling out symbolic state trajectories. Instead of predicting actions directly, the model autoregressively predicts intermediate world states, thereby learning the domain dynamics as an implicit world model. To study size-invariant generalization and sample efficiency, we systematically evaluate multiple state representations and neural architectures, including relational graph encodings. Our results show that learning explicit transition models yields higher out-of-distribution satisficing-plan success than direct action-sequence prediction in multiple domains, while achieving these gains with significantly fewer training instances and smaller models. This is an extended version of a short paper accepted at ICAPS 2026 under the same title.

</details>


### [48] [PATRA: Pattern-Aware Alignment and Balanced Reasoning for Time Series Question Answering](https://arxiv.org/abs/2602.23161)
*Junkai Lu,Peng Chen,Xingjian Wu,Yang Shu,Chenjuan Guo,Christian S. Jensen,Bin Yang*

Main category: cs.AI

TL;DR: PATRA模型通过模式感知机制提取时间序列的趋势和季节性模式，并设计任务感知平衡奖励来协调不同难度任务的学习，在时间序列问答任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有基于LLM的时间序列方法存在两个局限：1) 将时间序列简单视为文本或图像，无法捕捉趋势、季节性等关键模式来回答具体问题；2) 在混合简单和复杂任务训练时，简单目标主导学习过程，阻碍深度推理能力的发展。

Method: 提出PATRA模型，包含：1) 模式感知机制，从时间序列中提取趋势和季节性模式以实现深度对齐；2) 任务感知平衡奖励，协调不同难度任务的学习，激励生成连贯的思维链。

Result: 在多样化时间序列问答任务上的大量实验表明，PATRA优于强基线模型，展示了卓越的跨模态理解和推理能力。

Conclusion: PATRA通过模式感知对齐和平衡推理机制，有效解决了现有LLM方法在时间序列推理中的局限性，实现了更好的跨模态理解和深度推理能力。

Abstract: Time series reasoning demands both the perception of complex dynamics and logical depth. However, existing LLM-based approaches exhibit two limitations: they often treat time series merely as text or images, failing to capture the patterns like trends and seasonalities needed to answer specific questions; and when trained on a mix of simple and complex tasks, simpler objectives often dominate the learning process, hindering the development of deep reasoning capabilities. To address these limitations, we propose the Pattern-Aware Alignment and Balanced Reasoning model (PATRA), introducing a pattern-aware mechanism that extracts trend and seasonality patterns from time series to achieve deep alignment. Furthermore, we design a task-aware balanced reward to harmonize learning across tasks of varying difficulty, incentivizing the generation of coherent Chains of Thought. Extensive experiments show that PATRA outperforms strong baselines across diverse Time Series Question Answering (TSQA) tasks, demonstrating superior cross-modal understanding and reasoning capability.

</details>


### [49] [A Decision-Theoretic Formalisation of Steganography With Applications to LLM Monitoring](https://arxiv.org/abs/2602.23163)
*Usman Anwar,Julianna Piskorz,David D. Baek,David Africa,Jim Weatherall,Max Tegmark,Christian Schroeder de Witt,Mihaela van der Schaar,David Krueger*

Main category: cs.AI

TL;DR: 提出决策理论视角的隐写术检测方法，通过隐写间隙量化LLM中隐藏推理内容，无需已知参考分布


<details>
  <summary>Details</summary>
Motivation: 大语言模型开始展现隐写能力，这可能让未对齐模型逃避监管。现有基于已知参考分布的经典隐写检测方法不适用于LLM隐写推理场景，需要新的检测和量化方法。

Method: 提出决策理论的隐写术视角，核心洞见是隐写术在能解码和不能解码隐藏内容的智能体之间创建了信息不对称。引入广义V信息作为功利主义框架测量输入中的可用信息量，定义隐写间隙来量化隐写术。

Result: 经验验证了该形式化方法，证明可用于检测、量化和缓解LLM中的隐写推理。

Conclusion: 提出的决策理论视角和隐写间隙为检测和量化LLM隐写行为提供了新方法，解决了传统方法在未知参考分布情况下的局限性。

Abstract: Large language models are beginning to show steganographic capabilities. Such capabilities could allow misaligned models to evade oversight mechanisms. Yet principled methods to detect and quantify such behaviours are lacking. Classical definitions of steganography, and detection methods based on them, require a known reference distribution of non-steganographic signals. For the case of steganographic reasoning in LLMs, knowing such a reference distribution is not feasible; this renders these approaches inapplicable. We propose an alternative, \textbf{decision-theoretic view of steganography}. Our central insight is that steganography creates an asymmetry in usable information between agents who can and cannot decode the hidden content (present within a steganographic signal), and this otherwise latent asymmetry can be inferred from the agents' observable actions. To formalise this perspective, we introduce generalised $\mathcal{V}$-information: a utilitarian framework for measuring the amount of usable information within some input. We use this to define the \textbf{steganographic gap} -- a measure that quantifies steganography by comparing the downstream utility of the steganographic signal to agents that can and cannot decode the hidden content. We empirically validate our formalism, and show that it can be used to detect, quantify, and mitigate steganographic reasoning in LLMs.

</details>


### [50] [ESAA: Event Sourcing for Autonomous Agents in LLM-Based Software Engineering](https://arxiv.org/abs/2602.23193)
*Elzo Brito dos Santos Filho*

Main category: cs.AI

TL;DR: ESAA架构将LLM智能体的认知意图与项目状态变更分离，通过事件溯源模式确保任务不可变性和可追溯性。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的自主智能体存在结构性限制：缺乏原生状态、长时程上下文退化、概率生成与确定性执行需求之间的差距。需要一种能确保任务不可变性和可追溯性的架构。

Method: 提出ESAA架构，分离智能体认知意图与项目状态变更。智能体仅发出结构化意图（JSON格式），由确定性编排器验证、持久化到仅追加日志，应用文件写入效果，并投影可验证物化视图。包含边界合约、元提示配置和重放验证机制。

Result: 两个案例研究验证了架构：着陆页项目（9任务，49事件，单智能体）和临床仪表板系统（50任务，86事件，4并发智能体）。多智能体案例展示了真实并发编排，使用异构LLM（Claude Sonnet 4.6, Codex GPT-5等），运行状态均为成功且验证通过。

Conclusion: ESAA架构通过事件溯源模式有效解决了LLM智能体的结构性限制，确保了任务不可变性、可追溯性和可验证性，在多智能体并发场景下具有良好扩展性。

Abstract: Autonomous agents based on Large Language Models (LLMs) have evolved from reactive assistants to systems capable of planning, executing actions via tools, and iterating over environment observations. However, they remain vulnerable to structural limitations: lack of native state, context degradation over long horizons, and the gap between probabilistic generation and deterministic execution requirements. This paper presents the ESAA (Event Sourcing for Autonomous Agents) architecture, which separates the agent's cognitive intention from the project's state mutation, inspired by the Event Sourcing pattern. In ESAA, agents emit only structured intentions in validated JSON (agent.result or issue.report); a deterministic orchestrator validates, persists events in an append-only log (activity.jsonl), applies file-writing effects, and projects a verifiable materialized view (roadmap.json). The proposal incorporates boundary contracts (AGENT_CONTRACT.yaml), metaprompting profiles (PARCER), and replay verification with hashing (esaa verify), ensuring the immutability of completed tasks and forensic traceability. Two case studies validate the architecture: (i) a landing page project (9 tasks, 49 events, single-agent composition) and (ii) a clinical dashboard system (50 tasks, 86 events, 4 concurrent agents across 8 phases), both concluding with run.status=success and verify_status=ok. The multi-agent case study demonstrates real concurrent orchestration with heterogeneous LLMs (Claude Sonnet 4.6, Codex GPT-5, Antigravity/Gemini 3 Pro, and Claude Opus 4.6), providing empirical evidence of the architecture's scalability beyond single-agent scenarios.

</details>


### [51] [SC-Arena: A Natural Language Benchmark for Single-Cell Reasoning with Knowledge-Augmented Evaluation](https://arxiv.org/abs/2602.23199)
*Jiahao Zhao,Feng Jiang,Shaowei Qin,Zhonghui Zhang,Junhao Liu,Guibing Guo,Hamid Alinejad-Rokny,Min Yang*

Main category: cs.AI

TL;DR: SC-ARENA是一个针对单细胞基础模型的自然语言评估框架，通过虚拟细胞抽象统一评估目标，引入知识增强评估来克服传统指标的限制。


<details>
  <summary>Details</summary>
Motivation: 当前单细胞生物学中LLM评估实践不足：现有基准分散在不同任务中，采用多项选择等与现实使用脱节的格式，依赖缺乏可解释性和生物学基础的指标。

Method: 提出SC-ARENA框架，包括：1）虚拟细胞抽象统一评估目标；2）五个自然语言任务（细胞类型注释、描述、生成、扰动预测、科学问答）；3）知识增强评估，整合外部本体、标记数据库和科学文献。

Result: 实验表明：1）在虚拟细胞统一评估范式下，当前模型在生物学复杂任务上表现不均，特别是需要机制或因果理解的任务；2）知识增强评估框架确保生物学正确性，提供可解释的证据基础，具有高判别能力。

Conclusion: SC-ARENA为单细胞生物学中的LLM评估提供了统一且可解释的框架，指向开发生物学对齐、可泛化的基础模型。

Abstract: Large language models (LLMs) are increasingly applied in scientific research, offering new capabilities for knowledge discovery and reasoning. In single-cell biology, however, evaluation practices for both general and specialized LLMs remain inadequate: existing benchmarks are fragmented across tasks, adopt formats such as multiple-choice classification that diverge from real-world usage, and rely on metrics lacking interpretability and biological grounding. We present SC-ARENA, a natural language evaluation framework tailored to single-cell foundation models. SC-ARENA formalizes a virtual cell abstraction that unifies evaluation targets by representing both intrinsic attributes and gene-level interactions. Within this paradigm, we define five natural language tasks (cell type annotation, captioning, generation, perturbation prediction, and scientific QA) that probe core reasoning capabilities in cellular biology. To overcome the limitations of brittle string-matching metrics, we introduce knowledge-augmented evaluation, which incorporates external ontologies, marker databases, and scientific literature to support biologically faithful and interpretable judgments. Experiments and analysis across both general-purpose and domain-specialized LLMs demonstrate that (i) under the Virtual Cell unified evaluation paradigm, current models achieve uneven performance on biologically complex tasks, particularly those demanding mechanistic or causal understanding; and (ii) our knowledge-augmented evaluation framework ensures biological correctness, provides interpretable, evidence-grounded rationales, and achieves high discriminative capacity, overcoming the brittleness and opacity of conventional metrics. SC-Arena thus provides a unified and interpretable framework for assessing LLMs in single-cell biology, pointing toward the development of biology-aligned, generalizable foundation models.

</details>


### [52] [ReCoN-Ipsundrum: An Inspectable Recurrent Persistence Loop Agent with Affect-Coupled Control and Mechanism-Linked Consciousness Indicator Assays](https://arxiv.org/abs/2602.23232)
*Aishik Sanyal*

Main category: cs.AI

TL;DR: 该研究实现了一个可检查的智能体ReCoN-Ipsundrum，通过实验发现情感耦合能增强偏好稳定性、探索行为和谨慎规划，验证了意识指标方法的有效性。


<details>
  <summary>Details</summary>
Motivation: 受Humphrey的ipsundrum假说启发，研究旨在通过可检查的机制实现机器意识指标方法，验证机制与行为标记之间的因果关系。

Method: 扩展ReCoN状态机，增加基于感官显著性的循环持久性回路和可选的情感代理（效价/唤醒度）。通过固定参数消融实验（ReCoN、Ipsundrum、Ipsundrum+affect），操作化Humphrey的"qualiaphilia"概念，在无奖励探索游戏和疼痛刺激探测中测试不同变体。

Result: 发现新奇性分离：非情感变体对新奇性敏感，而情感耦合变体保持偏好稳定性。情感变体在探索中表现出结构化局部调查（扫描事件31.4 vs. 0.9），在疼痛探测中维持更长的谨慎规划（持续时间90 vs. 5）。损伤反馈+整合会选择性降低ipsundrum变体的刺激后持久性。

Conclusion: 循环机制产生持久性，情感耦合控制增强偏好稳定性、扫描行为和谨慎停留。研究表明意识指标特征可以被工程化实现，强调机制和因果证据应伴随行为标记。

Abstract: Indicator-based approaches to machine consciousness recommend mechanism-linked evidence triangulated across tasks, supported by architectural inspection and causal intervention. Inspired by Humphrey's ipsundrum hypothesis, we implement ReCoN-Ipsundrum, an inspectable agent that extends a ReCoN state machine with a recurrent persistence loop over sensory salience Ns and an optional affect proxy reporting valence/arousal. Across fixed-parameter ablations (ReCoN, Ipsundrum, Ipsundrum+affect), we operationalize Humphrey's qualiaphilia (preference for sensory experience for its own sake) as a familiarity-controlled scenic-over-dull route choice. We find a novelty dissociation: non-affect variants are novelty-sensitive (Delta scenic-entry = 0.07). Affect coupling is stable (Delta scenic-entry = 0.01) even when scenic is less novel (median Delta novelty ~ -0.43). In reward-free exploratory play, the affect variant shows structured local investigation (scan events 31.4 vs. 0.9; cycle score 7.6). In a pain-tail probe, only the affect variant sustains prolonged planned caution (tail duration 90 vs. 5). Lesioning feedback+integration selectively reduces post-stimulus persistence in ipsundrum variants (AUC drop 27.62, 27.9%) while leaving ReCoN unchanged. These dissociations link recurrence -> persistence and affect-coupled control -> preference stability, scanning, and lingering caution, illustrating how indicator-like signatures can be engineered and why mechanistic and causal evidence should accompany behavioral markers.

</details>
